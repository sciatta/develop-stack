{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-19 17:49:23 "},"src/bigdata/hadoop/Hadoop配置.html":{"url":"src/bigdata/hadoop/Hadoop配置.html","title":"Hadoop配置","keywords":"","body":"端口映射 core-site.xml fs.defaultFS 8020 NameNode的RPC地址 hdfs-site.xml dfs.namenode.http-address 50070 NameNode的http地址 dfs.namenode.secondary.http-address 50090 SecondaryNameNode的http地址 数据目录 hdfs-site.xml dfs.namenode.name.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas/ NameNode fsimage 可以配置多个目录，不同的目录存放相同的fsimage dfs.namenode.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits NameNode edits dfs.namenode.checkpoint.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name SecondNameNode fsimage dfs.namenode.checkpoint.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits SecondNameNode edits dfs.datanode.data.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas DataNode 可以配置多个目录，不同的目录存放不同的数据文件 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 16:02:10 "},"src/bigdata/hadoop/hdfs/HDFS命令.html":{"url":"src/bigdata/hadoop/hdfs/HDFS命令.html","title":"HDFS命令","keywords":"","body":"hdfs hdfs dfs hdfs有两种命令风格 hadoop fs hdfs dfs 两种命令等价 help hadoop fs -help ls hdfs dfs -help ls ls # 查看根目录文件列表 hdfs dfs -ls / # 递归显示目录内容 hdfs dfs -ls -R / # 显示本地文件系统列表，默认hdfs # file:// 表示本地文件协议 hdfs dfs -ls file:///bigdata/ touchz # 创建空文件 hdfs dfs -touchz /mynote appendToFile # 向文件末尾追加内容 # 注意命令区分大小写 hdfs dfs -appendToFile hello /mynote cat # 查看文件内容 hdfs dfs -cat /mynote put # 上传本地文件到hdfs hdfs dfs -put hello /h1 copyFromLocal（同put） hdfs dfs -copyFromLocal hello /h2 moveFromLocal # 上传成功，删除本机文件 hdfs dfs -moveFromLocal hello /h3 get # 下载hdfs文件到本地 hdfs dfs -get /h1 hello copyToLocal（同get） hdfs dfs -copyToLocal /h1 hello1 mkdir # 创建目录 hdfs dfs -mkdir /shell rm # 删除文件到垃圾桶 # 不能删除目录 hdfs dfs -rm /h1 # 递归删除目录 hdfs dfs -rm -r /shell rmr # 递归删除目录 # 不建议使用，可使用 rm -r 代替 hdfs dfs -rmr /hello mv # 目的文件或目录不存在，修改文件名或目录名 # 目的文件存在，不可移动 hdfs dfs -mv /h2 /h22 # 目的目录存在，将子文件或子目录移动到目录 hdfs dfs -mv /h22 /hello cp # 拷贝文件 # 若目的文件存在，不可复制 hdfs dfs -cp /bigf /bf # 拷贝文件到目的目录 hdfs dfs -cp /bigf /hello find # 查找文件 hdfs dfs -find / -name \"h*\" text # 查看文件内容，若为SequenceFile，即使压缩，也可以正常查看压缩前内容 hdfs dfs -text /sequence/none expunge # 清空回收站，同时创建回收站checkpoint hdfs dfs -expunge hdfs getconf namenodes # 获取NameNode节点名称，可能有多个 hdfs getconf -namenodes confKey # 用相同的命令可以获得其他属性值 # 获取最小块大小 默认1048576byte（1M） hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size nnRpcAddresses # 获取NameNode的RPC地址 hdfs getconf -nnRpcAddresses hdfs dfsadmin safemode # 查看当前安全模式状态 hdfs dfsadmin -safemode get # 进入安全模式 # 安全模式只读 # 增删改不可以，查可以 hdfs dfsadmin -safemode enter # 退出安全模式 hdfs dfsadmin -safemode leave allowSnapshot 快照顾名思义，就是相当于对我们的hdfs文件系统做一个备份，我们可以通过快照对我们指定的文件夹设置备份，但是添加快照之后，并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 # 创建快照之前，先要允许该目录创建快照 hdfs dfsadmin -allowSnapshot /main # 禁用 hdfs dfsadmin -disallowSnapshot /main # 指定目录创建快照 # Created snapshot /main/.snapshot/s20200113-114345.126 # 可以通过浏览器访问 http://192.168.2.100:50070/explorer.html#/main/.snapshot/s20200113-114345.126 hdfs dfs -createSnapshot /main # 创建快照指定名称 hdfs dfs -createSnapshot /main snap1 # 快照重命名 hdfs dfs -renameSnapshot /main snap1 snap2 # 列出当前用户下的所有快照目录 hdfs lsSnapshottableDir # 比较两个快照的不同 hdfs snapshotDiff /main snap1 snap2 # 删除快照 hdfs dfs -deleteSnapshot /main snap1 hdfs fsck # 查看文件的文件、块、位置信息 hdfs fsck /h3 -files -blocks -locations hdfs namenode format # 格式化NameNode，只在初次搭建集群时使用 hdfs namenode -format hdfs oiv # 查看fsimage内容 offine image view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oiv -i fsimage_0000000000000000196 -p XML -o test.xml hdfs oev # 查看edits内容 offine edits view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oev -i edits_0000000000000000001-0000000000000000009 -p XML -o test.xml hadoop hadoop fs（同hdfs dfs） hadoop checknative # 查看本地库安装状态 hadoop checknative hadoop jar # 在集群上执行自定义的jar hadoop jar hadoop-mapreduce-wordcount-1.0-SNAPSHOT.jar com.sciatta.hadoop.mapreduce.wordcount.WordCount hadoop archive # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main hadoop distcp # hadoop 集群间数据拷贝 hadoop distcp hdfs://node01:8020/test hdfs://cluster:8020/ Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 15:32:50 "},"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html":{"url":"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html","title":"HDFS核心功能原理","keywords":"","body":"hdfs优缺点 优点 高容错性 数据自动保存多个副本； 某一个副本丢失后，可以自动恢复。 适合批处理 它是通过移动计算而不是移动数据； 它会把数据位置暴露给计算框架。 适合大数据处理 数据规模，可以处理数据规模达到GB、TB、甚至PB级别的数据； 文件规模，能够处理百万级别规模以上的文件数量； 节点规模：能够处理10K节点的规模。 流式数据访问 一次写入，多次读取，不能修改，只能追加； 它能保证数据的一致性。 可构建在廉价机器上，通过多副本机制，提高可靠性 它通过多副本机制，提高可靠性； 它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 缺点 不适合低延时数据访问，比如毫秒级的数据访问 不适合毫秒级的数据访问； 它适合高吞吐率的场景，就是在某一时间内写入大量的数据。 无法高效的对大量小文件进行存储 存储大量小文件，会占用NameNone大量内存存储元数据。但NameNode的内存总是有限的； 小文件存储的寻址时间会超过读取时间，违反了HDFS的设计目标。 不支持并发写入、文件随机修改 一个文件只能有一个线程写，不允许多个线程多时写； 仅支持数据append，不支持文件的随机修改。 hdfs写入流程 概述 写入本地file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode向HDFS写入数据，先调用DistributedFileSystem的 create 方法获取FSDataOutputStream。 DistributedFileSystem调用NameNode的 create 方法，发出文件创建请求。NameNode对待上传文件名称和路径做检验，如上传文件是否已存在同名目录，文件是否已经存在，递归创建文件的父目录（如不存在）等。并将操作记录在edits文件中。 ClientNode调用FSDataOutputStream向输出流输出数据（假设先写block1）。 FSDataOutputStream调用NameNode的 addBlock 方法申请block1的blockId和block要存储在哪几个DataNode（假设DataNode1，DataNode2和DataNode3）。若pipeline还没有建立，则根据位置信息建立pipeline。 同返回的第一个DataNode节点DataNode1建立socket连接，向其发送package。同时，此package会保存一份到ackqueue确认队列中。 写数据时先将数据写到一个校验块chunk中，写满512字节，对chunk计算校验和checksum值（4字节）。 以带校验和的checksum为单位向本地缓存输出数据（本地缓存占9个chunk），本地缓存满了向package输入数据，一个package占64kb。 当package写满后，将package写入dataqueue数据队列中。 将package从dataqueue数据对列中取出，沿pipeline发送到DataNode1，DataNode1保存，然后将package发送到DataNode2，DataNode2保存，再向DataNode3发送package。DataNode3接收到package，然后保存。 package到达DataNode3后做校验，将校验结果逆着pipeline回传给ClientNode。 DataNode3将校验结果传给DataNode2，DataNode2做校验后将校验结果传给DataNode1，DataNode1做校验后将校验结果传给ClientNode。 ClientNode根据校验结果判断，如果”成功“，则将ackqueue确认队列中的package删除；如果”失败“，则将ackqueue确认队列中的package取出，重新放入到dataqueue数据队列末尾，等待重新沿pipeline发送。 当block1的所有package发送完毕。即DataNode1、DataNode2和DataNode3都存在block1的完整副本，则三个DataNode分别调用NameNode的 blockReceivedAndDeleted方法。NameNode会更新内存中DataNode和block的关系。 ClientNode关闭同DataNode建立的pipeline。 文件仍存在未发送的block2，则继续执行4。直到文件所有数据传输完成。 全部数据输出完成，调用FSDataOutputStream的 close 方法。 ClientNode调用NameNode的 complete 方法，通知NameNode全部数据输出完成。 容错 假设当前构建的pipeline是DataNode1、DataNode2和DataNode3。当数据传输过程中，DataNode2中断无法响应，则当前pipeline中断，需要重建。 先将ackqueue中的所有package取出放回到dataqueue末尾。 ClientNode调用NameNode的 updateBlockForPipeline 方法，为当前block生成新的版本，如ts1（本质是时间戳），然后将故障DataNode2从pipeline中删除。 FSDataOutputStream调用NameNode的 getAdditionalDataNode 方法，由NameNode分配新的DataNode，假设是DataNode4。 FSDataOutputStream把DataNode1、DataNode3和DataNode4建立新的pipeline，DataNode1和DataNode3上的block版本设置为ts1，通知DataNode1或DataNode3将block拷贝到DataNode4。 新的pipeline创建好后，FSDataOutputStream调用NameNode的 updataPipeline 方法更新NameNode元数据。之后，按照正常的写入流程完成数据输出。 后续，当DataNode2从故障中恢复。DataNode2向NameNode报送所有block信息，NameNode发现block为旧版本（非ts1），则通过DataNode2的心跳返回通知DataNode2将此旧版本的block删除。 分析源码 解惑 [x] Lease（租约） 在HDFS写文件中是通过Lease（租约）来维护写文件凭证的，所以得到一个文件的写权限之后将其租约进行存储并定时更新。 [ ] block、package数据格式，如何校验（TCP协议，为什么会出现丢包），输出的基本单位（package），如何输出 [ ] hdfs block的上限是多少？ hdfs读取流程 概述 获取file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode调用DistributedFileSystem的 open 方法获取FSDataInputStream。 DistributedFileSystem向NameNode发出请求获取file文件的元数据，包括所有块所在的DataNode的位置信息。 ClientNode调用FSDataInputStream获取数据流。 FSDataInputStream调用就近DanaNode获取block1。DanaNode开始传输数据给客户端，从磁盘里面读取数据输入流，以Packet为单位来做校验。ClientNode以Packet为单位接收数据，先在本地缓存，然后写入目标文件。 文件仍存在未读取的block2，则继续执行4。直到文件所有数据读取完成。 全部数据接收完成，关闭数据流FSDataInputStream。 分析源码 解惑 [ ] 文件块信息LocatedBlocks包含哪些重要信息？ [ ] 重传机制 [ ] DataNode优先位置策略 [ ] 数据的版本问题 NN和SNN功能剖析 概述 NameNode对集群中元数据进行管理，外围节点需要频繁随机存取元数据。 如何支持快速随机存取？因此需要把元数据存储在内存中。但内存中的数据在服务器断电后就会丢失，所以内存中的元数据需要被持久化。 持久化哪里？持久化到文件系统中，存储为fsimage文件。随着时间的流逝，fsimage文件会变得越来越庞大，同时对内存和fsimage元数据的增、删、改、查操作，fsimage文件的大小就会成为存取速度的瓶颈。 如何优化？引入edits日志文件。fsimage为某一时间节点的全量元数据，而edits日志为最新元数据。也就是说，Namenode同时对内存和edits日志进行操作。 之后又会出现edits日志越来越大，以及如何同fsimage合并的问题？系统引入了SecondNameNode，其负责将edits日志和fsimage合并，然后将最新的fsimage推送给NameNode，而NameNode则是向最新生成的edits日志文件写入元数据。 工作机制 如果NameNode是首次启动，则需要格式化HDFS，生成fsimage和edits；否则，启动时读取fsimage和edits到内存中初始化元数据。 ClientNode向NameNode发起元数据操作请求（增删改查），NameNode将元数据先写入edits（防止NameNode挂掉，内存中元数据丢失导致客户端无法访问数据），再写入内存中。 SecondNameNode向NameNode定时发起请求确认是否需要checkpoint。如果满足到达设置定时时间间隔或edits文件写满，则发起checkpoint请求；否则，继续等待。 checkpoint需满足条件： 时间达到一个小时fsimage与edits就会进行合并 dfs.namenode.checkpoint.period 3600s hdfs操作达到1000000次也会进行合并 dfs.namenode.checkpoint.txns 1000000 检查间隔 每隔多长时间检查一次hdfs dfs.namenode.checkpoint.check.period 60s 请求执行check point。 NameNode滚动生成新的edits.new文件，后续ClientNode对元数据操作请求都记录到edits.new文件中。 SecondNameNode通过http get获取NameNode滚动前的edits和fsimage文件。 SecondNameNode将fsimage读入内存，逐条执行edits，合并生成fsimage.ckpt文件。 SecondNameNode通过http post将fsimage.ckpt文件发送到NameNode上。 NameNode将fsimage.ckpt改名为fsimage（此文件为此刻全量元数据，待后续NameNode重启加载），将edits.new改名为edits。同时，会更新fstime。 解惑 [ ] 如何回放？ [ ] NameNode内存不够，如何解决？ 联邦机制（什么是？） 增加物理内存。 DataNode工作机制和数据存储 工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件：一个是数据本身，一个是元数据包括数据块的长度，数据块的校验和，以及时间戳。 DataNode启动后向NameNode注册，通过后则周期性（1小时）的向NameNode上报所有的块信息。 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令。如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器。 数据完整性 当Client读取DataNode上block的时候，会计算checksum。如果计算后的checksum，与block创建时值不一样，说明block已经损坏。这时Client需要读取其他DataNode上的block。 DataNode在其文件创建后周期验证checksum。 掉线参数设置 DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval dfs.namenode.heartbeat.recheck-interval 300000ms dfs.heartbeat.interval 3s 解惑 [x] 一个DataNode上存储的所有数据块可以有相同的，为什么？ 一般出于安全性和高可用性考虑，并不会把一个block的多个副本放在同一个datanode上。但也不是绝对。例如，三个datanode，副本默认是三个的话，那么正常来说，每个节点上存储一个block副本是最好的（安全、可靠性高，单节点出现问题，并不会丢失数据），如果把3个副本都放在一个节点上，一旦这个节点出现问题，数据就可能丢失了；如果副本数是5个的话，那么就存在同一个datanode有多个副本了，即副本数比datanode数多的时候，必然存在一个datanode上存放多个相同block了。 小文件治理 概述 hdfs中文件以block存储在DataNode中，而所有文件的元数据全部存储在NameNode的内存中。无论文件大小，都会占用NameNode元数据的内存存储空间，大约占用150K左右。所以，系统中如果有大量小文件的话，会出现DataNode的磁盘容量没有充分利用，而NameNode的内存却被大量消耗，然而NameNode的内存是有容量限制的。所以，需要对小文件治理。 HAR方案 本质启动MapReduce，因此需要首先启动Yarn。 创建归档文件 # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main # 源文件不会删除 查看归档文件 # 显示归档包含文件 # 归档文件的类型是d，即目录 # -R 递归列出目录内容 hdfs dfs -ls -R /main/data.har # 显示归档包含实际内容 hdfs dfs -ls -R har:///main/data.har 解压归档文件 # -p 如果目录已经存在，不会失败 hdfs dfs -mkdir -p /main/out hdfs dfs -cp har:///main/data.har/* /main/out SequenceFile方案 SequenceFile是由record构成，每个record是由键值对构成，其中文件名作为record的key，而文件内容作为record的value。Record间随机插入Sync，方便定位到Record的边界。 SequenceFile是可以分割的，所以可以利用MapReduce切分，独立运算。 HAR不支持压缩，而SequenceFile支持压缩。支持两类压缩： Record压缩 Block压缩，一次性压缩多条Record作为一个Block；每个Block开始处都需要插入Sync 当不指定压缩算法时，默认使用zlib压缩 无论是否压缩，采用何种算法，均可使用 hdfs dfs -text 命令查看文件内容 一般情况下，以Block压缩为最好选择。因为一个Block包含多条Record，利用Record间的相似性进行压缩，压缩效率更高。 把已有小文件转存为SequenceFile较慢，相比先写小文件，再写SequenceFile而言，直接将数据写入SequenceFile是更好的选择，省去小文件作为中间媒介。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 14:55:45 "},"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html":{"url":"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html","title":"MapReduce工作原理","keywords":"","body":"概述 MapReduce的核心思想是“分而治之”，把大任务分解为小任务并行计算。Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。Reduce负责“合”，即对Map阶段的结果进行全局汇总。 编程模型 Map阶段 第一步：InputFormat 设置InputFormat类，将我们的数据切分成 (key, value) 输出。本质是把大任务拆分为互相独立的小任务。如TextInputFormat，输入的是block，输出的key是行偏移位置，value是每行的内容。 InputFormat 描述 TextInputFormat 1、默认将每个block作为一个split2、输出的key是行偏移位置，value是每行的内容 CombineTextInputFormat 1、解决小文件导致过多split的问题。涉及到虚拟存储和切片过程，可以自定义split大小2、输出的key是行偏移位置，value是每行的内容 KeyValueTextInputFormat 1、默认将每个block作为一个split2、以自定义分隔符进行分割，输出相应的key和value NLineInputFormat 1、以输入文件的N行作为一个split2、输出的key是行偏移位置，value是每行的内容 第二步：MapTask 自定义map逻辑，对输入的 (key, value) 进行处理，转换成新的 (key, value) 进行输出。本质是把小任务拆分为最小可计算任务。例如需求是统计单词数量，第一步是TextInputFormat，则MapTask输入的key是行偏移位置，value是每行的内容。因此，可以进一步拆分单行的内容，输出的key是一个单词，value是1。后面把所有相同单词的value累加，即为一个单词出现的数量。最后汇总所有单词即可满足需求。 Shuffle阶段 第三步：Partition 对输入的 (key, value) 进行分区。满足条件的key划分到一个分区中，一个分区发送到一个ReduceTask Order：对不同分区的数据按照key进行排序 Combine：对分组后的数据进行规约(combine操作)，降低数据的网络拷贝（可选步骤） Group：对排序后的数据进行分组，将相同key的value放到一个集合当中 Reduce阶段 ReduceTask：自定义Reduce逻辑，对输入的key，value对进行处理，转换成新的key，value对进行输出 OutputFormat：设置Outputformat将输出的key，value对数据进行保存到文件中 MapTask工作机制 MapTask个数 MapTask的并行度是由什么决定的？ 其中，block是hdfs系统存储文件的单位。而每个MapTask处理一个切片split的数据量，split是逻辑上对输入进行切片。 切片大小的计算公式：Math.max(minSize, Math.min(maxSize, blockSize)); 其中，mapreduce.input.fileinputformat.split.minsize=1 ，mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 。block的默认大小是128M，所以split的默认大小是128M，同block的大小一致。 如果要控制split的数量，则只需要改变minsize或maxsize就可以改变切片的大小。如果自定义split大于128M，minsize要大于128M；如果自定义split小于128M，maxsize要小于128M。 如果有1000个小文件，每个文件在1K-100M之间，默认情况下有1000个block，1000个split，1000个MapTask并行处理？ 通过CombineTextInputFormat来控制小文件的切片数量，从而控制MapTask的数量。 切片机制： CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4m 切片过程包括虚拟存储和切片两步： 虚拟存储：将所有小文件依次同MaxInputSplitSize参数作比较 小于参数，则直接作为一个逻辑块 大于参数，且不大于2倍参数，则均分作为两个逻辑块（防止出现太小的切片） 大于2倍参数，则先按参数切割一个逻辑块，剩下的继续1、2、3步判断 如：MaxInputSplitSize设置为4M，输入文件大小为8.02M，满足3，则先划分一个4M的逻辑块，剩下人4.02继续判断；4.02满足2，则均分为两个2.01M的逻辑块。 切片：将所有虚拟存储逻辑块依次同MaxInputSplitSize参数作比较 如果大于等于参数，则作为一个切片 如果小于参数，则同下一个虚拟存储逻辑块一起作为一个切片 如：MaxInputSplitSize设置为4M，有4个小文件大小：1.7M、5.1M、3.4M以及6.8M 虚拟存储为6个逻辑块：1.7M 2.55M 2.55M 3.4M 3.4M 3.4M 切片为3个切片：（1.7M+2.55M=4.25M）（2.55M+3.4M=5.95M）（3.4M+3.4M=6.8M） 测试场景： 10个文件：0.1K 2个文件：8.1M 8K 虚拟存储：4M 2.05M 2.05M 8K 切片：4M 4.1M 8K ReduceTask工作机制 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-22 21:18:09 "},"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html":{"url":"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html","title":"GitBook&GitHub&Typora最佳实践","keywords":"","body":"概述 目前有众多的知识管理软件，对于比较私密的文档使用印象笔记，不仅支持Markdown这种轻量级的标记语言，也支持复杂的富文本语言，但是一些比较有用的功能则需要收费，比如查看历史版本、离线访问、容量限制。最近，使用了一下阿里的语雀，书写体验、排版和易操作方面确实属于不错的一款产品，可是，某一天断网了，不支持离线访问是硬伤。因此，想要寻找如下一款产品满足如下需求： 沉浸式的书写体验，不需要丰富的社交场景，也不需要过于酷炫的功能，只想安安静静的码字。 离线编辑，想什么时候访问自己做主。同时，同步是必不可少的。 支持版本管理。 不收费，也没有容量限制。想写多少写多少。 支持私有笔记。 支持同其他同事、同学沟通，因为大部分都是程序员，所以可以查看Markdown文件是必不可少的。 对笔记中的目录结构没有限制。 对笔记中关联的图像，本地和远程都可以访问，而且没有容量限制。 可以方便的导出其他格式，比如html，PDF等。 通过调研，使用GitHub&GitBook&Typora方案，可以完美的解决以上问题。 搭建环境 GitHub GitHub创建develop-stack项目，其中.gitignore选择gitbook 回到本地创建本地目录 mkdir note && cd note 下载项目到本地 git clone https://github.com/sciatta/develop-stack.git GitBook 安装nodejs brew install node 安装GitBook命令行工具 npm install -g gitbook-cli 在develop-stack目录执行 gitbook init 初始化Gitbook目录环境，生成两个重要文件README.md和SUMMARY.md Typora 编辑SUMMARY.md描述项目的目录和文件结构 在develop-stack目录执行 gitbook init 。GitBook会查找SUMMARY.md文件中描述的目录和文件，如果没有则会将其创建。注意，如果删除SUMMARY.md描述项目的目录和文件结构，执行 gitbook init 命令不会删除相应的目录或文件，需要手动维护。 GitBook 本地运行 gitbook serve --port 8088 开启GitBook服务。通过 http://localhost:8080 访问本地服务。在执行命令的同时会执行构建命令 gitbook build 生成 _book 目录。注意， _book 目录是临时目录，每次构建时全部重建。如果退出服务的话，执行 control + c 即可 本地构建 gitbook build ./ docs 。 GitHub 执行 git add 命令 git add -A 将文件提交到暂存区 执行 git commit 命令 git commit -m “test” 将文件提交到本地仓库 执行 git push 命令 git push origin master 将文件提交到远程仓库 GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Source选择 master branch/docs folder 。成功后显示 Your site is ready to be published at https://sciatta.github.io/develop-stack/ ，稍后即可访问GitHub Pages网站。 GitBook插件 在develop-stack目录创建book.json文件，配置插件后，运行 gitbook install 命令自动安装插件。 hide-element 隐藏默认gitbook左侧提示：Published with GitBook { \"plugins\": [ \"hide-element\" ], \"pluginsConfig\": { \"hide-element\": { \"elements\": [\".gitbook-link\"] } } } expandable-chapters gitbook默认目录没有折叠效果。 { \"plugins\": [ \"expandable-chapters\" ] } code 在代码区域的右上角添加一个复制按钮，点击一键复制代码。 { \"plugins\" : [ \"code\" ] } splitter 左侧目录和右侧文章可以拖动调节宽度。 { \"plugins\": [ \"splitter\" ] } search-pro 支持中英文。 { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ] } pageview-count 记录每个文章页面被访问的次数。本质是访问 https://hitcounter.pythonanywhere.com/ { \"plugins\": [ \"pageview-count\"] } tbfed-pagefooter 在每个文章下面标注版权信息和文章时间。 { \"plugins\": [ \"tbfed-pagefooter\" ], \"pluginsConfig\": { \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy sciatta.com 2020\", \"modify_label\": \"修订时间: \", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" } } } popup 点击可以在新窗口展示图片。 { \"plugins\": [ \"popup\" ] } auto-scroll-table 为避免表格过宽，增加滚动条。 { \"plugins\": [\"auto-scroll-table\"] } -sharing 去掉 GitBook 默认的分享功能。由于默认的一些推特，脸书都需要翻墙，所以将分享功能全部关闭。 \"plugins\": [ \"-sharing\" ] github-buttons 给 GitBook 添加 GitHub 的图标来显示 star 和 follow。 { \"plugins\": [ \"github-buttons\" ], \"pluginsConfig\": { \"github-buttons\": { \"buttons\": [{ \"user\": \"sciatta\", \"repo\": \"develop-stack\", \"type\": \"star\", \"count\": true, \"size\": \"small\" }] } } } github 在右上角显示 github 仓库的图标链接 { \"plugins\": [ \"github\" ], \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/sciatta\" } } } Google统计 添加Google统计 Google统计 https://analytics.google.com/ 在一个平台上可以全面分析业务数据，进而做出更明智的决策。在网站注册，获取跟踪ID { \"plugins\": [\"ga\"], \"pluginsConfig\": { \"ga\": { \"token\": \"UA-156491400-1\" } } } BaiDu 添加BaiDu统计 百度统计https://tongji.baidu.com/。在网站注册，获取跟踪ID { \"plugin\": [\"baidu\"], \"pluginsConfig\": { \"baidu\": { \"token\": \"c6612709c010da681bbd4b785968a638\" } } } Donate Gitbook 捐赠打赏插件 { \"plugins\": [\"donate\"], \"pluginsConfig\": { \"donate\": { \"wechat\": \"/assets/wechat.jpg\", \"alipay\": \"/assets/alipay.jpg\", \"title\": \"\", \"button\": \"捐赠\", \"alipayText\": \" \", \"wechatText\": \" \" } } } anchors 标题带有 github 样式的锚点 { \"plugins\" : [ \"anchors\" ] } anchor-navigation-ex 页面内导航，一键回到顶部。 { \"plugins\": [\"anchor-navigation-ex\"], \"pluginsConfig\": { \"anchor-navigation-ex\": { \"showLevel\": true, \"associatedWithSummary\": false, \"printLog\": false, \"multipleH1\": true, \"mode\": \"float\", \"showGoTop\": true, \"float\": { \"floatIcon\": \"fa fa-navicon\", \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" }, \"pageTop\": { \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" } } } } sitemap 生成站点地图，便于爬虫抓取页面。可以通过http://www.sciatta.com/sitemap.xml 访问。 { \"plugins\": [\"sitemap\"], \"pluginsConfig\": { \"sitemap\": { \"hostname\": \"http://www.sciatta.com/\" } } } Typora设置 偏好设置 | 编辑器 | 图片插入 复制图片到 ./${filename}.assets 文件夹 优先使用相对路径 偏好设置 | 通用 | 启动选项 | 打开指定目录 选择工作目录 绑定域名 获取GitHub Pages的IP地址 ping -c 3 sciatta.github.io 注意，ping的时候不需要加仓库的名称。 配置阿里云 进入阿里云解析列表，添加记录： 记录类型 主机记录 记录值 A @ 185.199.111.153 A www 185.199.111.153 配置GitHub Pages GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Custom domain：www.sciatta.com Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 20:09:00 "},"src/virtualization/vmware/VMware系统安装.html":{"url":"src/virtualization/vmware/VMware系统安装.html","title":"VMware系统安装","keywords":"","body":"Mac安装VMware 安装VMware_Fusion_Pro_11.5 注册码 7HYY8-Z8WWY-F1MAN-ECKNY-LUXYX 设置网络 偏好设置 | 网络 | 新建vmnet2自定义网络连接 选择：使用NAT 选择：将MAC主机连接到该网络 取消选择：通过DHCP在该网络上提供地址 修改子网 # 空格需用\\转义 cd /Library/Preferences/VMware\\ Fusion sudo vi networking answer VNET_2_HOSTONLY_SUBNET 192.168.2.0 修改网关 cd vmnet2 sudo vi nat.conf # NAT gateway address ip = 192.168.2.2 偏好设置 | 网络 | vmnet2 取消选择：将MAC主机连接到该网络 | 应用 选择：将MAC主机连接到该网络 | 应用 目的是为了使配置生效。 创建自定义虚拟机 选择操作系统 Linux | Centos 7 64 创建完成后设置 内存：2048 MB 硬盘：40 GB 网络适配器：vmnet2 启动磁盘：CD/DVD（即设置BIOS） 安装Centos7 设置中选择 CD/DVD（IDE） 选中连接CD/DVD驱动器 选择镜像CentOS-7-x86_64-DVD-1810.iso 启动 install Centos 7 language：english date & time：Asia/shanghai installation destination：automatic partitioning selected network & hostname：ens33 | on root password：root（太短，双击确认即可） Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 16:52:11 "},"src/virtualization/vmware/VMware部署Hadoop生态环境.html":{"url":"src/virtualization/vmware/VMware部署Hadoop生态环境.html","title":"VMware部署Hadoop生态环境","keywords":"","body":"环境准备 准备三台虚拟机 ip设置 vi /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=\"static\" IPADDR=192.168.2.100 NETMASK=255.255.255.0 GATEWAY=192.168.2.2 DNS1=192.168.2.2 准备三台linux机器，IP地址分别设置成为 第一台机器IP地址：192.168.2.100 第二台机器IP地址：192.168.2.101 第三台机器IP地址：192.168.2.102 关闭防火墙 root用户下执行 systemctl stop firewalld systemctl disable firewalld 关闭selinux root用户下执行 vi /etc/selinux/config SELINUX=disabled 更改主机名 vi /etc/hostname node01 第一台主机名更改为：node01 第二台主机名更改为：node02 第三台主机名更改为：node03 更改主机名与IP地址映射 vi /etc/hosts 192.168.2.100 node01 192.168.2.101 node02 192.168.2.102 node03 同步时间 定时同步阿里云服务器时间 yum -y install ntpdate crontab -e */1 * * * * /usr/sbin/ntpdate time1.aliyun.com 添加用户 三台linux服务器统一添加普通用户hadoop，并给以sudo权限，用于以后所有的大数据软件的安装 并统一设置普通用户的密码为 hadoop useradd hadoop passwd hadoop 为普通用户添加sudo权限 visudo hadoop ALL=(ALL) ALL 定义统一目录 定义三台linux服务器软件压缩包存放目录，以及解压后安装目录，三台机器执行以下命令，创建两个文件夹，一个用于存放软件压缩包目录，一个用于存放解压后目录 # root 用户执行 mkdir -p /bigdata/soft # 软件压缩包存放目录 mkdir -p /bigdata/install # 软件解压后存放目录 chown -R hadoop:hadoop /bigdata # 将文件夹权限更改为hadoop用户 安装JDK 使用hadoop用户来重新连接三台机器，然后使用hadoop用户来安装jdk软件。上传压缩包到第一台服务器的/bigdata/soft下面，然后进行解压，配置环境变量，三台机器都依次安装 ```bash # 分别上传jdk文件 scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.100:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.101:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.102:/bigdata/soft cd /bigdata/soft/ tar -zxf jdk-8u141-linux-x64.tar.gz -C /bigdata/install/ sudo vi /etc/profile #添加以下配置内容，配置jdk环境变量 export JAVA_HOME=/bigdata/install/jdk1.8.0_141 export PATH=:$JAVA_HOME/bin:$PATH # 立即生效 source /etc/profile ``` ## hadoop用户免密码登 三台机器在hadoop用户下执行以下命令生成公钥与私钥 ```bash # 三台机器在hadoop用户下分别执行 ssh-keygen -t rsa # 三台机器拷贝公钥到node01 ssh-copy-id node01 # 在node01的hadoop用户下执行，将authorized_keys拷贝到node02和node03 # node01已经存在authorized_keys cd /home/hadoop/.ssh/ scp authorized_keys node02:$PWD scp authorized_keys node03:$PWD ``` # 安装zookeeper集群 注意：三台机器一定要保证时钟同步 zookeeper分发到node01 分发 本机执行 scp zookeeper-3.4.5-cdh5.14.2.tar.gz hadoop@192.168.2.100:/bigdata/soft 解压 node01执行 cd /bigdata/soft tar -zxvf zookeeper-3.4.5-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置文件 node01执行 cd /bigdata/install/zookeeper-3.4.5-cdh5.14.2/conf cp zoo_sample.cfg zoo.cfg mkdir -p /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas vi zoo.cfg dataDir=/bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas autopurge.snapRetainCount=3 autopurge.purgeInterval=1 server.1=node01:2888:3888 server.2=node02:2888:3888 server.3=node03:2888:3888 添加myid配置 node01执行 echo 1 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid zookeeper分发到node02和node03 分发 node01执行 scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node02:/bigdata/install/ scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node03:/bigdata/install/ 修改myid配置 node02执行 echo 2 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid node03执行 echo 3 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid 启动zookeeper服务 三台机器分别执行 /bigdata/install/zookeeper-3.4.5-cdh5.14.2/bin/zkServer.sh start # 查看启动状态 /bigdata/install/zookeeper-3.4.5-cdh5.14.2/bin/zkServer.sh status 安装hadoop CDH软件版本重新进行编译 为何编译 CDH和Apache发布包不支持C程序库。本地库可以用于支持压缩算法和c程序调用。 安装JDK 需要版本jdk1.7.0_80（1.8编译会出现错误） 安装maven # 解压缩 tar -zxvf apache-maven-3.0.5-bin.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export MAVEN_HOME=/bigdata/install/apache-maven-3.0.5 export MAVEN_OPTS=\"-Xms4096m -Xmx4096m\" export PATH=:$MAVEN_HOME/bin:$PATH # 立即生效 source /etc/profile 安装findbugs # 安装wget sudo yum install -y wget # 下载findbugs cd /bigdata/soft wget --no-check-certificate https://sourceforge.net/projects/findbugs/files/findbugs/1.3.9/findbugs-1.3.9.tar.gz/download -O findbugs-1.3.9.tar.gz # 解压findbugs tar -zxvf findbugs-1.3.9.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export FINDBUGS_HOME=/bigdata/install/findbugs-1.3.9 export PATH=:$FINDBUGS_HOME/bin:$PATH # 立即生效 source /etc/profile 安装依赖 sudo yum install -y autoconf automake libtool cmake sudo yum install -y ncurses-devel sudo yum install -y openssl-devel sudo yum install -y lzo-devel zlib-devel gcc gcc-c++ sudo yum install -y bzip2-devel 安装protobuf # 解压缩 tar -zxvf protobuf-2.5.0.tar.gz -C ../install/ # 执行 cd /bigdata/install/protobuf-2.5.0 ./configure # root 用户执行 make && make install 安装snappy # 解压缩 cd /bigdata/soft/ tar -zxf snappy-1.1.1.tar.gz -C ../install/ # 执行 cd ../install/snappy-1.1.1/ ./configure # root 用户执行 make && make install 编译 # 解压缩 tar -zxvf hadoop-2.6.0-cdh5.14.2-src.tar.gz -C ../install/ # 编译 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 # 编译不支持snappy压缩 mvn package -Pdist,native -DskipTests -Dtar # 编译支持snappy压缩 mvn package -DskipTests -Pdist,native -Dtar -Drequire.snappy -e -X 安装hadoop集群 安装环境服务部署规划 服务器IP HDFS HDFS HDFS YARN YARN 历史日志服务器 192.168.2.100 NameNode SecondaryNameNode DataNode ResourceManager NodeManager JobHistoryServer 192.168.2.101 DataNode NodeManager 192.168.2.102 DataNode NodeManager 上传压缩包并解压 重新编译之后支持snappy压缩的hadoop包上传到第一台服务器并解压 主机执行 scp hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz hadoop@192.168.2.100:/bigdata/soft node01执行 cd /bigdata/soft/ tar -zxvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C ../install/ 查看hadoop支持的压缩方式以及本地库 node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 bin/hadoop checknative 如果出现openssl为false，那么所有机器在线安装openssl su root yum -y install openssl-devel 修改配置文件 修改core-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi core-site.xml fs.defaultFS hdfs://node01:8020 hadoop.tmp.dir /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas io.file.buffer.size 4096 fs.trash.interval 10080 修改hdfs-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hdfs-site.xml dfs.hosts /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/accept_host dfs.hosts.exclude /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/deny_host --> dfs.namenode.secondary.http-address node01:50090 dfs.namenode.http-address node01:50070 dfs.namenode.name.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas dfs.datanode.data.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas dfs.namenode.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits dfs.namenode.checkpoint.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name dfs.namenode.checkpoint.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits dfs.replication 2 dfs.permissions false dfs.blocksize 134217728 修改hadoop-env.sh node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hadoop-env.sh export JAVA_HOME=/bigdata/install/jdk1.8.0_141 修改mapred-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi mapred-site.xml mapreduce.framework.name yarn mapreduce.job.ubertask.enable true mapreduce.jobhistory.address node01:10020 mapreduce.jobhistory.webapp.address node01:19888 修改yarn-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi yarn-site.xml yarn.resourcemanager.hostname node01 yarn.nodemanager.aux-services mapreduce_shuffle 修改slaves文件 node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi slaves node01 node02 node03 创建文件存放目录 node01 执行 mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits 安装包分发 node01 执行 cd /bigdata/install/ scp -r hadoop-2.6.0-cdh5.14.2/ node02:$PWD scp -r hadoop-2.6.0-cdh5.14.2/ node03:$PWD 配置hadoop的环境变量 三台机器执行 sudo vi /etc/profile export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2 export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 配置完成之后生效 source /etc/profile 集群启动 要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个集群。 注意：首次启动HDFS时，必须对其进行格式化操作。本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。 node01 执行 hdfs namenode -format 或者 hadoop namenode –format 单个节点逐一启动 # 在主节点上使用以下命令启动 HDFS NameNode: hadoop-daemon.sh start namenode # 在每个从节点上使用以下命令启动 HDFS DataNode: hadoop-daemon.sh start datanode # 在主节点上使用以下命令启动 YARN ResourceManager: yarn-daemon.sh start resourcemanager # 在每个从节点上使用以下命令启动 YARN nodemanager: yarn-daemon.sh start nodemanager # 以上脚本位于$HADOOP_PREFIX/sbin/目录下 # 如果想要停止某个节点上某个角色，只需要把命令中的start改为stop即可 脚本一键启动 如果配置了 etc/hadoop/slaves 和 ssh 免密登录，则可以使用程序脚本启动所有Hadoop两个集群的相关进程，在主节点所设定的机器上执行。 node01 执行 启动集群 start-dfs.sh start-yarn.sh mr-jobhistory-daemon.sh start historyserver 停止集群 stop-dfs.sh stop-yarn.sh mr-jobhistory-daemon.sh stop historyserver 浏览器查看启动页面 hdfs集群访问地址 http://192.168.2.100:50070/dfshealth.html#tab-overview yarn集群访问地址 http://192.168.2.100:8088/cluster jobhistory访问地址 http://192.168.2.100:19888/jobhistory Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 20:02:28 "},"src/virtualization/docker/Docker架构.html":{"url":"src/virtualization/docker/Docker架构.html","title":"Docker架构","keywords":"","body":"Docker架构 基础 什么是容器 ​ 作为容器，包含了开发、共享、部署的一切，包括代码，运行时，系统库，系统工具和相关的设置。因此，它可以快速运行，并可以在多台不同环境的计算机间无缝运行，避免解决各种因软件和硬件环境差异带来的各种开发和系统问题。 安装 ​ 登录DockerHub ​ 下载Docker.dmg，并安装 卸载 ​ 在【应用程序】卸载docker ​ 无法使用docker命令 架构 Docker使用的是client-server架构。Docker client同Docker daemon（守护进程）通信，Docker daemon负责构建、运行和分发Docker容器。Docker client和Docker daemon可以运行在同一个系统内，Docker client也可以连接到一个远程的Docker daemon。 ​ The Docker daemon ​ Docker daemon（dockerd）监听Docker API请求，管理Docker对象（镜像，容器，网络和卷）。一个daemon也可以同其他daemons通信来管理Docker服务。 ​ The Docker client ​ Docker client（docker）是Docker用户与Docker交互的主要方式。当您使用docker run等命令时，client会将这些命令发送给dockerd，dockerd会执行这些命令。docker命令使用docker API。Docker client可以与多个daemon通信。 ​ Docker registries ​ Docker registry存储Docker镜像。Docker Hub是一个公共的注册中心，任何人均可使用，Docker默认从Docker Hub中获取镜像。 ​ Docker objects ​ IMAGES 镜像 ​ 创建Docker容器的只读模板。为了创建自己的镜像，你需要创建一个Dockerfile，定义一些语法步骤，来创建和运行一个镜像。在Dockerfile中的每一个指令创建镜像中的一层。当你改变Dockerfile，然后重建镜像，仅仅改变的那些层被重建。 ​ CONTAINERS 容器 ​ 一个容器是一个镜像的运行实例。一个容器与其他容器，主机隔离。 ​ SERVICES 服务 ​ 服务允许跨多个Docker daemon来扩展容器，多个管理者和工作者作为一个集群工作。一个集群的每一个成员都是一个Docker daemon，这些daemon使用Docker API来通信。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 20:01:50 "},"src/virtualization/docker/Docker命令.html":{"url":"src/virtualization/docker/Docker命令.html","title":"Docker命令","keywords":"","body":"Docker命令 管理命令 docker builder docker checkpoint docker config docker container docker container run docker container run --publish 8000:8080 --detach --name tc ti:1.0 --detach: 在后台运行容器，打印容器ID --name: 给容器指定一个名称 --publish: 发布容器的端口到主机，要求Docker将主机8000端口的流量转发到容器的8080端口 在一个新容器中运行命令。其中，容器名称必须唯一。 docker container ls docker container ls -a -a: 显示所有容器，默认仅显示正在运行的容器 列举所有容器 docker container rm docker container rm --force tc --force: 强制删除正在运行的容器 删除一个或多个容器。 docker context docker image docker image build docker image build -t ti:1.0 . -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签 执行Dockerfile文件命令，构建镜像。成功后显示 Successfully tagged testubuntu:1.0。 docker image ls docker image ls 列举所有镜像 docker image rm docker image rm ti 删除本地镜像文件，rm后可以是镜像名称，也可以是镜像 ID。 docker image tag docker image tag ti:1.0 sciatta/ti:1.0 为镜像创建标签，创建的新标签指向的是原镜像。 在Docker Hub共享镜像，必须命名为/:的样式。 docker image pull docker image push docker image push sciatta/ti:1.0 上传镜像到仓库。 docker network docker node docker plugin docker secret docker service docker stack docker swarm docker system docker trust docker volume 命令 docker attach docker build docker commit docker cp docker create docker deploy docker diff docker events docker exec docker exec -it tc ps aux 在一个正在运行的容器内执行命令。 docker export docker history docker images docker import docker info docker inspect docker kill docker load docker login docker login 输入用户名和密码，登录到docker仓库。默认是Docker Hub。如果没有Docker ID（用户名），需要在https://hub.docker.com注册。 登录成功后显示Login Succeeded。 docker logout docker logs docker pause docker port docker ps docker pull docker push docker rename docker restart docker rm docker rmi docker run docker run -i -t sciatta/ti /bin/bash -i: 以交互模式运行容器 -t: 为容器重新分配一个伪输入终端 如果本机没有sciatta/ti镜像，从配置仓库pull镜像，同运行命令docker pull。 创建一个新的容器，同运行命令docker container create。 Docker为容器创建一个可读写的文件系统作为最后一层。允许一个正在运行的容器，创建和修改本机文件系统的文件、目录。 在没有指定网络选项时，Docker创建一个网络接口，连接容器到默认的网络，包括为容器分配一个IP地址。默认，容器可以使用主机的网络连接来连接到外部网络。 Docker启动容器，执行/bin/bash。因为容器以交互式运行，并且已经附加到终端 ，所以当日志输出到你的终端时，可以使用键盘输入。 当输入exit退出/bin/bash命令后，容器停止运行，但容器没有被删除。可以重新启动或者删除容器。 docker save docker search docker start docker stats docker stop docker tag docker top docker unpause docker update docker version docker wait Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:59:48 "},"src/virtualization/docker/Dockerfile参考.html":{"url":"src/virtualization/docker/Dockerfile参考.html","title":"Dockerfile参考","keywords":"","body":"Dockerfile参考 概述 Docker从Dockerfile文件中读取指令，自动构建镜像。 docker build 从Dockerfile和上下文构建镜像。构建上下文是在指定位置 PATH 或 URL 下的文件集合。其中，PATH是本地文件系统的目录，URL是Git仓库的位置。上下文的处理是递归的。 docker build . 以当前目录作为上下文。 构建过程是运行在 Docker daemon，而并不是CLI。构建过程的第一件事是发送整个上下文（递归）到daemon。因此，最好是把Dockerfile放置在一个空目录作为上下文，仅增加对构建需要的文件。为了使用上下文下的文件，可以使用COPY指令。为了提高构建性能，可以在上下文目录增加一个.dockerignore文件来排除文件和目录。 注释 指令不是大小写敏感的，但按照惯例，把指令全部写成大写来与参数作区分。Docker按顺序运行Dockerfile内的指令。一个Dockerfile必须以FROM指令开始（可在解析器指令、注释、全局范围参数之后）。 Docker会将以＃开头的行视为注释，除非该行是有效的解析器指令。一行中其他任何地方的＃标记均被视为参数。注释中不支持换行符。 # Comment RUN echo 'we are running some # of cool things' 第一个 # 为注释；第二个 # 被视为参数，正常输出 we are running some # of cool things 。 解析器指令 解析器指令是可选的，影响Dockerfile后续行的执行。解析器指令不会增加层，也不会显示为一个构件步骤。解析器指令被写成一个特殊的注释# directive=value。处理完注释，空行或构件指令后，Docker不再寻找解析器指令。 而是将格式化为解析器指令的任何内容都视为注释，并且不会尝试验证它是否可能是解析器指令。 因此，所有解析器指令必须位于Dockerfile的最顶部。 解析器指令不是大小写敏感的，但按照惯例，全部写成小写。约定还应在任何解析器指令之后包含一个空白行。 解析器指令不支持换行符。而非换行符的空格字符是支持的。 syntax # syntax=[remote image reference] escape # escape=\\ (backslash) 转义指令设置Dockerfile中的转义字符，如果没有指定，默认是 \\ 。 .dockerignore 文件 docker CLI 发送上下文到 docker daemon之前，会首先在上下文的root目录查找.dockerignore文件。如果存在，CLI会修改上下文，排除同模式匹配的文件和目录。这样可以避免发送不必要的大或敏感文件和目录到daemon。如果需要这些文件或目录的话，可以使用 ADD 或 COPY 指令。 CLI将.dockerignore文件解释为以换行符分隔的模式列表，类似于Unix shell的文件组。为了匹配，上下文的根被认为是工作目录和根目录。例如，模式 /foo/bar 和 foo/bar 都在PATH或位于URL的git仓库根目录的foo子目录中排除名为bar的文件或目录。 如果.dockerignore文件中第一行以 # 开始 ，则被看做是注释，在CLI解释之前被忽略。 # 注释 # comment注释 * 匹配任意字符 /temp匹配在root的直接子目录中，以temp为前缀的文件或目录 //temp* 匹配在root的二级子目录中，以temp为前缀的文件或目录 ** 匹配任意数量的目录，包括0个 */.go排除所有目录以.go结尾的文件，包括root目录 ! 排除例外 !README.md包含文件README.md ? 匹配一个字符 temp?匹配在root中以一个字符扩展temp的所有文件或目录 FROM FROM [AS ] FROM [:] [AS ] FROM [@] [AS ] 一个有效的Dockerfile文件必须包含一个 FROM 指令。 ARG 是唯一一个可以先于FROM的指令。 FROM可以在单个Dockerfile中多次出现，以创建多个映像或将一个构建阶段用作对另一个构建阶段的依赖。 只需在每个新的FROM指令之前记录一次提交输出的最后一个镜像ID。 每个FROM指令清除由先前指令创建的任何状态。 可以增加 AS 到 FROM 指令，为一个新的构建阶段提供一个可选的名称。名称可以用于后续 FROM 和 COPY --from= 指令，来在这个阶段引用镜像。 tag 和 digest 值是可选的。如果省略，构建器默认latest tag。如果没有找到tag，则返回一个错误。 ARG VERSION=latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION VERSION定义先于FROM指令，因此在FROM指令之后，ARG不可以用于其他指令。在构建阶段如果需要使用ARG的默认值，则使用ARG指令不赋予值即可。 RUN RUN 以shell方式运行，默认 /bin/sh -c； 如：RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME' RUN [\"executable\", \"param1\", \"param2\"] 以exec方式。注意双引号，JSON格式; 如：RUN [\"/bin/bash\", \"-c\", \"echo hello\"] 在当前镜像的顶层创建一个新层来执行命令，然后提交结果。 不像shell方式，以exec方式方式并不会调用命令shell。也就是说shell处理将不会发生。例如 RUN [ \"echo\", \"$HOME\" ]在 $HOME上将不会进行变量替换。如果需要进行shell处理，则以shell形式，或直接执行shell，如：RUN [ \"sh\", \"-c\", \"echo $HOME\" ]。会进行环境变量扩展，而不是docker。 CMD CMD [\"executable\",\"param1\",\"param2\"] exec方式 CMD [\"param1\",\"param2\"] ENTRYPOINT的默认参数 CMD command param1 param2 shell方式 在Dockerfile中仅可以有一条 CMD 指令。如果有多条，仅有最后一条有效。 主要目的为正在执行容器提供默认执行方式。当运行镜像时，CMD指令被执行。 运行 docker run 指定参数，将覆盖 CMD 中的默认相同参数。 在构建阶段，RUN 执行一个命令，然后提交结果；而 CMD 并不在构建阶段执行，针对镜像的预期指定命令，即在容器执行时执行。 LABEL LABEL = = = ... 为镜像增加元数据。一个LABEL是一个 key-value 对。为了在LABEL的value中包括空字符，需要使用引号和反斜杠。 一个镜像可以有多个LABEL；可以指定多个LABEL在一行。 LABEL可以包括在父镜像中，被用于继承。如果LABEL重复，后者覆盖前者。使用 docker inspect 查看LABEL。 LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" MAINTAINER (deprecated) MAINTAINER 生成镜像的作者。LABEL比MAINTAINER更灵活，建议使用LABEL替换。 例如 LABEL maintainer=\"SvenDowideit@home.org.au\" EXPOSE EXPOSE [/...] 在运行时，容器监听的指定网络端口。可以指定端口监听 TCP 或是 UDP，如果协议没有指定，则TCP是默认协议。 EXPOSE 指令实际并没有发布端口，它的主要功能是在构建镜像者和运行容器者之间作为一种规约，也就是预期要发布的端口。当运行容器时，实际要发布端口的话，使用 -p 标志发布并映射一个或多个端口，或 -P 发布所有 EXPOSE 的端口并映射到高阶端口。 EXPOSE 80/udp 默认TCP协议，可以指定协议为UDP。 不管 EXPOSE 如何设置，都可以在运行时使用 -p 标志覆盖。 ENV ENV ENV = ... 在构建阶段，环境变量可以像使用变量一样用于后续的指令中，被Dockerfile所解析。环境变量可以在Dockerfile中以$variable_name 或 ${variable_name} 方式来引用。 如果在变量前面加上转义字符 \\ ，则按字面字符解释。如 $foo 或者 ${foo}，被分别解释为 $foo 和 ${foo} 。 环境变量在运行时持续存在。可以使用docker inspect 查看。 可以在一个command中设置一个环境变量，使用 RUN = 。 ADD ADD [--chown=:] ... ADD [--chown=:] [\"\",... \"\"] 1、路径包括空字符 2、--chown 只支持UNIX系统 从复制文件，目录或者远程 URL，到镜像文件系统。 可以指定多个资源，如果它们是文件或目录，则将其路径解释为相对于构建上下文的路径。 ADD hom* /mydir/ ADD hom?.txt /mydir/ 可以是一个绝对路径，也可以是一个相对 WORKDIR 的相对路径。 ADD test relativeDir/ # adds \"test\" to `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # adds \"test\" to /absoluteDir/ 路径必须在构建上下文内部；不可使用../something /something，因为docker构建的第一步（FROM）已经把上下文目录发送到docker daemon。 如果是一个URL，并且没有以斜杠结束，则文件从URL下载并复制到。 如果是一个URL，并且以斜杠结束，则文件名按照URL进行推导，文件下载为/。如 ADD http://example.com/foobar / 将创建文件 /foobar 。 如果是一个目录，则目录的所有内容被复制，包括文件系统的元数据。注意，目录本身不会被复制，仅是内容。 如果是一个本地tar压缩格式，则被解压缩为一个目录。来自远程URL的资源不被解压缩。 如果多个资源被指定，或者是目录，或者是通配符形式，则必须是一个目录，必须以 反斜杠结尾。 如果不存在，路径所有缺失的目录被创建。 COPY COPY [--chown=:] ... COPY [--chown=:] [\"\",... \"\"] 从复制文件，目录，到容器文件系统。 ENTRYPOINT ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec form, preferred) ENTRYPOINT command param1 param2 (shell form) 配置容器，作为可执行方式运行。 以exec方式运行，命令行参数会追加到 docker run ENTRYPOINT之后，并覆盖CMD指定的元素。可以搭配 CMD 命令使用，一般是变参会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参。可以使用ENTRYPOINT的exec方式，作为稳定默认命令和参数。然后使用两种形式的CMD设置更可能更改的其他默认值。 FROM nginx ENTRYPOINT [\"nginx\", \"-c\"] # 定参 CMD [\"/etc/nginx/nginx.conf\"] # 变参 docker run nginx:test 不传参运行，启动主进程 nginx -c /etc/nginx/nginx.conf docker run nginx:test -c /etc/nginx/new.conf 传参运行，容器内会默认运行 nginx -c /etc/nginx/new.conf 允许向ENTRYPOINT传参。如 docker run -d ，将 -d 传入到ENTRYPOINT。可以使用 docker run --entrypoint 覆盖 ENTRYPOINT指令。 以Shell方式运行，可防止使用任何 CMD 或 run 命令行参数，但缺点是ENTRYPOINT将作为/bin/sh -c 的子命令启动，该子命令不传递信号。这意味着executable将不是容器的PID 1，并且不会接收Unix信号，因此executable将不会从 docker stop 接收到信号。 多个ENTRYPOINT指令，只有最后一个有效。 ENTRYPOINT和CMD交互 No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUME VOLUME [\"/var/log/\"] VOLUME /var/log or VOLUME /var/log /var/db 创建一个指定名称的挂载点，可以将主机或其他容器的目录挂载到容器中。当容器删除时，数据被持久化而不会丢失。 主机目录是在容器运行时声明的：主机目录（挂载点）从本质上说是依赖于主机的。 这是为了保证镜像的可移植性，因为不能保证给定的主机目录在所有主机上都可用。 因此，您无法从Dockerfile中挂载主机目录。VOLUME 指令不支持指定host-dir参数。 当创建或运行容器时，必须指定挂载点。 USER USER [:] USER [:] USER指令设置运行镜像时要使用的用户名（或UID）以及可选的用户组（或GID），以及Dockerfile中跟随该镜像的所有RUN，CMD和ENTRYPOINT指令。 如果用户没有组，则该镜像（或接下来指令）将用root组运行。 WORKDIR WORKDIR /path/to/workdir WORKDIR指令为Dockerfile中跟在其后的所有RUN，CMD，ENTRYPOINT，COPY和ADD指令设置工作目录。 如果WORKDIR不存在，即使后续的Dockerfile指令中未使用，WORKDIR也将被创建。 WORKDIR指令可在Dockerfile中多次使用。 如果提供了相对路径，则它将相对于上一个WORKDIR指令的路径。 WORKDIR /a WORKDIR b WORKDIR c RUN pwd 最后输出的WORKDIR是 /a/b/c 。 ARG ARG [=] ARG指令定义的变量，用户可以在构建阶段，通过 docker build 指令使用 --build-arg = 覆盖已定义的变量。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:58:59 "},"src/os/linux/Linux命令.html":{"url":"src/os/linux/Linux命令.html","title":"Linux命令","keywords":"","body":"Linux命令 A B bye C cat Concatenate FILE(s), or standard input, to standard output. cat file1 file2 > file3 1、将file1内容和file2内容输出到file3 2、“命令 > 文件”将标准输出重定向到一个文件中（清空原有文件的数据） 3、“命令 >> 文件”将标准输出重定向到一个文件中（追加到原有内容的后面） cat -n file1 file2 > file3 由1开始对所有输出的行编号，包括空行 cat -b file1 file2 > file3 由1开始对所有输出的行编号，不包括空行 cat 拷贝标准输入到标准输出 cat file1 拷贝file1到标准输出 cat file1 file2 拷贝file1和file2到标准输出 cat /dev/null > file1 1、清空file1的内容 2、dev/null：在类Unix系统中，/dev/null称空设备，它丢弃一切写入其中的数据（但报告写入操作成功）， 读取它则会立即得到一个EOF cat file1 > /dev/null 不会得到任何信息，因为将本来该通过标准输出显示的文件信息重定向到了/dev/null中 chgrp chmod chown cp Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. -a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。 -d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。 -f：覆盖已经存在的目标文件而不给出提示。 -i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答\"y\"时目标文件将被覆盖。 -p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。 -r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。 -l：不复制文件，只是生成链接文件。 cp -r test test1 复制test目录下的内容到test1 cd dirName：要切换的目标目录。可为绝对路径或相对路径。\"~\" 表示为 home 目录，\".\" 则是表示目前所在的目录，\"..\" 则表示目前目录位置的上一层目录。 cd a/b 切换当前工作目录为目标目录 cd 若目录名称省略，则变换至使用者的 home 目录 D df Show information about the file system on which each FILE resides, or all file systems by default. df -h 显示文件系统的磁盘使用情况统计 Filesystem Size Used Avail Use% Mounted on 显示文件系统、挂载点等信息 du Summarize disk usage of each FILE, recursively for directories. du -h 只显示当前目录和子目录的大小 du -h * 显示当前目录的文件大小，子目录，以及递归子目录的大小（子目录下文件不显示大小） du -sh * 仅显示当前目录的子目录和文件的总计大小 E F find search for files in a directory hierarchy find . -name \"file*\" 查找当前目录和子目录下所有匹配样式file*的文件或目录，输出路径 find . -name \"*.lastUpdated\" | xargs rm -rf tp G grep Search for PATTERN in each FILE or standard input. grep java file* 在当前目录文件名为file前缀的文件中，查找包含java的文件，并输出行内容 grep -rn java . 递归方式查找当前目录和其子目录 grep -n '2019-10-24 00:01:11' *.log H I J K L less Less is a program similar to more, but which allows backward movement in the file as well as forward movement. Also, less does not have to read the entire input file before starting, so with large input files it starts up faster than text editors like vi. G 移动到最后一行 g 移动到第一行 /字符串 向下搜索“字符串” ?字符串 向上搜索“字符串”的功能 n 重复前一个搜索（与 / 或 ? 有关） N 反向重复前一个搜索（与 / 或 ? 有关） Enter（j） 向下移动一行 k 向上移动一行 空格键（d） 向下翻一页 b 向上翻一页 q（ZZ） 退出less命令 ps -ef |less ps查看进程信息并通过less分页显示 history | less 查看命令历史使用记录并通过less分页显示 less file1 file2 1、浏览多个文件 2、输入 :n 切换到file2 :p 切换到file1 ls ls -al 默认当前目录，显示所有文件和目录，包括隐藏档 M more 空格键 向下翻一页 b 向上翻一页 more -s file1 如有连续两行以上空白行则以一行空白行显示 more +5 file1 从第5行开始显示 mv mv file1 file 移动并改名 mv ok/ no ok和no都是目录。当no不存在时，则将ok改名为no；当no存在时，将ok目录转移到no目录。 mv ok/* no ok和no都是目录。当no不存在时，失败；当no存在时，将ok目录所有内容转移到no目录。 mkdir Create the DIRECTORY(ies), if they do not already exist. mkdir [-p] dirName -p 确保目录名称存在，不存在的就建一个。 mkdir -p a/b 在当前工作目录下创建目录，父目录不存在，则一起创建 N O P pwd Print the full filename of the current working directory. pwd [--help][--version] --help 在线帮助。 --version 显示版本信息。 pwd 输出当前工作路径 Q R rcp rm Remove (unlink) the FILE(s). rm [options] name... -i 删除前逐一询问确认。 -f 即使原档案属性设为唯读，亦直接删除，无需逐一确认。 -r 将目录及以下之档案亦逐一删除。 rm services rm: remove regular file ‘services’? n 删除文件需要询问 rm -r test 删除目录 rm -rf test 强制删除所有文件和目录（即使只读），不需要询问 S ssh ssh root@172.16.92.132 1、登录远程服务器，接着输入密码 2、iTerm远程连接centos服务器，显示命令帮助为中文，而centos服务器内部可以显示英文。 centos语言为US，iTerm语言为CN，因此需要修改iTerm。 1）vim ~/.zshrc 2）末尾添加 export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8 3）source ~/.zshrc scp T touch touch services -rw-r--r--. 1 root root 670293 Dec 31 17:10 services -rw-r--r--. 1 root root 670293 Dec 31 21:07 services 文件存在，更新修改时间为当前时间 touch file1 文件不存在，创建空文件 U V W which Write the full path of COMMAND(s) to standard output. which [文件...] -n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 -p 与-n参数相同，但此处的包括了文件的路径。 -w 指定输出时栏位的宽度。 -V 显示版本信息。 which bash which指令会在环境变量$PATH设置的目录里查找符合条件的文件 which -a bash 打印所有匹配的路径（不仅只有第一个） wc Print newline, word, and byte counts for each FILE, and a total line ifmore than one FILE is specified. With no FILE, or when FILE is -,read standard input. A word is a non-zero-length sequence of charactersdelimited by white space. wc [-clw][--help][--version][文件...] -c或--bytes 只显示Bytes数。 -m或--chars 只显示字符数。 -l或--lines 只显示行数。 -w或--words 只显示单词数。 --help 在线帮助。 --version 显示版本信息。 wc file1 file2 2 3 17 file1 3 4 18 file2 5 7 35 total 行数 单词数 字节数，以及总统计值 X Y Z Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:38:51 "},"src/os/linux/iTerm的使用.html":{"url":"src/os/linux/iTerm的使用.html","title":"iTerm的使用","keywords":"","body":"iTerm的使用 快捷键 水平分隔当前屏幕 shift+command+D 垂直分隔当前屏幕 command+D 向所有TAB的所有窗口广播命令 shift+command+i 向当前TAB的所有窗口广播命令 option+command+i 将窗口分组广播命令 1、方法一 所有窗口分组 option+command+i 单独一个窗口分组 shift+control+option+command+i 2、方法二 单独一个窗口分组 shift+control+option+command+i 其他窗口分在一组 shift+control+option+command+i 恢复窗口分组 shift+option+command+i Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:57:55 "},"src/ide/intellijidea/IDEA配置Java开发环境.html":{"url":"src/ide/intellijidea/IDEA配置Java开发环境.html","title":"IDEA配置Java开发环境","keywords":"","body":"配置mvn Preferences | Build, Execution, Deployment | Build Tools | Maven 修改maven home directory /Users/yangxiaoyu/work/install/apache-maven-3.0.5 修改user setting file /Users/yangxiaoyu/work/install/apache-maven-3.0.5/conf/settings.xml 配置jdk 项目右键 | open module settings | sdks | + jdk 指定 jdk home path 项目右键 | open module settings | project 指定sdk 项目右键 | open module settings | modules | language level: 8 Preferences | Build, Execution, Deployment | compiler | java compiler module 设置为8 project bytecode version 设置为8 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 16:13:35 "},"src/ide/intellijidea/IDEA创建GitHub项目.html":{"url":"src/ide/intellijidea/IDEA创建GitHub项目.html","title":"IDEA创建GitHub项目","keywords":"","body":"GitHub空项目 在GitHub上创建空项目，如：hadoop-main IDEA创建project check out from version control | 选择 git 指定URL https://github.com/sciatta/hadoop-main.git 和本地目录 | clone | 选择 create project from existing sources 后续默认选择 初始化project git flow init 后续在develop分支开发 IDEA创建module 创建 parent pom module hadoop-main | new module | maven groupid: com.sciatta.hadoop artifact: hadoop-main module name: hadoop-main # hadoop-main根目录作为module parent content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main # 修改pom.xml pom 创建child pom module hadoop-main | new module | maven parent: hadoop-main add as module to: hadoop-main artifact: hadoop-hdfs module name: hadoop-hdfs content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs # 修改pom.xml pom 创建child jar module hadoop-hdfs | new module | maven parent: hadoop-hdfs add as module to: hadoop-hdfs artifact: hadoop-hdfs-example module name: hadoop-hdfs-example content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs/hadoop-hdfs-example # 修改pom.xml jar Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 20:17:03 "}}