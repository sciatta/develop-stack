{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-19 17:49:23 "},"src/bigdata/hadoop/hdfs/HDFS命令.html":{"url":"src/bigdata/hadoop/hdfs/HDFS命令.html","title":"HDFS命令","keywords":"","body":"hdfs hdfs dfs hdfs有两种命令风格 hadoop fs hdfs dfs 两种命令等价 help hadoop fs -help ls hdfs dfs -help ls ls # 查看根目录文件列表 hdfs dfs -ls / # 递归显示目录内容 hdfs dfs -ls -R / # 显示本地文件系统列表，默认hdfs # file:// 表示本地文件协议 hdfs dfs -ls file:///bigdata/ touchz # 创建空文件 hdfs dfs -touchz /mynote appendToFile # 向文件末尾追加内容 # 注意命令区分大小写 hdfs dfs -appendToFile hello /mynote cat # 查看文件内容 hdfs dfs -cat /mynote put # 上传本地文件到hdfs hdfs dfs -put hello /h1 copyFromLocal（同put） hdfs dfs -copyFromLocal hello /h2 moveFromLocal # 上传成功，删除本机文件 hdfs dfs -moveFromLocal hello /h3 get # 下载hdfs文件到本地 hdfs dfs -get /h1 hello copyToLocal（同get） hdfs dfs -copyToLocal /h1 hello1 mkdir # 创建目录 hdfs dfs -mkdir /shell rm # 删除文件到垃圾桶 # 不能删除目录 hdfs dfs -rm /h1 # 递归删除目录 hdfs dfs -rm -r /shell rmr # 递归删除目录 # 不建议使用，可使用 rm -r 代替 hdfs dfs -rmr /hello mv # 目的文件或目录不存在，修改文件名或目录名 # 目的文件存在，不可移动 hdfs dfs -mv /h2 /h22 # 目的目录存在，将子文件或子目录移动到目录 hdfs dfs -mv /h22 /hello cp # 拷贝文件 # 若目的文件存在，不可复制 hdfs dfs -cp /bigf /bf # 拷贝文件到目的目录 hdfs dfs -cp /bigf /hello find # 查找文件 hdfs dfs -find / -name \"h*\" text # 查看文件内容，若为SequenceFile，即使压缩，也可以正常查看压缩前内容 hdfs dfs -text /sequence/none expunge # 清空回收站，同时创建回收站checkpoint hdfs dfs -expunge hdfs getconf namenodes # 获取NameNode节点名称，可能有多个 hdfs getconf -namenodes confKey # 用相同的命令可以获得其他属性值 # 获取最小块大小 默认1048576byte（1M） hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size nnRpcAddresses # 获取NameNode的RPC地址 hdfs getconf -nnRpcAddresses hdfs dfsadmin safemode # 查看当前安全模式状态 hdfs dfsadmin -safemode get # 进入安全模式 # 安全模式只读 # 增删改不可以，查可以 hdfs dfsadmin -safemode enter # 退出安全模式 hdfs dfsadmin -safemode leave allowSnapshot 快照顾名思义，就是相当于对我们的hdfs文件系统做一个备份，我们可以通过快照对我们指定的文件夹设置备份，但是添加快照之后，并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 # 创建快照之前，先要允许该目录创建快照 hdfs dfsadmin -allowSnapshot /main # 禁用 hdfs dfsadmin -disallowSnapshot /main # 指定目录创建快照 # Created snapshot /main/.snapshot/s20200113-114345.126 # 可以通过浏览器访问 http://192.168.2.100:50070/explorer.html#/main/.snapshot/s20200113-114345.126 hdfs dfs -createSnapshot /main # 创建快照指定名称 hdfs dfs -createSnapshot /main snap1 # 快照重命名 hdfs dfs -renameSnapshot /main snap1 snap2 # 列出当前用户下的所有快照目录 hdfs lsSnapshottableDir # 比较两个快照的不同 hdfs snapshotDiff /main snap1 snap2 # 删除快照 hdfs dfs -deleteSnapshot /main snap1 hdfs fsck # 查看文件的文件、块、位置信息 hdfs fsck /h3 -files -blocks -locations hdfs namenode format # 格式化NameNode，只在初次搭建集群时使用 hdfs namenode -format hdfs oiv # 查看fsimage内容 offine image view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oiv -i fsimage_0000000000000000196 -p XML -o test.xml hdfs oev # 查看edits内容 offine edits view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oev -i edits_0000000000000000001-0000000000000000009 -p XML -o test.xml hadoop hadoop fs（同hdfs dfs） hadoop checknative # 查看本地库安装状态 hadoop checknative hadoop jar # 在集群上执行自定义的jar hadoop jar hadoop-mapreduce-wordcount-1.0-SNAPSHOT.jar com.sciatta.hadoop.mapreduce.wordcount.WordCount hadoop archive # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main hadoop distcp # hadoop 集群间数据拷贝 hadoop distcp hdfs://node01:8020/test hdfs://cluster:8020/ Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 15:32:50 "},"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html":{"url":"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html","title":"HDFS核心功能原理","keywords":"","body":"hdfs优缺点 优点 高容错性 数据自动保存多个副本； 某一个副本丢失后，可以自动恢复。 适合批处理 它是通过移动计算而不是移动数据； 它会把数据位置暴露给计算框架。 适合大数据处理 数据规模，可以处理数据规模达到GB、TB、甚至PB级别的数据； 文件规模，能够处理百万级别规模以上的文件数量； 节点规模：能够处理10K节点的规模。 流式数据访问 一次写入，多次读取，不能修改，只能追加； 它能保证数据的一致性。 可构建在廉价机器上，通过多副本机制，提高可靠性 它通过多副本机制，提高可靠性； 它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 缺点 不适合低延时数据访问，比如毫秒级的数据访问 不适合毫秒级的数据访问； 它适合高吞吐率的场景，就是在某一时间内写入大量的数据。 无法高效的对大量小文件进行存储 存储大量小文件，会占用NameNone大量内存存储元数据。但NameNode的内存总是有限的； 小文件存储的寻址时间会超过读取时间，违反了HDFS的设计目标。 不支持并发写入、文件随机修改 一个文件只能有一个线程写，不允许多个线程多时写； 仅支持数据append，不支持文件的随机修改。 hdfs写入流程 概述 写入本地file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode向HDFS写入数据，先调用DistributedFileSystem的 create 方法获取FSDataOutputStream。 DistributedFileSystem调用NameNode的 create 方法，发出文件创建请求。NameNode对待上传文件名称和路径做检验，如上传文件是否已存在同名目录，文件是否已经存在，递归创建文件的父目录（如不存在）等。并将操作记录在edits文件中。 ClientNode调用FSDataOutputStream向输出流输出数据（假设先写block1）。 FSDataOutputStream调用NameNode的 addBlock 方法申请block1的blockId和block要存储在哪几个DataNode（假设DataNode1，DataNode2和DataNode3）。若pipeline还没有建立，则根据位置信息建立pipeline。 同返回的第一个DataNode节点DataNode1建立socket连接，向其发送package。同时，此package会保存一份到ackqueue确认队列中。 写数据时先将数据写到一个校验块chunk中，写满512字节，对chunk计算校验和checksum值（4字节）。 以带校验和的checksum为单位向本地缓存输出数据（本地缓存占9个chunk），本地缓存满了向package输入数据，一个package占64kb。 当package写满后，将package写入dataqueue数据队列中。 将package从dataqueue数据对列中取出，沿pipeline发送到DataNode1，DataNode1保存，然后将package发送到DataNode2，DataNode2保存，再向DataNode3发送package。DataNode3接收到package，然后保存。 package到达DataNode3后做校验，将校验结果逆着pipeline回传给ClientNode。 DataNode3将校验结果传给DataNode2，DataNode2做校验后将校验结果传给DataNode1，DataNode1做校验后将校验结果传给ClientNode。 ClientNode根据校验结果判断，如果”成功“，则将ackqueue确认队列中的package删除；如果”失败“，则将ackqueue确认队列中的package取出，重新放入到dataqueue数据队列末尾，等待重新沿pipeline发送。 当block1的所有package发送完毕。即DataNode1、DataNode2和DataNode3都存在block1的完整副本，则三个DataNode分别调用NameNode的 blockReceivedAndDeleted方法。NameNode会更新内存中DataNode和block的关系。 ClientNode关闭同DataNode建立的pipeline。 文件仍存在未发送的block2，则继续执行4。直到文件所有数据传输完成。 全部数据输出完成，调用FSDataOutputStream的 close 方法。 ClientNode调用NameNode的 complete 方法，通知NameNode全部数据输出完成。 容错 假设当前构建的pipeline是DataNode1、DataNode2和DataNode3。当数据传输过程中，DataNode2中断无法响应，则当前pipeline中断，需要重建。 先将ackqueue中的所有package取出放回到dataqueue末尾。 ClientNode调用NameNode的 updateBlockForPipeline 方法，为当前block生成新的版本，如ts1（本质是时间戳），然后将故障DataNode2从pipeline中删除。 FSDataOutputStream调用NameNode的 getAdditionalDataNode 方法，由NameNode分配新的DataNode，假设是DataNode4。 FSDataOutputStream把DataNode1、DataNode3和DataNode4建立新的pipeline，DataNode1和DataNode3上的block版本设置为ts1，通知DataNode1或DataNode3将block拷贝到DataNode4。 新的pipeline创建好后，FSDataOutputStream调用NameNode的 updataPipeline 方法更新NameNode元数据。之后，按照正常的写入流程完成数据输出。 后续，当DataNode2从故障中恢复。DataNode2向NameNode报送所有block信息，NameNode发现block为旧版本（非ts1），则通过DataNode2的心跳返回通知DataNode2将此旧版本的block删除。 分析源码 解惑 [x] Lease（租约） 在HDFS写文件中是通过Lease（租约）来维护写文件凭证的，所以得到一个文件的写权限之后将其租约进行存储并定时更新。 [ ] block、package数据格式，如何校验（TCP协议，为什么会出现丢包），输出的基本单位（package），如何输出 [ ] hdfs block的上限是多少？ hdfs读取流程 概述 获取file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode调用DistributedFileSystem的 open 方法获取FSDataInputStream。 DistributedFileSystem向NameNode发出请求获取file文件的元数据，包括所有块所在的DataNode的位置信息。 ClientNode调用FSDataInputStream获取数据流。 FSDataInputStream调用就近DanaNode获取block1。DanaNode开始传输数据给客户端，从磁盘里面读取数据输入流，以Packet为单位来做校验。ClientNode以Packet为单位接收数据，先在本地缓存，然后写入目标文件。 文件仍存在未读取的block2，则继续执行4。直到文件所有数据读取完成。 全部数据接收完成，关闭数据流FSDataInputStream。 分析源码 解惑 [ ] 文件块信息LocatedBlocks包含哪些重要信息？ [ ] 重传机制 [ ] DataNode优先位置策略 [ ] 数据的版本问题 NN和SNN功能剖析 概述 NameNode对集群中元数据进行管理，外围节点需要频繁随机存取元数据。 如何支持快速随机存取？因此需要把元数据存储在内存中。但内存中的数据在服务器断电后就会丢失，所以内存中的元数据需要被持久化。 持久化哪里？持久化到文件系统中，存储为fsimage文件。随着时间的流逝，fsimage文件会变得越来越庞大，同时对内存和fsimage元数据的增、删、改、查操作，fsimage文件的大小就会成为存取速度的瓶颈。 如何优化？引入edits日志文件。fsimage为某一时间节点的全量元数据，而edits日志为最新元数据。也就是说，Namenode同时对内存和edits日志进行操作。 之后又会出现edits日志越来越大，以及如何同fsimage合并的问题？系统引入了SecondNameNode，其负责将edits日志和fsimage合并，然后将最新的fsimage推送给NameNode，而NameNode则是向最新生成的edits日志文件写入元数据。 工作机制 如果NameNode是首次启动，则需要格式化HDFS，生成fsimage和edits；否则，启动时读取fsimage和edits到内存中初始化元数据。 ClientNode向NameNode发起元数据操作请求（增删改查），NameNode将元数据先写入edits（防止NameNode挂掉，内存中元数据丢失导致客户端无法访问数据），再写入内存中。 SecondNameNode向NameNode定时发起请求确认是否需要checkpoint。如果满足到达设置定时时间间隔或edits文件写满，则发起checkpoint请求；否则，继续等待。 checkpoint需满足条件： 时间达到一个小时fsimage与edits就会进行合并 dfs.namenode.checkpoint.period 3600s hdfs操作达到1000000次也会进行合并 dfs.namenode.checkpoint.txns 1000000 检查间隔 每隔多长时间检查一次hdfs dfs.namenode.checkpoint.check.period 60s 请求执行check point。 NameNode滚动生成新的edits.new文件，后续ClientNode对元数据操作请求都记录到edits.new文件中。 SecondNameNode通过http get获取NameNode滚动前的edits和fsimage文件。 SecondNameNode将fsimage读入内存，逐条执行edits，合并生成fsimage.ckpt文件。 SecondNameNode通过http post将fsimage.ckpt文件发送到NameNode上。 NameNode将fsimage.ckpt改名为fsimage（此文件为此刻全量元数据，待后续NameNode重启加载），将edits.new改名为edits。同时，会更新fstime。 解惑 [ ] 如何回放？ [ ] NameNode内存不够，如何解决？ 联邦机制（什么是？） 增加物理内存。 DataNode工作机制和数据存储 工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件：一个是数据本身，一个是元数据包括数据块的长度，数据块的校验和，以及时间戳。 DataNode启动后向NameNode注册，通过后则周期性（1小时）的向NameNode上报所有的块信息。 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令。如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器。 数据完整性 当Client读取DataNode上block的时候，会计算checksum。如果计算后的checksum，与block创建时值不一样，说明block已经损坏。这时Client需要读取其他DataNode上的block。 DataNode在其文件创建后周期验证checksum。 掉线参数设置 DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval dfs.namenode.heartbeat.recheck-interval 300000ms dfs.heartbeat.interval 3s 解惑 [x] 一个DataNode上存储的所有数据块可以有相同的，为什么？ 一般出于安全性和高可用性考虑，并不会把一个block的多个副本放在同一个datanode上。但也不是绝对。例如，三个datanode，副本默认是三个的话，那么正常来说，每个节点上存储一个block副本是最好的（安全、可靠性高，单节点出现问题，并不会丢失数据），如果把3个副本都放在一个节点上，一旦这个节点出现问题，数据就可能丢失了；如果副本数是5个的话，那么就存在同一个datanode有多个副本了，即副本数比datanode数多的时候，必然存在一个datanode上存放多个相同block了。 小文件治理 概述 hdfs中文件以block存储在DataNode中，而所有文件的元数据全部存储在NameNode的内存中。无论文件大小，都会占用NameNode元数据的内存存储空间，大约占用150K左右。所以，系统中如果有大量小文件的话，会出现DataNode的磁盘容量没有充分利用，而NameNode的内存却被大量消耗，然而NameNode的内存是有容量限制的。所以，需要对小文件治理。 HAR方案 本质启动MapReduce，因此需要首先启动Yarn。 创建归档文件 # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main # 源文件不会删除 查看归档文件 # 显示归档包含文件 # 归档文件的类型是d，即目录 # -R 递归列出目录内容 hdfs dfs -ls -R /main/data.har # 显示归档包含实际内容 hdfs dfs -ls -R har:///main/data.har 解压归档文件 # -p 如果目录已经存在，不会失败 hdfs dfs -mkdir -p /main/out hdfs dfs -cp har:///main/data.har/* /main/out SequenceFile方案 SequenceFile是由record构成，每个record是由键值对构成，其中文件名作为record的key，而文件内容作为record的value。Record间随机插入Sync，方便定位到Record的边界。 SequenceFile是可以分割的，所以可以利用MapReduce切分，独立运算。 HAR不支持压缩，而SequenceFile支持压缩。支持两类压缩： Record压缩 Block压缩，一次性压缩多条Record作为一个Block；每个Block开始处都需要插入Sync 当不指定压缩算法时，默认使用zlib压缩 无论是否压缩，采用何种算法，均可使用 hdfs dfs -text 命令查看文件内容 一般情况下，以Block压缩为最好选择。因为一个Block包含多条Record，利用Record间的相似性进行压缩，压缩效率更高。 把已有小文件转存为SequenceFile较慢，相比先写小文件，再写SequenceFile而言，直接将数据写入SequenceFile是更好的选择，省去小文件作为中间媒介。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 14:55:45 "},"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html":{"url":"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html","title":"MapReduce工作原理","keywords":"","body":"概述 MapReduce的核心思想是“分而治之”，把大任务分解为小任务并行计算。Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。Reduce负责“合”，即对Map阶段的结果进行全局汇总。 编程模型 Map阶段 第一步：InputFormat 设置InputFormat类，以split作为输入单位，将数据切分成 (key, value) 输出。本质是把大任务拆分为互相独立的小任务。如TextInputFormat，输入的是split（默认大小和block一样），输出的key是行偏移位置，value是每行的内容。 InputFormat 描述 TextInputFormat 1、默认将每个block作为一个split；2、输出的key是行偏移位置，value是每行的内容 CombineTextInputFormat 1、解决小文件导致过多split的问题。涉及到虚拟存储和切片过程，可以自定义split大小；2、输出的key是行偏移位置，value是每行的内容 KeyValueTextInputFormat 1、默认将每个block作为一个split；2、以自定义分隔符进行分割，输出相应的key和value NLineInputFormat 1、以输入文件的N行作为一个split；2、输出的key是行偏移位置，value是每行的内容 InputFormat输入格式类 InputSplit输入分片类：InputFormat输入格式类将输入文件分成一个个分片InputSplit；每个MapTask对应一个split分片 RecordReader记录读取器类：：读取分片数据，一行记录生成一个键值对 第二步：MapTask 自定义map逻辑，对输入的 (key, value) 进行处理，转换成新的 (key, value) 进行输出。本质是把小任务拆分为最小可计算任务。例如统计单词数量，MapTask输入的key是行偏移位置，value是每行的内容。可以进一步拆分单行的内容，输出的key是一个单词，value是1。后面把所有相同单词的value累加，即为一个单词出现的数量。最后汇总所有单词即可。 Shuffle阶段 第三步：Partition 对输入的 (key, value) 进行分区。满足条件的key划分到一个分区中，一个分区发送到一个ReduceTask。 如果指定6个分区，而ReduceTask的个数是3，则会出现异常； 如果指定6个分区，而ReduceTask的个数是9，则后3个ReduceTask没有数据输入。 第四步：Order 排序是MapReduce框架中最重要的操作之一。 MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 各种排序的分类： 部分排序 MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序 全排序 最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构 辅助排序 在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。 二次排序 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 第五步：Combine 对分组后的数据进行规约(combine操作)，降低数据的网络拷贝（可选步骤） Combiner的父类是Reducer，区别在于Combiner是在MapTask节点运行，而Reduce在ReduceTask节点运行，接收全局所有MapTask的输出结果。Combiner的意义在于对MapTask的输出做局部汇总，减少网络传输量。但Combiner应用的前提是不影响最终的业务逻辑。 第六步：Group 对排序后的数据进行分组，将相同key的value放到一个集合当中。 GroupingComparator是MapReduce当中reduce端的一个功能组件，主要的作用是决定哪些数据作为一组，调用一次reduce的逻辑，默认是每个不同的key作为多个不同的组，每个组调用一次reduce逻辑，我们可以自定义GroupingComparator实现不同的key作为同一个组，调用一次reduce逻辑。 Reduce阶段 第七步：ReduceTask 自定义Reduce逻辑，对输入的 (key, value) 进行处理，转换成新的 (key, value) 进行输出 第八步：OutputFormat 设置Outputformat将输出的 (key, value) 保存到文件中 MapTask工作机制 MapTask个数 MapTask的并行度是由什么决定的？ 在MapReduce中每个MapTask处理一个切片split的数据量，注意block是hdfs系统存储数据的单位，而切片是每个MapTask处理数据量单位。 block：hdfs在物理上把文件数据切分成一块一块。 split：是逻辑上对输入进行切片，并不会影响磁盘文件。 一个job的Map阶段的并行度是由客户端提交job时的切片数决定的 每一个切片分配一个MapTask并行处理 默认情况下，切片大小和block大小相等 切片时不考虑数据整体，而是针对单个文件单独切片 切片大小的计算公式：Math.max(minSize, Math.min(maxSize, blockSize)); 其中， mapreduce.input.fileinputformat.split.minsize=1 mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue block的默认大小是128M，所以split的默认大小是128M，同block的大小一致。 如果要控制split的数量，则只需要改变minsize或maxsize就可以改变切片的大小。如果自定义split大于128M，minsize要大于128M；如果自定义split小于128M，maxsize要小于128M。 如果有1000个小文件，每个文件在1K-100M之间，默认情况下有1000个block，1000个split，1000个MapTask并行处理，效率如何？ 默认情况下，使用的TextInputFormat按照文件规划切片，不管文件多小（小于128M）都会作为一个单独的切片，启动一个MapTask处理。而启动MapTask所消耗的资源要远大于计算，因此采用TextInputFormat效率极低。通过CombineTextInputFormat来控制小文件的切片数量，可以在逻辑上将多个小文件规划到一个切片中，从而控制MapTask的数量。 CombineTextInputFormat切片机制： CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4m 切片过程包括虚拟存储和切片两步： 虚拟存储：将所有小文件依次同MaxInputSplitSize参数作比较，使得最后划分的逻辑块都不大于MaxInputSplitSize 小于参数，则直接作为一个逻辑块 大于参数，且不大于2倍参数，则均分作为两个逻辑块（防止出现太小的切片） 大于2倍参数，则先按MaxInputSplitSize参数切割一个逻辑块，剩下的继续1、2、3步判断 如：MaxInputSplitSize设置为4M，输入文件大小为8.02M，满足3，则先划分一个4M的逻辑块，剩下人4.02继续判断；4.02满足2，则均分为两个2.01M的逻辑块。 切片：将所有虚拟存储逻辑块依次同MaxInputSplitSize参数作比较，使得最后划分的切片接近MaxInputSplitSize 如果大于等于 MaxInputSplitSize ，则作为一个切片。继续下一个逻辑块开始判断 如果小于 MaxInputSplitSize ，则同下一个虚拟存储逻辑块一起作为一个切片。继续1、2步判断，直到满足条件1或所有虚拟存储逻辑块合并完成 如：MaxInputSplitSize设置为4M，有4个小文件大小：1.7M、5.1M、3.4M以及6.8M 虚拟存储为6个逻辑块：1.7M、（2.55M+2.55M）、3.4M、（3.4M+3.4M） 合并为3个切片：（1.7M+2.55M=4.25M）、（2.55M+3.4M=5.95M）、（3.4M+3.4M=6.8M） 测试场景 MaxInputSplitSize = 4M 测试用例 虚拟存储 切片 MapTask 10个文件：0.1K 10个0.1k的逻辑块 1k（只有一个切片，因为所有文件合并后仍小于4M） 1 2个文件：8.1M、8K （4M+2.05M+2.05M）、8K 4M、（2.05M+2.05M）、8K 3 工作机制 Read阶段：InputFormat以split作为输入单位，将数据切分成 (k1, v1) 输出。 Map阶段：将 (k1, v1) 交给用户编写map()函数处理，并产生 (k2, v2) 输出。 Collect阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的 (k2, v2) 分区（调用Partitioner），并写入一个环形内存缓冲区中。 Spill阶段：即“溢写”，当环形缓冲区满80%后，MapTask会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作； 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销） 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 ReduceTask工作机制 ReduceTask个数 ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置： // 默认值是1，手动设置为4 job.setNumReduceTasks(4); // 可以设置为0，不需要ReduceTask处理 job.setNumReduceTasks(0); 工作机制 Copy阶段：ReduceTask启动线程从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 Sort：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。注意，Merge和Sort两个阶段，交替进行。 Reduce阶段：按照key排序的数据，调用GroupingComparator对数据分组，使得相同key的不同value放到一个集合中，每一组数据 (k2, v2) 只调用一次reduce()函数，输出 (k3, v3) 。 Write阶段：最后由OutputFormat将计算结果写到HDFS上。一个ReduceTask对应一个文件。 Shuffle中的数据压缩 压缩算法 在shuffle阶段，从map阶段输出的数据，都要通过网络拷贝发送到reduce阶段。这一过程，涉及到大量的网络IO，如果数据能够进行压缩，那么数据的发送量就会少很多。 hadoop支持的压缩算法 压缩格式 工具 算法 文件扩展名 是否可切分 DEFLATE 无 DEFLATE .deflate 否 Gzip gzip DEFLATE .gz 否 bzip2 bzip2 bzip2 bz2 是 LZO lzop LZO .lzo 否 LZ4 无 LZ4 .lz4 否 Snappy 无 Snappy .snappy 否 各种压缩算法对应使用的java类 压缩格式 java类 DEFLATE org.apache.hadoop.io.compress.DeFaultCodec Gzip org.apache.hadoop.io.compress.GZipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec LZ4 org.apache.hadoop.io.compress.Lz4Codec Snappy org.apache.hadoop.io.compress.SnappyCodec 常见的压缩速率比较 压缩算法 原始文件大小 压缩后的文件大小 压缩速度 解压缩速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO-bset 8.3GB 2GB 4MB/s 60.6MB/s LZO 8.3GB 2.9GB 135 MB/s 410 MB/s snappy 8.3GB 1.8GB 172MB/s 409MB/s 常用的压缩算法主要有 LZO 和 snappy 等。 启用压缩 编码中设置 Map阶段压缩 Configuration configuration = new Configuration(); configuration.set(\"mapreduce.map.output.compress\",\"true\"); configuration.set(\"mapreduce.map.output.compress.codec\",\"org.apache.hadoop.io.compress.SnappyCodec\"); Reduce阶段压缩 configuration.set(\"mapreduce.output.fileoutputformat.compress\",\"true\"); configuration.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\"); configuration.set(\"mapreduce.output.fileoutputformat.compress.codec\",\"org.apache.hadoop.io.compress.SnappyCodec\"); 修改mapred-site.xml（全局） Map阶段压缩 mapreduce.map.output.compress true mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.SnappyCodec Reduce阶段压缩 mapreduce.output.fileoutputformat.compress true mapreduce.output.fileoutputformat.compress.type RECORD mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress.SnappyCodec 注意，所有节点都需要修改mapred-site.xml。修改后，重启集群生效。 数据倾斜 什么是数据倾斜 数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。 常见的数据倾斜有以下几类： 数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。 数据大小倾斜——部分记录的大小远远大于平均值。 在map端和reduce端都有可能发生数据倾斜。 在map端的数据倾斜可以考虑使用combine：导致磁盘IO和网络IO过大 在reduce端的数据倾斜常常来源于MapReduce的默认分区器：ReduceTask繁忙空闲严重不均 数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源。 如何诊断哪些键存在数据倾斜 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的最大值。 为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。 运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础 解决数据倾斜 Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键 如何减小reduce端数据倾斜的性能损失？常用方式有： 自定义分区 基于输出键的背景知识进行自定义分区。 例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。 Combine 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。 combine的目的就是在Map端聚合并精简数据。 抽样和范围分区 Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。在有数据倾斜时就很有问题。 使用分区器需要首先了解数据的特性。TotalOrderPartitioner 中，可以通过对原始数据进行抽样得到的结果集来预设分区边界值。 TotalOrderPartitioner 中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。 数据大小倾斜的自定义策略 在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。处理这种情况并不容易。可以参考以下方法：设置mapreduce.input.linerecordreader.line.maxlength 来限制RecordReader读取的最大长度。RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。 MapReduce中的join操作 Order订单表 id date pid amount 1001 20150710 P0003 2 1002 20150710 P0002 3 1002 20150710 P0003 3 Product产品表 id name categoryid price P0001 xiaomi 1000 2000 P0002 apple 1000 5000 P0003 samsung 1000 3000 产品表和订单表是一对多关系，即一个产品可以属于多个订单，而一个订单只能有一个产品（唯一产品id，多个相同产品由amount字段确定）。两张表的数据以文件形式存储在hdfs上，且数据量非常大。现需要用MapReduce程序来实现SQL查询运算。 select o.id,o.date,p.name,p.categoryid,p.price from order o join product p on o.pid = p.id reduce端的join操作 两张表首先会划分为多个split输入，启动split数量的MapTask。把两张表的产品id放到Mapper的key中，把产品记录或订单记录放在Mapper的value中。经过Shuffle阶段，会将与产品id关联的订单和产品记录放到一个集合中作为value送到Reducer中，然后遍历集合将订单和产品合并。 假设，产品表相对较小，而在订单表中99%的订单都包含一种相同产品，对于多个并行计算的ReduceTask，则会出现99%的数据发送到同一个ReduceTask，造成其非常繁忙，而其他启动的ReduceTask则有可能出现空闲。 这就是数据倾斜 问题。并行计算力无法充分利用。为解决问题，可以自定义Partitioner，将数据尽量均匀的分区到不同的ReduceTask。注意，之所以出现数据倾斜，是由于在reduce端出现的问题，因此可以省略reduce一步，直接在map端完成数据的合并工作。即map端的join操作。 map端的join操作 适用于关联小表情况，在程序初始化时保存全局hdfs缓存文件路径，后续在mapper启动时一次读取小表数据放入缓存。这样，就可以在map阶段join操作，完全利用MapTask的并发算力，快速完成join操作。 注意，在不设置ReduceTask的情况下，默认仍有一个ReduceTask，可以观察输出文件 part-r-00000 第二位是r，表示由ReduceTask输出；因为所有逻辑都在MapTask中完成，不需要ReduceTask，因此设置 job.setNumReduceTasks(0); ，可以观察输出文件 part-m-00000 第二位是m，表示由MapTask输出。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-01 15:34:32 "},"src/bigdata/hadoop/yarn/YARN架构.html":{"url":"src/bigdata/hadoop/yarn/YARN架构.html","title":"YARN架构","keywords":"","body":"YARN架构 YARN 的全称是 Yet Another Resource Negotiator，YARN 是经典的主从 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave。 ResourceManager（RM） RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，主要有两个组件构成： 调度器：Scheduler； 调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。 应用程序管理器：Applications Manager，ASM。 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动 AM、监控 AM 运行状态并在失败时重新启动它等。 NodeManager（NM） NM 是每个节点上运行的资源和任务管理器。 它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态； 它接收并处理来自 AM 和 RM 的请求（Container 启动/停止）。 ApplicationMaster（AM） 提交的每个作业都会包含一个 AM，主要功能包括： 与 RM 协商以获取资源（用 container 表示）； 将得到的任务进一步分配给内部的任务； 与 NM 通信以启动/停止任务； 监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。 MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。 Container Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，以及环境变量、启动命令等任务运行相关的信息。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。 作业提交流程 client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 。 新的作业ID（应用ID）和 资源提交路径 由 RM 分配。 client 核实作业的输出，计算输入的split，将作业的资源（包括Jar包、配置文件、split信息）拷贝给 HDFS。 client 调用 RM 的 submitApplication() 来提交作业。 当 RM 收到 submitApplication() 的请求时，就将该请求发给 scheduler，scheduler 分配 container，并与对应的 NM 通信，要求其在这个 container 中启动 AM。 MapReduce 作业的 AM 是一个主类为 MRAppMaster 的 Java 应用，其通过构造一些 bookkeeping 对象来监控作业的进度，得到任务的进度和完成报告。 MRAppMaster 通过 HDFS 得到由 client 计算好的输入 split。然后为每个输入 split 创建 MapTask，根据 mapreduce.job.reduces 创建 ReduceTask。 如果作业很小，AM会选择在其自己的 JVM 中运行任务；如果不是小作业，那么 AM 向 RM 请求 container 来运行所有的 MapTask 和 ReduceTask。请求是通过心跳来传输的，包括每个 MapTask 的数据位置（如存放输入split的主机名和机架）。scheduler 利用这些信息来调度任务，尽量将任务分配给存储数据的节点，或退而分配给同机架的其他节点。 当一个任务由 RM 调度分配一个 container 后，AM与该 NM 通信，要求其在这个 container 中启动任务。其中，任务是一个主类为 YarnChild 的 Java 应用执行。 YarnChild 在运行任务之前首先本地化任务需要的资源，如作业配置、JAR文件、以及HDFS的所有文件。将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 YarnChild 运行 MapTask 或 ReduceTask。YarnChild运行在一个专用的JVM中，但是YARN不支持JVM重用。 YARN中的任务将其进度和状态返回给AM，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。同时，client通过 mapreduce.client.progressmonitor.pollinterval 向AM请求进度更新，向用户展示。 应用程序运行完成后，AM 向 RM 请求注销并关闭自己。 容错 对于分布式系统，不论是调度系统还是其他系统，容错机制都是非常必要的，YARN 需要容错的地方，有以下四个地方： ApplicationMaster 容错：ResourceManager 会和 ApplicationMaster 保持通信，一旦发现 ApplicationMaster 失败或者超时，会为其重新分配资源并重启。重启后 ApplicationMaster 的运行状态需要自己恢复，如 MRAppMaster 会把相关状态记录到 HDFS 上，重启后从 HDFS 读取运行状态恢复； NodeManager 容错：NodeManager 如果超时，则 ResourceManager 会认为它失败，将其上的所有 container 标记为失败并通知相应的 ApplicationMaster，由 AM 决定如何处理（可以重新分配资源运行某一个任务，也可以整个作业失败，重新运行所有任务）； container 容错：如果 ApplicationMaster 在一定时间内未启动分配的 container，RM 会将其收回；如果 Container 运行失败，RM 会通知对应的 AM 由其处理； ResourceManager 容错：RM 采用 HA 机制。 任务调度器 资源调度器是YARN最核心的组件之一，是一个插拔式的服务组件，负责整个集群资源的管理和分配。YARN提供了三种可用的资源调度器：FIFO、Capacity Scheduler、Fair Scheduler。 先进先出调度器（FIFO Scheduler） FIFO 是最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线性队列相应的位置，在资源调度时，按照队列的先后顺序、先进先出地进行调度和资源分配。很明显这种调度器过于简单，在实际的生产中，应用不是很多，毕竟需要调度的作业是有不同优先级的。 容量调度器（Capacity Scheduler） 容量调度器是 Yahoo 为 Hadoop 开发的多用户调度器，应用于用户量众多的应用场景，与公平调度器相比，其更强调资源在用户之间而非作业之间的公平性。 它将用户和任务组织成多个队列，每个队列可以设定资源最低保障和使用上限，当一个队列的资源有剩余时，可以将剩余资源暂时分享给其他队列。调度器在调度时，优先将资源分配给资源使用率最低的队列（即队列已使用资源量占分配给队列的资源量比例最小的队列）；在队列内部，则按照作业优先级的先后顺序遵循 FIFO 策略进行调度。 容量调度器有以下几点特点： 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源； 灵活性：如果一个队列资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列； 多重租赁：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束； 安全保证：每个队列有严格的 ACL 列表规定它访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序； 动态更新配置文件：管理员可以根据需要动态修改各种配置参数。 举例： root ​ — dev 60% ​ — spark 50% ​ — hadoop 50% ​ — prod 40% 假设job1提交到队列dev.spark，此时prod队列没有任务，dev.hadoop队列也没有任务。如果运行job需要的资源占30%够用，直接运行即可；否则，会弹性的先占用dev.hadoop队列的容量，占到60%容量资源；如仍然不够，则继续弹性占用prod队列容量。为了防止dev队列占用太多资源，可以为其设置上限，如75%。 Apache版本默认使用容量调度器。 公平调度器（Fair Scheduler） 公平调度器先将用户的任务分配到多个队列中，每个队列设定资源分配最低保障和最高上限，管理员也可以指定队列的优先级，优先级高的队列将会被分配更多的资源，当一个队列资源有剩余时，可以临时将剩余资源共享给其他队列。公平调度器的调度过程如下： 根据每个队列的最小资源保障，将系统中的部分资源分配给各个队列； 根据队列的指定优先级将剩余资源按照比例分配给各个队列； 在各个队列中，按照作业的优先级或者根据公平策略将资源分配给各个作业； 公平调度器有以下几个特点： 支持抢占式调度，即如果某个队列长时间未能被分配到公平共享量的资源，则调度器可以杀死过多分配资源的队列的任务，以空出资源供这个队列使用； 强调作业之间的公平性：在每个队列中，公平调度器默认使用公平策略来实现资源分配，这种公平策略是最大最小公平算法的一种具体实现，可以尽可能保证作业间的资源分配公平性； 负载均衡：公平调度器提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到给各个节点上； 调度策略配置灵活：允许管理员为每个队列单独设置调度策略； 提高小应用程序响应时间：由于采用了最大最小公平算法，小作业可以快速获得资源并运行完成。 举例： A、B、C三个队列，每个队列中的作业按照优先级分配资源，优先级越高分配的资源越多，但都可以分配到资源，以确保公平。当在资源有限的情况下，每个作业需要获得的资源与实际获得的资源存在差距，这个差距称作”缺额“。在同一个队列中，作业的”缺额“越大，则越优先被调度执行。此时，A、B、C三个队列，每个队列中会有多个作业同时运行。 CDH版本默认使用公平调度器。 配置多用户资源隔离 配置 修改yarn-site.xml node01执行 yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler yarn.scheduler.fair.allocation.file /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/fair-scheduler.xml yarn.scheduler.fair.preemption true yarn.scheduler.fair.preemption.cluster-utilization-threshold 0.8f yarn.scheduler.fair.user-as-default-queue true default is True yarn.scheduler.fair.allow-undeclared-pools false default is True 添加fair-scheduler.xml node01执行 30 512mb,4vcores 102400mb,100vcores 100 1.0 fair 512mb,4vcores 30720mb,30vcores 100 fair 1.0 * 512mb,4vcores 20480mb,20vcores 100 fair 2.0 hadoop hadoop hadoop hadoop 512mb,4vcores 20480mb,20vcores 100 fair 1.0 develop develop develop develop 512mb,4vcores 20480mb,20vcores 100 fair 1.5 test,hadoop,develop test test group_businessC,supergroup 分发 node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop scp yarn-site.xml fair-scheduler.xml node02:$PWD scp yarn-site.xml fair-scheduler.xml node03:$PWD 重启YARN集群 stop-yarn.sh start-yarn.sh 修改作业的提交队列 当前用户和用户组是hadoop hadoop // 不设置队列时，使用当前系统用户和用户组提交队列，提交至队列root.hadoop // 不可提交至队列root.develop(也可以简写成develop) ，因为其限制用户和用户组是develop develop // configuration.set(\"mapred.job.queue.name\", \"develop\"); 当前用户和用户组是develop develop // 可提交至队列root.develop configuration.set(\"mapred.job.queue.name\", \"develop\"); Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-05 14:25:13 "},"src/bigdata/zookeeper/ZooKeeper集群安装部署.html":{"url":"src/bigdata/zookeeper/ZooKeeper集群安装部署.html","title":"ZooKeeper集群安装部署","keywords":"","body":"安装zookeeper集群 注意：三台机器一定要保证时钟同步 zookeeper分发到node01 分发 本机执行 scp zookeeper-3.4.5-cdh5.14.2.tar.gz hadoop@192.168.2.100:/bigdata/soft 解压 node01执行 cd /bigdata/soft tar -zxvf zookeeper-3.4.5-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置文件 node01执行 cd /bigdata/install/zookeeper-3.4.5-cdh5.14.2/conf cp zoo_sample.cfg zoo.cfg mkdir -p /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas vi zoo.cfg # 注释原 dataDir=/tmp/zookeeper dataDir=/bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas autopurge.snapRetainCount=3 autopurge.purgeInterval=1 server.1=node01:2888:3888 server.2=node02:2888:3888 server.3=node03:2888:3888 添加myid配置 node01执行 echo 1 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid zookeeper分发到node02和node03 分发 node01执行 scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node02:/bigdata/install/ scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node03:/bigdata/install/ 修改myid配置 node02执行 echo 2 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid node03执行 echo 3 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid 配置环境变量 三台机器分别执行 vi /etc/profile export ZOOKEEPER_HOME=/bigdata/install/zookeeper-3.4.5-cdh5.14.2 export PATH=$PATH:$ZOOKEEPER_HOME/bin # 立即生效 source /etc/profile 启动zookeeper服务 三台机器分别执行 # 启动 # jps 每个节点上都有QuorumPeerMain进程 zkServer.sh start # 查看启动状态 # 一个leader、其他follower zkServer.sh status # 停止 zkServer.sh stop Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-29 14:49:34 "},"src/bigdata/zookeeper/ZooKeeper分布式协调框架.html":{"url":"src/bigdata/zookeeper/ZooKeeper分布式协调框架.html","title":"ZooKeeper分布式协调框架","keywords":"","body":"概述 ZooKeeper是一个集中的服务，用于维护配置信息、命名、提供分布式同步和提供组服务。所有这些类型的服务都以某种形式被分布式应用程序使用 开发人员需花较多的精力实现如何使多个程序协同工作的逻辑，导致没有时间更好的思考实现程序本身的逻辑 由于实现这类服务的困难，应用程序最初通常会忽略它们，这使得它们在出现更改时变得脆弱，并且难以管理 即使正确地实现了这些服务，在部署应用程序时，这些服务的不同实现也会导致管理复杂性 分布式框架中协同工作的逻辑是共性的需求 ZooKeeper简单易用，能够很好的解决分布式框架在运行中，出现的各种协调问题 集群主备切换 节点的上下线感知 统一命名服务 状态同步服务 集群管理 分布式应用配置管理 ZooKeeper集群也是主从架构 主角色：leader 从角色：follower或observer，统称为learner zkCli 命令行 Zookeeper有一个类似Linux系统的简版文件系统，目录结构是树状结构。 # 连接ZooKeeper集群 # -server 客户端随机连接三个服务器中的一个 zkCli.sh -server node01:2181,node02:2181,node03:2181 # 帮助命令 help # 退出 quit # 列出文件列表 ls / # 创建节点并指定数据；若没有指定节点数据，则无法创建节点；不可一次创建多级节点 create /test hello # 获取节点的数据 get /test # 修改节点的数据 set /test yes # 删除节点；若有子节点，则不可删除 delete /test # 删除当前节点和所有子节点 rmr /test 基本概念 通信方式 分布式通信方式 直接通过网络连接的方式进行通信 通过共享存储的方式，来进行通信或数据的传输 ZooKeeper使用第二种方式，提供分布式协调服务。 数据结构 ZooKeeper = 简版文件系统(Znode) + 原语 + 通知机制(Watcher) ZK文件系统，基于类似于文件系统的目录节点树方式的数据存储 原语，可简单理解成ZooKeeper的基本命令 Watcher 监听器 数据节点 ZNode ZNode 分为四类 持久节点 临时节点 非有序节点 create create -e 有序节点 create -s create -s -e 临时节点不能有子节点 持久节点 持久节点一旦创建，一直存在，只有通过删除命令才可以删除 # 创建节点 /zk_test，并设置数据 my_data create /zk_test my_data # 持久节点，只有显示的调用命令才能删除 delete /zk_test 临时节点 临时节点的生命周期同客户端会话session绑定，一旦会话失效，临时节点被删除 # client1 上创建临时节点 create -e /tmp tmpdata # client2 上查看client1创建的临时节点 # 此时存在 ls / # client1断开连接 close # client2 上查看 # 临时节点被自动删除 ls / 有序节点 一旦节点被标记上这个属性，那么在这个节点被创建时，ZooKeeper 就会自动在其节点后面追加上一个整型数字 这个整数是一个由父节点维护的自增数字。 提供了创建唯一名字的ZNode的方式 为防止多个客户端在同一个目录下创建同名ZNode导致失败的问题 # /tmp0000000006 create -s /tmp t 会话 Session 什么是会话 客户端要对ZooKeeper集群进行读写操作，得先与某一ZooKeeper服务器（任意一个均可）建立TCP长连接；此TCP长连接称为建立一个会话Session。 每个会话有超时时间：SessionTimeout。当客户端与集群建立会话后，如果超过SessionTimeout时间，两者间没有通信，会话超时。 会话的特点 客户端打开一个Session中的请求以FIFO（先进先出）的顺序执行。如客户端client01与集群建立会话后，先发出一个create请求，再发出一个get请求；那么在执行时，会先执行create，再执行get。 若打开两个Session，则无法保证Session间请求FIFO执行；只能保证一个session中请求的FIFO。 会话的声明周期 Client创建连接，首先是未连接转态，随着初始化后，进入连接中状态。当Client同ZooKeeper集群的某一个Server连接，则进入已连接状态。业务处理结束，Client主动关闭Session连接，此时进入已关闭状态。 当Client无法接收到Server的响应时，首先进入连接中状态，再次同ZooKeeper集群的某一个Server连接，如果连接成功，则进入已连接状态；如果始终无法与任意一个Server连接，则关闭Session连接进入已关闭状态。 请求 Request 读写请求 通过客户端向ZooKeeper集群中写数据 通过客户端从ZooKeeper集群中读数据 事务zxid 事务 客户端的写请求，会对ZooKeeper中的数据做出更改，如增删改的操作 每次写请求，会生成一次事务 每个事务有一个全局唯一的事务ID，用 ZXID 表示，全局自增 事务特点 ACID：原子性atomicity | 一致性consistency | 隔离性isolation | 持久性durability ZXID结构 通常是一个64位的数字。由epoch+counter组成 epoch、counter各32位 # 当前leader是选举出来的第几任，如13 /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/version-2/currentEpoch # 创建一个znode create -e /temp temp get /temp # counter = 00000023 # epoch = d # 第13任leader的第23个事务 # c 创建 m 修改 p 修改子节点 cZxid = 0xd00000023 mZxid = 0xd00000023 pZxid = 0xd00000023 监视与通知 Watcher Watcher是客户端在服务器端注册的事件监听器；Watcher用于监听znode上的某些事件，如znode数据修改、节点增删等；当监听到事件后，Watcher会触发通知客户端。注意：Watcher是一个单次触发的操作。 节点上下线感知 Watcher + 临时节点 # client1 创建临时节点 create -e /temp temp # client2 监控临时节点 ls /temp watcher # client1 模拟下线 close # client2 获得通知 # WATCHER:: # WatchedEvent state:SyncConnected type:NodeDeleted path:/temp 应用场景 HDFS HA 方案 NameNode存在单点故障问题，一旦NameNode宕机，直接导致HDFS无法对外提供服务。未解决此问题，可以增加一个备份NameNode，当主NameNode宕机，备NameNode自动快速切换响应外部请求。另外，元数据存在于NameNode的内存中，也存在了如何共享内存元数据的问题。 Hadoop 2.x 版本提出了高可用（High Availability，HA）解决方案。 主备切换 ZKFC 涉及角色 每个NameNode节点上各有一个ZKFC进程 ZKFC即 ZKFailoverController，作为独立进程存在，负责控制NameNode的主备切换 ZKFC会监控NameNode的健康状况，当发现Active NameNode异常时，通过Zookeeper集群进行NameNode主备选举，完成Active和Standby状态的切换 ZKFC在启动时，同时会初始化HealthMonitor和ActiveStandbyElector服务；同时会向HealthMonitor和ActiveStandbyElector注册相应的回调方法 HealthMonitor定时调用NameNode的HAServiceProtocol RPC接口（monitorHealth和getServiceStatus），监控NameNode的健康状态，并向ZKFC反馈 ActiveStandbyElector接收ZKFC的选举请求，通过Zookeeper自动完成NameNode主备选举 选举完成后回调ZKFC的主备切换方法对NameNode进行Active和Standby状态的切换 主备选举过程 全新集群选举 启动两个NameNode和ZKFC 两个ZKFC通过各自ActiveStandbyElector发起NameNode的主备选举，这个过程利用Zookeeper的写一致性和临时节点机制实现 当发起一次主备选举时，ActiveStandbyElector会尝试在Zookeeper创建临时节点/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，Zookeeper的写一致性保证最终只会有一个ActiveStandbyElector创建成功 ActiveStandbyElector从ZooKeeper获得选举结果 创建成功的 ActiveStandbyElector回调ZKFC的回调方法，将对应的NameNode切换为Active NameNode状态 而创建失败的ActiveStandbyElector回调ZKFC的回调方法，将对应的NameNode切换为Standby NameNode状态 不管是否选举成功，所有ActiveStandbyElector都会在临时节点ActiveStandbyElectorLock上注册一个Watcher监听器，来监听这个节点的状态变化事件 非全新集群选举 如果Active NameNode对应的HealthMonitor检测到NameNode状态异常时，通知对应ZKFC ZKFC会调用 ActiveStandbyElector 方法，删除在Zookeeper上创建的临时节点ActiveStandbyElectorLock（或者ActvieStandbyElector与ZooKeeper的session断开，临时节点也会被删除，但有可能此时原Active NameNode仍然是active状态） 此时，Standby NameNode的ActiveStandbyElector注册的Watcher就会监听到此节点的 NodeDeleted事件。 收到这个事件后，此ActiveStandbyElector发起主备选举，成功创建临时节点ActiveStandbyElectorLock，如果创建成功，则Standby NameNode被选举为Active NameNode 如何防止脑裂 在分布式系统中 双主 现象又称为脑裂，由于Zookeeper的 ”假死”、长时间的垃圾回收或其它原因都可能导致 双Active NameNode 现象，此时两个NameNode都可以对外提供服务，无法保证数据一致性。对于生产环境，这种情况的出现是毁灭性的，必须通过自带的 隔离（Fencing）机制预防此类情况。 正常状态 ActiveStandbyElector成功创建ActiveStandbyElectorLock临时节点后，会创建另一个ActiveBreadCrumb持久节点 ActiveBreadCrumb持久节点保存了Active NameNode的地址信息 当Active NameNode在正常的状态下断开Zookeeper Session，会一并删除临时节点ActiveStandbyElectorLock、持久节点ActiveBreadCrumb 异常状态 但如果ActiveStandbyElector在异常的状态下关闭Zookeeper Session（此时有可能由于active NameNode与ZooKeeper通信不畅导致，所以此时NameNode还处于active状态），那么持久节点ActiveBreadCrumb会保留下来 当另一个NameNode要由standby变成active状态时，会发现上一个Active NameNode遗留下来的ActiveBreadCrumb节点，那么会回调 ZKFC 的方法对旧的Active NameNode进行fencing 首先ZKFC会尝试调用旧Active NameNode的HAServiceProtocol RPC接口的transitionToStandby方法，看能否将其状态切换为Standby 如果transitionToStandby方法切换状态失败，那么就需要执行Hadoop自带的隔离措施，Hadoop目前主要提供两种隔离措施： sshfence：SSH to the Active NameNode and kill the process shellfence：run an arbitrary shell command to fence the Active NameNode 只有成功地fencing之后，选举成功的ActiveStandbyElector才会回调ZKFC的becomeActive方法，然后ZKFS调用transitionToActive将对应的NameNode切换为Active，开始对外提供服务 元数据同步 集群启动后一个NN处于active状态，并提供服务，处理客户端和DataNode的请求，并把editlog写到本地和share editlog（可以是NFS，QJM等）中。另外一个NN处于Standby状态，它启动的时候加载fsimage，然后周期性的从share editlog中获取editlog，保持与active的状态同步。为了实现standby在active挂掉后迅速提供服务，需要DN同时向两个NN汇报，使得Stadnby保存block to datanode信息，因为NN启动中最费时的工作是处理所有datanode的blockreport。 在主备切换过程中，新的Active NameNode必须确保与原Active NamNode元数据同步完成，才能对外提供服务 HA集群中不需要运行SecondaryNameNode、CheckpointNode或者BackupNode。事实上，HA架构中运行上述节点，将会出错 分布式锁 在单一进程多线程环境下，争抢共享资源，可以利用JUC完美解决。但随着系统发展，单一进程系统进一步发展为多进程分布式系统，这样就产生了如何解决共享资源争抢冲突的问题。可以借助ZooKeeper实现分布式锁方案。 方案一 Client 1、2、3同时向ZooKeeper请求创建临时节点 /lock，默认是无序节点，只有Client 1 创建成功，其他Client创建失败，因此Client 1 获得锁可以使用共享资源，而其他Client对该节点设置watcher等待通知事件 Client 1 任务结束，删除 /lock 或者 关闭同ZooKeeper的session连接 ZooKeeper会通知Client 2和Client 3，有删除节点事件 Client 2和Client 3同时向ZooKeeper请求创建临时节点 /lock，Client 3创建成功，而Client2创建失败，Client2继续对该节点设置watcher等待通知事件 方案一会产生惊群问题，多个进程请求共享资源但无法同时获得锁，未获得锁的进程只有休眠等待通知。一旦多个进程同时得到通知，全部进程会无序CPU调度，网络通信争抢锁，但此时又只会有一个进程获得锁，当有大量争抢进程时，会导致计算和网络资源的浪费。 方案二 Client 1、2、3同时向ZooKeeper询问root持久节点/group是否创建，如果未创建，则只会有一个进程创建root节点。Client 1创建root节点，然后创建临时有序节点/group/lock0000000001，然后Client1发现是第一个子节点，则获得锁；Client 2创建临时有序节点/group/lock0000000002，发现不是第一个子节点，则设置上一个/group/lock0000000001节点watcher等待通知；Client 3创建临时有序节点/group/lock0000000003，发现不是第一个子节点，则设置上一个/group/lock0000000002节点watcher等待通知 Client 1 任务结束，删除 /group/lock0000000001 或者 关闭同ZooKeeper的session连接 Client 2 获得通知，检查是否是第一个子节点，如果是则获得锁处理任务；否则，继续监听上一个子节点（Client 1可能异常结束，但非第一个节点） Client 2 任务结束，删除 /group/lock0000000002 Client 3 获得通知，检查是否是第一个子节点，如果是则获得锁处理任务 方案二多个进程获得锁顺序依赖于并发创建临时有序子节点顺序。此方案不失并发性，进程又可调度有序。 核心原理 ZAB算法 仲裁quorum 什么是仲裁quorum 发起proposal时，只要多数派同意，即可生效 为什么要仲裁 不需要所有的服务器都响应proposal就能生效，可以提高集群的响应速度 quorum数如何选择 集群节点数 / 2 + 1 如3节点集群：quorum = 3 / 2 + 1 = 2 脑裂 网络分区 网络通信故障，集群被分成了2部分 脑裂 原leader处于一个分区，而另外一个分区选举出新的leader，此时集群出现2个leader，导致集群紊乱。 防止脑裂导致集群中出现多个leader 为什么规则要求 可用节点数 > 集群总结点数 / 2 ？如果不这样限制，在集群出现脑裂的时候，可能会出现多个子集群同时服务的情况（即子集群各组选举出自己的leader）， 这样对整个zookeeper集群来说是紊乱的。换句话说，如果遵守上述规则进行选举，即使出现脑裂，集群最多也只能出现一个子集群可以提供服务的情况。 ZAB算法 分布式一致性协议 Paxos 偏向于理论、对如何应用到工程实践提及较少 Raft 在2013年提出，提出的时间虽然不长，但已经有很多系统基于Raft实现 Zab 的全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，Zab最大的特点是保证强一致性(strong consistency，或叫线性一致性linearizable consistency)。Zab协议有两种模式 恢复模式（选主） 因为ZooKeeper也是主从架构，当ZooKeeper集群没有主的角色leader时，从众多服务器中选举leader时，处于此模式。主要处理内部矛盾，我们称之为安内 广播模式（同步） 当集群有了leader后，客户端向ZooKeeper集群读写数据时，集群处于此模式。主要处理外部矛盾，我们称之为攘外 Zab 和 Raft 算法相似，区别如下 Zab心跳从follower到leader；Raft从leader到follower Zab任期称为epoch；Raft任期称为term Raft 动图演示地址： http://thesecretlivesofdata.com/raft/ ZooKeeper服务器个数 仲裁模式下，服务器个数选择为奇数。 zookeeper选举的规则：leader选举要求 可用节点数 > 总节点数 / 2 。 防止脑裂导致集群不可用 假设5个节点发生脑裂，划分成2个子集群 A集群 B集群 1 4 2 3 3 2 4 1 若满足 可用节点数 = 5 / 2 + 1 = 3，此时多种脑裂情况都会有一个子集群提供服务。 假设4个节点发生脑裂，划分成2个子集群 A集群 B集群 1 3 2 2 3 1 若满足 可用节点数 = 4 / 2 + 1 = 3，此时若两个子集群均匀划分，A和B集群都无法满足lead选举的要求 在容错能力相同情况下，奇数节点更节省资源 假设4个节点 可用节点数 = 4 / 2 + 1 = 3，允许1个节点宕机 假设5个节点 可用节点数 = 5 / 2 + 1 = 3，允许2个节点宕机 假设6个节点 可用节点数 = 6 / 2 + 1 = 4，允许2个节点宕机 5节点和6节点集群相比，容错能力相同，都允许有2个节点宕机，但5节点的quorum更小，响应速度更快且更节省资源；5节点和4节点集群相比，quorum相同，但5节点的容错能力更强。 读写操作（攘外） 读操作 客户端与一个follower建立Session连接，向ZooKeeper集群读取数据，如 get /test follower返回客户端 /test ZNode信息 写操作 客户端与一个follower建立Session连接，向ZooKeeper集群写入数据，如 create /test follower“权限”不够，询问“领导”leader，将写入数据请求转发给leader leader收到请求后非常“民主”，向所有follower发出 proposal 提案 create /test，包括leader自己。follower和leader收到提案后先记录下来 当有超过半数的 quorum （包括leader自己，quorum = N / 2 + 1）同意提案，则leader首先 commit 提交提案，创建 /test ZNode 然后leader通知所有follower commit 提交提案，follower各自创建 /test ZNode follower响应客户端写入数据成功 选举（安内） 全新集群leader选举 以3台机器组成ZooKeeper集群为例，当集群中有超过半数机器启动后，才可以选举leader。在选举过程中，每个Server都要广播投票，投票信息结构为 (sid, zxid)。Server接收到投票信息后开始Player Killing，PK逻辑是先比较zxid，大的获胜；如果相等，再比较sid，大的获胜。 ZooKeeper服务器的4中状态 looking：服务器处于寻找leader状态 leading：服务器作为leader的状态 following：服务器作为follower的状态 observing：服务器作为observer的状态 选举流程（假设Server1，Server2，Server3依次启动） 启动Server1，投票给自己，投票信息是(1, 0)，投票信息不变且没有过半数，没有选举出leader，仍为looking状态 启动Server2，Server2先在集群中查找是否已经选举出leader，若没有选举出leader，则开始进入选举投票流程，Server2的投票信息是(2, 0) Server1，Server2向集群中服务器广播自己的投票信息 处理投票 Server1给自己投票(1, 0)，接收到Server2投票(2, 0)，PK，结果是(2, 0)，Server1更新投票信息是(2, 0) Server2给自己投票(2, 0)，接收到Server1投票(1, 0)，PK，结果是(2, 0)，Server2结果不变 Server1，Server2向集群中服务器广播自己的投票结果，此轮投票结束 Server1两票都是(2, 0)，票数超过半数（quorum = N / 2 + 1），Server2当选leader Server2两票都是(2, 0)，票数超过半数，Server2当选leader Server2将服务器状态从looking改变为leading，成为leader，同时向集群中服务器广播 Server1收到广播信息后，将服务器状态从looking改变为following，成为follower 启动Server3，Server3先在集群中查找是否已经选举出leader，发现已选举出leader，则不再选举，直接将服务器状态从looking改变为following，成为follower 状态同步 当Leader完成选举后，Follower需要与新的Leader同步数据。 在Leader端做如下工作 Leader会构建一个NEWLEADER封包，包括当前最大的zxid，发送给所有的Follower或者Observer Leader给每个Follower创建一个线程LearnerHandler来负责处理每个Follower的数据同步请求，同时主线程开始阻塞，只有超过一半的Follower同步完成，同步过程才完成，Leader才能成为真正的Leader 根据同步算法进行同步操作 在Follower端做如下工作 选举完成后，尝试与Leader建立同步连接，如果一段时间没有连接上就报错超时，重新回到选举状态 向Leader发送FOLLOWERINFO封包，带上自己最大的zxid 根据同步算法进行同步操作 具体使用哪种同步算法取决于Follower当前最大的zxid，在Leader端会维护最小事务id minCommittedLog 和最大事务id maxCommittedLog 两个zxid，minComittedLog 是没有被快照存储的日志文件的第一条（每次快照储存完，会重新生成一个事务日志文件），maxCommittedLog 是事务日志中最大的事务。Zookeeper中实现了以下数据同步算法 直接差异化同步（DIFF同步） 仅回滚同步（TRUNC），即删除多余的事务日志，比如原来的Leader节点宕机后又重新加入，可能存在它自己写入并提交但是其他节点还没来得及提交的数据 先回滚（TRUNC）再差异化（DIFF）同步 全量同步（SNAP） DIFF 假设Leader端未被快照存储的zxid为0x500000001、0x500000002、0x500000003、0x500000004、0x500000005，此时Follower端最大已提交的zxid（即peerLastZxid）为0x500000003，因此需要把0x500000004、0x500000005同步给Follower，直接使用差异化同步（DIFF）即可。 Follower端同步过程如下： Follower端首先收到DIFF指令，进入DIFF同步阶段 Follower收到同步的数据和提交命令，并应用到内存数据库当中 同步完成后，Leader会发送一个NEWLEADER指令，通知Follower已经将最新的数据同步给Follower了，Follower收到NEWLEADER指令后反馈一个ack消息，表明自己已经同步完成 单个Follower同步完成后，Leader会进入集群的”过半策略”等待状态，当有超过一半的Follower都同步完成以后，Leader会向已经完成同步的Follower发送UPTODATE指令，用于通知Follower已经完成数据同步，可以对外提供服务了，最后Follower收到Leader的UPTODATE指令后，会终止数据同步流程，向Leader再次反馈一个ack消息。 TRUNC + DIFF Leader在本地提交事务完成，还没来得及把事务提交提议发送给其他节点前宕机了。假设集群有三个节点，分别是A、B、C，没有宕机前Leader是B，已经发送过0x500000001和0x500000002的数据修改提议和事务提交提议，并且发送了0x500000003的数据修改提议，但在B节点发送事务提交提议（leader已提交）之前，B宕机了，B最新的数据是0x500000003，但发送给A和C的事务提议失败了，A和C的最新数据依然是0x500000002，B宕机后，A和C会进行Leader选举，假设C成为新的Leader，并且进行过两次数据修改，对应的zxid为0x600000001、0x600000002（epoch自增），然而此时B机器恢复后加入新集群，重新进行数据同步，对B来说，peerLastZxid为0x500000003，对于当前的Leader C来说，minCommitedLog=0x500000001, maxCommittedLog=0x600000002（总共是0x500000001、0x500000002、0x600000001、0x600000002未被快照的事务）。这种情况下使用（TRUNC + DIFF）的同步方式，同步过程如下： B恢复并且向已有的集群（AC）注册后，向C发起同步连接的请求 B向Leader C发送FOLLOWERINFO封包，带上Follower自己最大的zxid（0x500000003） C发现自己没有0x500000003这个事务提交记录，就向B发送TRUNC指令，让B回滚到0x500000002 B回滚完成后，向C发送信息包，确认完成，并说明当前的zxid为0x500000002 C向B发送DIFF同步指令 B收到DIFF指令后进入同步状态，并向C发送ACK确认包 C陆续把对应的差异数据修改提议和Commit提议发给B，当数据发送完成后，再发送通知包给B B将数据修改提议应用于内存数据结构并Commit，当收到C通知已经同步完成后，B给回应ACK，并且结束同步 SNAP 当集群中某个节点宕机时间过长，在恢复并且加入集群时，集群中数据的事务日志文件已经生成多个，此时leader的minCommittedLog比该节点宕机时的最大zxid还要大（leader已生成快照）。例如假设ABC集群中B宕机，几天后才恢复，此时minCommittedLog为0x6000008731，而peerLastZxid为0x500000003，这种情况下采用全量同步（SNAP）的方式，同步过程如下： 当Leader C发现B的peerLastZxid小于minCommittedLog时，向B发送SNAP指令 B收到同步指令，进入同步阶段 Leader C会从内存数据库中获取全量的数据发送给B B获取数据处理完成后，C还会把全量同步期间产生的最新的数据修改提议和Commit提议以增量（DIFF）的方式发送给B Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-03-08 11:37:48 "},"src/bigdata/hive/Hive安装部署.html":{"url":"src/bigdata/hive/Hive安装部署.html","title":"Hive安装部署","keywords":"","body":"先决条件 hive是一个构建数据仓库的工具，只需要在一台服务器上安装，不需要在多台服务器上安装。 使用hadoop普通用户在node03上安装 搭建好三节点Hadoop集群 node03上安装MySQL服务 安装 # 拷贝到node03上 scp hive-1.1.0-cdh5.14.2.tar.gz hadoop@192.168.2.102:/bigdata/soft # 解压 cd /bigdata/soft tar -xzvf hive-1.1.0-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置 hive-env.sh cd /bigdata/install/hive-1.1.0-cdh5.14.2/conf mv hive-env.sh.template hive-env.sh vi hive-env.sh 修改内容 # 配置HADOOP_HOME路径 export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2/ # 配置HIVE_CONF_DIR路径 export HIVE_CONF_DIR=/bigdata/install/hive-1.1.0-cdh5.14.2/conf hive-site.xml vi hive-site.xml 修改内容 javax.jdo.option.ConnectionURL jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=latin1&amp;useSSL=false javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword root hive.cli.print.current.db true hive.cli.print.header true hive.server2.thrift.bind.host node03 hive-log4j.properties # 创建hive日志存储目录 mkdir -p /bigdata/install/hive-1.1.0-cdh5.14.2/logs/ cd /bigdata/install/hive-1.1.0-cdh5.14.2/conf mv hive-log4j.properties.template hive-log4j.properties vi hive-log4j.properties 修改内容 hive.log.dir=/bigdata/install/hive-1.1.0-cdh5.14.2/logs/ 拷贝mysql驱动包 scp mysql-connector-java-5.1.38.jar hadoop@192.168.2.102:/bigdata/soft # 由于运行hive时，需要向mysql数据库中读写元数据，所以需要将mysql的驱动包上传到hive的lib目录下 cp mysql-connector-java-5.1.38.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/ 配置环境变量 root用户下执行 su root vi /etc/profile 修改内容 export HIVE_HOME=/bigdata/install/hive-1.1.0-cdh5.14.2 export PATH=$PATH:$HIVE_HOME/bin 切换回hadoop su hadoop source /etc/profile 验证安装 Node03执行 # 启动hive cli命令行客户端 hive 查看数据库 show databases; 退出 quit; Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-06 14:19:42 "},"src/bigdata/hive/大数据分析利器之Hive.html":{"url":"src/bigdata/hive/大数据分析利器之Hive.html","title":"大数据分析利器之Hive","keywords":"","body":"数据仓库 基本概念 数据仓库的英文名称为Data Warehouse，可简写为DW或DWH。 数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。它出于分析性报告和决策支持的目的而创建。 数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用，这也是为什么叫“仓库”，而不叫“工厂”的原因。 主要特征 数据仓库是面向主题的（Subject-Oriented）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant）数据集合，用以支持管理决策。 主题的（Subject-Oriented） 数据仓库是一般从用户实际需求出发，将不同平台的数据源按设定主题进行划分整合，与传统的面向事务的操作型数据库不同，具有较高的抽象性。面向主题的数据组织方式，就是在较高层次对分析对象数据的一个完整、统一并一致的描述，能完整及统一地刻画各个分析对象所涉及的有关企业的各项数据，以及数据之间的联系。 集成的（Integrated） 数据仓库中存储的数据大部分来源于传统的数据库，但并不是将原有数据简单的直接导入，而是需要进行预处理。这是因为事务型数据中的数据一般都是有噪声的、不完整的和数据形式不统一的。这些“脏数据”的直接导入将对在数据仓库基础上进行的数据挖掘造成混乱。“脏数据”在进入数据仓库之前必须经过抽取、清洗、转换才能生成从面向事务转而面向主题的数据集合。数据集成是数据仓库建设中最重要，也是最为复杂的一步。 非易失的（Non-Volatile） 数据仓库中的数据主要为决策者分析提供数据依据。决策依据的数据是不允许进行修改的。即数据保存到数据仓库后，用户仅能通过分析工具进行查询和分析，而不能修改。数据的更新升级主要都在数据集成环节完成，过期的数据将在数据仓库中直接筛除。 时变的（Time-Variant） 数据仓库数据会随时间变化而定期更新，不可更新是针对应用而言，即用户分析处理时不更新数据。每隔一段固定的时间间隔后，抽取运行数据库系统中产生的数据，转换后集成到数据仓库中。随着时间的变化，数据以更高的综合层次被不断综合，以适应趋势分析的要求。当数据超过数据仓库的存储期限，或对分析无用时，从数据仓库中删除这些数据。关于数据仓库的结构和维护信息保存在数据仓库的元数据(Metadata)中，数据仓库维护工作由系统根据其中的定义自动进行或由系统管理员定期维护。 数据仓库和数据库的区别 数据库与数据仓库的区别实际讲的是OLTP 与 OLAP 的区别。 操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理OLTP。 分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing），一般针对某些主题的历史数据进行分析，支持管理决策。 数据仓库的出现，并不是要取代数据库。 数据库是面向事务的设计，数据仓库是面向主题设计的。 数据库一般存储业务数据，数据仓库存储的一般是历史数据。 数据库设计是尽量避免冗余，一般针对某一业务应用进行设计；比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析；数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。 数据库是为捕获数据而设计，数据仓库是为分析数据而设计。 以银行业务为例。数据库是事务系统的数据平台，客户在银行做的每笔交易都会写入数据库，被记录下来，这里，可以简单地理解为用数据库记账。数据仓库是分析系统的数据平台，它从事务系统获取数据，并做汇总、加工，为决策者提供决策的依据。比如，某银行某分行一个月发生多少交易，该分行当前存款余额是多少。如果存款又多，消费交易又多，那么该地区就有必要设立ATM了。 显然，银行的交易量是巨大的，通常以百万甚至千万次来计算。事务系统是实时的，这就要求时效性，客户存一笔钱需要几十秒是无法忍受的，这就要求数据库只能存储很短一段时间的数据。而分析系统是事后的，它要提供关注时间段内所有的有效数据。这些数据是海量的，汇总计算起来也要慢一些，但是，只要能够提供有效的分析数据就达到目的了。 数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。 分层架构 按照数据流入流出的过程，数据仓库架构可分为三层——源数据层、数据仓库层、数据应用层。 数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。 源数据层（ODS Operational Data Store）：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。 数据仓库层（DW Data Warehouse）：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。 数据应用层（DA Data Application）：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。 数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。 为什么要对数据仓库分层？ 用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。 通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。 Hive 概念 Hive是基于Hadoop的一个数据仓库工具。 可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。 其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储支持，即Hive可以理解为一个将SQL转换为MapReduce任务的工具，甚至更进一步可以说Hive就是一个MapReduce的客户端。 与数据库区别 对比项 Hive RDBMS 查询语言 HQL SQL 数据存储 HDFS Raw Device or local FS 执行器 MapReduce Executor 数据插入 支持批量导入/单条插入 支持批量导入/单条插入 数据操作 覆盖追加 行级更新删除 处理数据规模 大 小 执行延迟 高 低 分区 支持 支持 索引 0.8版本之后加入简单索引 支持复杂索引 扩展性 高（好） 有限（差） 数据加载模式 读时模式（快） 写时模式（慢） 应用场景 海量数据查询 实时查询 Hive 具有 SQL 数据库的外表，但应用场景完全不同。 Hive 只适合用来做海量离线数据统计分析，也就是数据仓库。 优缺点 优点 操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。 避免了去写MapReduce，减少开发人员的学习成本。 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 缺点 Hive 的查询延迟很严重 Hive 不支持事务 架构原理 用户接口：Client CLI（hive shell） JDBC/ODBC（java访问hive） WEBUI（浏览器访问hive，可以使用HUE） 元数据：Metastore 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore Hadoop集群 使用HDFS进行存储，使用MapReduce进行计算 Driver：驱动器 解析器（SQL Parser） 将SQL字符串转换成抽象语法树AST 对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误 编译器（Physical Plan）：将AST编译生成逻辑执行计划 优化器（Query Optimizer）：对逻辑执行计划进行优化 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说默认就是MapReduce任务 交互方式 Hive交互式shell 不推荐 启动 hive CLI # Hive CLI is deprecated and migration to Beeline is recommended. hive 执行语句 -- 列出数据库 show databases; -- 退出 quit; JDBC服务 推荐 启动 hiveserver2 服务 # 前台启动 hive --service hiveserver2 # nohup 不挂断的运行 # & 后台运行 # 0 – stdin (standard input) | 1 – stdout (standard output) | 2 – stderr (standard error) # 将2重定向到&1，&1再重定向到文件中 nohup hive --service hiveserver2 > /home/hadoop/hiveserver2log/hs2.log 2>&1 & # 检查后台服务，会有一个RunJar进程 jps beeline连接hiveserver2服务 # 启动客户端 beeline 连接数据库服务 -- 输入用户名hadoop，密码hadoop(首次需要设置) !connect jdbc:hive2://node03:10000 -- 列出数据库 show databases; -- 查看帮助 help -- 退出 !quit Hive命令 数仓搭建好后，执行脚本 执行HQL语句 # 使用 –e 参数来直接执行hql语句 hive -e \"show databases\" 执行HQL脚本 创建脚本 vi myhive.hql 脚本内容 create database if not exists myhive; 执行脚本 # 执行 hive -f myhive.hql # 检查 hive -e \"show databases\" 数据类型 基本数据类型 类型名称 描述 举例 boolean true/false true tinyint 1字节的有符号整数 1 smallint 2字节的有符号整数 1 int 4字节的有符号整数 1 bigint 8字节的有符号整数 1 float 4字节单精度浮点数 1.0 double 8字节单精度浮点数 1.0 string 字符串（不设长度） “abc” varchar 字符串（1-65355长度，超长截断） “abc” timestamp 时间戳 1563157873 date 日期 20190715 复合数据类型 类型名称 描述 定义 array 一组有序的字段，字段类型必须相同 col array map 一组无序的键值对 col map struct 一组命名的字段，字段类型可以不同 col struct array类型字段的元素访问方式 准备数据 t_array.txt 1 zhangsan beijing,shanghai 2 lisi shanghai,tianjin,wuhan 建表 -- 建表 -- field间空格分隔 -- array用,分隔 create table myhive.t_array(id string,name string,locations array) row format delimited fields terminated by ' ' collection items terminated by ','; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/t_array.txt' into table myhive.t_array; -- 查询 select * from myhive.t_array; -- 通过下标获取元素 select id,name,locations[0],locations[1] from myhive.t_array; -- 记录的数组元素若不存在，则为NULL select id,name,locations[0],locations[1],locations[2] from myhive.t_array; map类型字段的元素访问方式 准备数据 t_map.txt 1 name:zhangsan#age:30 2 name:lisi#age:40 建表 -- 建表 -- field间空格分隔 -- map中的每一个kv对以#分隔（本质是集合），kv以:分隔 create table myhive.t_map(id string,info map) row format delimited fields terminated by ' ' collection items terminated by '#' map keys terminated by ':'; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/t_map.txt' into table myhive.t_map; -- 查询 select * from myhive.t_map; -- 通过键获取值 -- 只能单独访问map的k或v，不能直接访问集合kv对 select id,info['name'],info['age'] from t_map; struct类型字段的元素访问方式 准备数据 t_struct.txt 1 zhangsan:30:beijing 2 lisi:40:shanghai 建表 -- 建表 create table myhive.t_struct(id string,info struct) row format delimited fields terminated by ' ' collection items terminated by ':' ; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/t_struct.txt' into table myhive.t_struct; -- 查询 select * from myhive.t_struct; -- 类似对象获取属性方法 select id,info.name,info.age,info.address from myhive.t_struct; DDL（Data Definition Language） 数据库DDL操作 创建数据库 -- 重复创建失败 Database test already exists create database test; -- 不存在才创建 -- 默认hdfs存储路径：/user/hive/warehouse/test.db -- 在hdfs中数据库映射为目录 create database if not exists test; 列出数据库 -- 列出所有数据库 show databases; -- 模糊查询数据库 show databases like '*t*'; 查询数据库信息 -- 数据库信息 desc database test; -- 数据库扩展信息 desc database extended test; 切换数据库 -- 切换到当前数据库 use test; 删除数据库 -- 删除存在数据库 -- 删除不存在的数据库失败 Database does not exist: test drop database test; -- 存在才删除 drop database if exists test; -- 当数据库有表存在时，需要级联强制删除 -- 慎用 drop database if exists myhive cascade; 表DDL操作 创建内部表 直接建表 use myhive; -- 在hdfs中表映射为目录 create table stu(id int, name string); -- 可以通过 insert into 向hive表中插入数据 -- 但不建议这么做，因为每个 insert into 转换成MapReduce后会生成一个小文件 -- 在hdfs中表数据映射为文件 insert into stu(id,name) values(1,\"zhangsan\"); insert into stu(id,name) values(2,\"lisi\"); -- 查询表数据 select * from stu; 查询建表 -- 通过 AS 查询语句完成建表，将子查询的结果存入新表 -- hdfs只有一个文件 create table if not exists myhive.stu1 as select id, name from stu; like建表 -- 根据已存在表的结构建表，没有数据 create table if not exists myhive.stu2 like stu; 创建内部表并指定字段之间的分隔符，指定文件的存储格式，以及数据存放的位置 -- 默认 \\001（非打印字符）分隔 field -- 自定义 \\t 分隔 field create table if not exists myhive.stu3(id int, name string) row format delimited fields terminated by '\\t' stored as textfile location '/user/stu3'; -- hdfs文件以\\t分隔存储每行数据的各个字段 insert into myhive.stu3(id,name) values(1,\"zhangsan\"); 创建外部表 外部表加载hdfs其他路径下已存在的数据文件，因此外部表不会独占数据文件，当删除表时，不会删除相应的数据文件 创建外部表需要加 external 关键字 location 字段可以指定，也可以不指定 当不指定location时，默认存放在指定数据库位置下。若没有指定数据库，则保存在default数据库下 当指定location时，使用location作为数据目录，数据库下不会再创建相应表的文件夹 create external table myhive.teacher (t_id string, t_name string) row format delimited fields terminated by '\\t'; 插入数据 通过 insert into 方式不推荐 通过 load data 方式加载数据到内部表或外部表 -- 加载本地文件 -- 拷贝 load data local inpath '/home/hadoop/hivedatas/teacher.csv' into table myhive.teacher; -- 加载hdfs文件 -- 剪切 -- overwrite 覆盖原有数据；否则追加 load data inpath '/hivetest/teacher.csv' overwrite into table myhive.teacher; 内部表与外部表的互相转换 内部表删除后，表的元数据和真实数据都被删除 外部表删除后，仅仅只是把该表的元数据删除，真实数据还在，后期可以恢复 使用时机 内部表由于删除表的时候会同步删除HDFS的数据文件，所以确定如果一个表仅仅是你独占使用，其他人不使用的时候就可以创建内部表，如果一个表的文件数据其他人也要使用，那么就创建外部表 外部表用在数据仓库的ODS层 内部表用在数据仓库的DW层 -- 内部表转换为外部表 -- EXTERNAL_TABLE alter table stu set tblproperties('EXTERNAL'='TRUE'); -- 外部表转换为内部表 alter table teacher set tblproperties('EXTERNAL'='FALSE'); 创建分区表 如果hive当中所有的数据都存入到一个目录下，那么在使用MR计算程序的时候，读取整个目录下面的所有文件来进行计算（全量扫描，性能低），就会变得特别慢，因为数据量太大了 实际工作中一般都是计算前一天的数据（日增），所以我们只需要将前一天的数据挑出来放到一个目录下面即可，专门去计算前一天的数据 这样就可以使用hive当中的分区表，通过分目录的形式，将每一天的数据都分成为一个目录，然后我们计算数据的时候，通过指定前一天的目录即可只计算前一天的数据 在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天，或者每小时进行切分成一个个的小的文件，这样去操作小的文件就会容易得多 创建分区表 -- 按month分区，不需要是表字段 -- 分区对应数据库表的一个字段，分区下所有数据的分区字段值相同 -- load data 之后才会出现分区文件夹 create table myhive.score(s_id string, c_id string, s_score int) partitioned by (month string) row format delimited fields terminated by '\\t'; -- 按year、month和day分区 create table myhive.score1 (s_id string, c_id string, s_score int) partitioned by (year string, month string, day string) row format delimited fields terminated by '\\t'; 加载数据 -- score -- month=201806对应hdfs的文件夹名 load data local inpath '/home/hadoop/hivedatas/score.csv' into table myhive.score partition (month='201806'); -- score1 -- score1/year=2018/month=06/day=01 load data local inpath '/home/hadoop/hivedatas/score.csv' into table myhive.score1 partition (year='2018', month='06', day='01'); 查询表分区 show partitions myhive.score; 添加表分区 -- 添加之后就可以在hdfs看到相应的文件夹 alter table myhive.score add partition(month='201805'); -- 同时添加多个分区 alter table myhive.score add partition(month='201804') partition(month='201803'); 删除表分区 alter table myhive.score drop partition(month='201806'); 综合举例 -- hdsf 创建日期目录，每日增加日期文件夹和数据 hdfs -mkdir /hivetest/day=20180607 -- 上传数据 hdfs dfs -put score.csv /hivetest/day=20180607 -- 创建外部分区表，同时指定数据位置 -- 表删除后，实际数据不删除 create external table myhive.score2(s_id string, c_id string, s_score int) partitioned by (day string) row format delimited fields terminated by '\\t' location '/hivetest'; -- 可以观察到虽然创建表成功，但没有创建相应的分区，也就是没有相应的MetaStore元数据 show partition myhive.score2; -- 表数据是空的 select * from myhive.score2; -- 解决办法：1、添加表分区(繁琐)；2、metastore check repair命令自动添加元数据 msck repair table myhive.score2; -- day=20180607 show partitions myhive.score2; -- 数据加载成功 select * from myhive.score2; 创建分桶表 分桶是相对分区进行更细粒度的划分 Hive表或分区表可进一步的分桶 分桶将整个数据内容按照某列取hash值，对桶的个数取模的方式决定该条记录存放在哪个桶当中；具有相同hash值的数据进入到同一个文件中 作用 取样sampling更高效 提升某些查询操作效率，例如map side join 开启参数支持 -- 开启对分桶表的支持 -- set hive.enforce.bucketing; 可以查询是否支持分桶，默认是false set hive.enforce.bucketing=true; -- 设置与桶相同的reduce个数（默认只有一个reduce） set mapreduce.job.reduces=4; 创建分桶表 -- 创建分桶表 -- 分桶字段是表的字段 create table myhive.user_buckets_demo(id int, name string) clustered by(id) into 4 buckets row format delimited fields terminated by '\\t'; -- 创建普通表 create table myhive.user_demo(id int, name string) row format delimited fields terminated by '\\t'; 准备数据 buckets.txt 1 anzhulababy1 2 anzhulababy2 3 anzhulababy3 4 anzhulababy4 5 anzhulababy5 6 anzhulababy6 7 anzhulababy7 8 anzhulababy8 9 anzhulababy9 10 anzhulababy10 加载数据 -- 普通表加载数据 load data local inpath '/home/hadoop/hivedatas/buckets.txt' overwrite into table myhive.user_demo; -- 加载数据到分桶表 -- 可以在hdsf中观察user_buckets_demo表所属的文件夹下共有4个文件(对于4个ReduceTask) insert into table myhive.user_buckets_demo select * from myhive.user_demo; 抽样查询分桶表的数据 TABLESAMPLE语法：TABLESAMPLE (BUCKET x OUT OF y [ON colname]) 。其中，x表示从第几个桶开始采样数据，桶序号从1开始，y表示桶数，colname表示每一行被采样的列。 -- 将user_demo以id作为采样列，划分为两个桶，返回第一个桶的数据 select * from myhive.user_demo tablesample(bucket 1 out of 2 on id); -- 以随机数作为采样列，因此每一次返回的数据不同 select * from myhive.user_demo tablesample(bucket 1 out of 2 on rand()); -- 显然，对 user_demo 采样，需对全表扫描。如果该表事先就是分桶表的话，采样效率会更高 -- user_buckets_demo 本事是分桶表，共有4桶 -- y 取值2，含义是分两桶，取第一桶采样数据。但表本身有4桶，共取两桶数据 4/2=2 作为采样数据，分别是第一桶和第三桶 -- 对数据除以4取余数，值为0，1，2，3 -- 第1桶 [0] 4 8 -- 第2桶 [1] 1 5 9 -- 第3桶 [2] 2 6 10 -- 第4桶 [3] 3 7 select * from myhive.user_buckets_demo tablesample(bucket 1 out of 2); -- 4/8=1/2 取第一桶的1/2作为采样数据 select * from myhive.user_buckets_demo tablesample(bucket 1 out of 8); 删除表 drop table myhive.stu2; 修改表结构信息 修改表名 use myhive; alter table teacher2 rename to teacher1; 增加列 use myhive; -- 已有记录的新增字段值为NULL alter table stu1 add columns(address string,age int); 修改列 use myhive; alter table stu1 change column address address_id int; 列出数据库表 use myhive; show tables; 查询表结构信息 -- 简要信息，只有字段名、类型和描述 desc myhive.stu; -- 详细信息 -- MANAGED_TABLE 内部表 desc formatted myhive.stu; DML（Data Manipulation Language） 数据导入 直接向表中插入数据 强烈不建议 create table myhive.score3 like score; -- 生成MR，对应hdfs一个小文件 insert into table myhive.score3 partition(month ='201807') values ('001','002','100'); 通过load加载数据 重要 -- overwrite只覆盖指定分区数据 -- 不会生成MR load data local inpath '/home/hadoop/hivedatas/score.csv' overwrite into table myhive.score3 partition(month='201806'); 通过查询加载数据 重要 create table myhive.score5 like score; -- 生成MR，所有数据对应一个文件 -- overwrite覆盖原有数据 insert overwrite table myhive.score5 partition(month='201806') select s_id,c_id,s_score from myhive.score3; 通过查询创建表并加载数据 create table myhive.score6 as select * from score; 创建表时指定location create external table myhive.score7 (s_id string,c_id string,s_score int) row format delimited fields terminated by '\\t' location '/hivetest/score'; 上传数据文件 # 上传文件到指定位置 hdfs dfs -put score.csv /hivetest/score 通过导入导出的数据 create table myhive.teacher2 like teacher; -- hdfs目录结构： -- /hivetest/teacher/_metadata -- /hivetest/teacher/data/teacher.csv export table myhive.teacher to '/hivetest/teacher'; -- 导入数据 import table myhive.teacher2 from '/hivetest/teacher'; 数据导出 insert 导出 -- 导出到本地 -- 生成MR，结果字段默认分隔为'\\001' insert overwrite local directory '/home/hadoop/hivedatas/stu' select * from myhive.stu1; -- 格式化输出结果 insert overwrite local directory '/home/hadoop/hivedatas/stu2' row format delimited fields terminated by ',' select * from myhive.stu1; -- 导出到hdfs insert overwrite directory '/hivetest/export/stu' row format delimited fields terminated by ',' select * from myhive.stu1; Hive 命令 -- 字段间以 \\t 分隔 hive -e 'select * from myhive.stu1;' > /home/hadoop/hivedatas/student.txt export导出到HDFS export table myhive.stu1 to '/hivetest/student'; 静态分区 需手动指定分区 创建分区表 create table myhive.order_partition( order_number string, order_price double, order_time string) partitioned BY(month string) row format delimited fields terminated by '\\t'; 准备数据 order.txt 10001 100 2019-03-02 10002 200 2019-03-02 10003 300 2019-03-02 10004 400 2019-03-03 10005 500 2019-03-03 10006 600 2019-03-03 10007 700 2019-03-04 10008 800 2019-03-04 10009 900 2019-03-04 加载数据 -- 加载数据时手动指定分区 -- 执行成功后，分区和数据全部建立 load data local inpath '/home/hadoop/hivedatas/order.txt' overwrite into table myhive.order_partition partition(month='2019-03'); -- 查询数据 select * from myhive.order_partition where month='2019-03'; 动态分区 数据导入时自动创建分区 创建表 -- 创建普通表 create table myhive.t_order( order_number string, order_price double, order_time string) row format delimited fields terminated by '\\t'; -- 创建目标分区表 create table myhive.order_dynamic_partition( order_number string, order_price double) partitioned BY(order_time string) row format delimited fields terminated by '\\t'; 准备数据 order_partition.txt 注意数据格式的规则正确性，否则会出现异常分区，导致查询数据出现问题 10001 100 2019-03-02 10002 200 2019-03-02 10003 300 2019-03-02 10004 400 2019-03-03 10005 500 2019-03-03 10006 600 2019-03-03 10007 700 2019-03-04 10008 800 2019-03-04 10009 900 2019-03-04 加载数据 -- 向普通表加载数据 load data local inpath '/home/hadoop/hivedatas/order_partition.txt' overwrite into table myhive.t_order; -- 支持自动分区需设置参数 -- 自动分区 set hive.exec.dynamic.partition=true; -- 非严格模式 set hive.exec.dynamic.partition.mode=nonstrict; -- 加载数据 insert into table myhive.order_dynamic_partition partition(order_time) select order_number, order_price, order_time from t_order; -- 查询数据 select * from myhive.order_dynamic_partition where order_time='2019-03-02'; select * from myhive.order_dynamic_partition where order_time='2019-03-03'; select * from myhive.order_dynamic_partition where order_time='2019-03-04'; 查询数据 基本 SQL 语言大小写不敏感 SQL 可以写在一行或者多行 关键字不能被缩写也不能分行 各子句一般要分行写 使用缩进提高语句的可读性 算术运算符 运算符 描述 A+B A和B 相加 A-B A减去B A*B A和B 相乘 A/B A除以B A%B A对B取余 A&B A和B按位取与 A|B A和B按位取或 A^B A和B按位取异或 ~A A按位取反 比较运算符 操作符 支持的数据类型 描述 A=B 基本数据类型 如果A等于B则返回true，反之返回false AB 基本数据类型 如果A和B都为NULL，则返回true，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL A<>B, A!=B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回true，反之返回false A 基本数据类型 A或者B为NULL，则返回NULL；如果A小于B，则返回true，反之返回false A 基本数据类型 A或者B为NULL，则返回NULL；如果A小于等于B，则返回true，反之返回false A>B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于B，则返回true，反之返回false A>=B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于等于B，则返回true，反之返回false A [NOT] BETWEEN B AND C 基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为true，反之为false。如果使用NOT关键字则可达到相反的效果。 A IS NULL 所有数据类型 如果A等于NULL，则返回true，反之返回false A IS NOT NULL 所有数据类型 如果A不等于NULL，则返回true，反之返回false IN(数值1, 数值2) 所有数据类型 使用 IN运算显示列表中的值 A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。like不是正则，而是通配符 A RLIKE B, A REGEXP B STRING 类型 B是一个正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。 逻辑运算符 操作符 操作 描述 A AND B 逻辑并 如果A和B都是true则为true，否则false A OR B 逻辑或 如果A或B或两者都是true则为true，否则false NOT A 逻辑否 如果A为false则为true，否则false 查询 -- 全表查询 select * from myhive.stu1; -- 选择特定字段查询 select id,name from myhive.stu1; -- 重命名字段 select id,name as stuName from myhive.stu1; -- as可以省略 select id,name stuName from myhive.stu1; -- 限制返回行数 select * from myhive.score limit 5; -- 条件过滤 select * from myhive.score where s_score > 60; 函数 -- 求总行数 select count(*) cnt from myhive.score; -- 求某一字段的最大值 select max(s_score) from myhive.score; -- 求某一字段的最小值 select min(s_score) from myhive.score; -- 求字段值的总和 select sum(s_score) from myhive.score; -- 求字段值的平均数 select avg(s_score) from myhive.score; 分组 Group By语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。 group by -- 先按s_id分组,在对字段s_score求平均数 select s_id, avg(s_score) from myhive.score group by s_id; having having 与 where 不同点 where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据 where后面不能使用分组函数，而having后面可以使用分组函数 having只用于group by分组统计语句 select s_id, avg(s_score) as avgScore from myhive.score group by s_id having avgScore > 60; 连接 Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。 表的别名 使用别名可以简化查询 使用表名前缀可以提高执行效率 -- 创建表 create table myhive.course (c_id string, c_name string, t_id string) row format delimited fields terminated by '\\t'; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/course.csv' overwrite into table myhive.course; -- join查询 select * from myhive.teacher t join myhive.course c on t.t_id = c.t_id; 内连接 inner join 只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来 use myhive; select * from teacher t inner join course c on t.t_id = c.t_id; 左外连接 left outer join join操作符左边表中符合where子句的所有记录将会被返回 如果右边表的指定字段没有符合条件的值，就使用null值替代 use myhive; select * from teacher t left outer join course c on t.t_id = c.t_id; 右外连接 right outer join join操作符右边表中符合where子句的所有记录将会被返回 如果左边表的指定字段没有符合条件的值，就使用null值替代 use myhive; select * from teacher t right outer join course c on t.t_id = c.t_id; 满外连接 full outer join 将会返回所有表中符合where语句条件的所有记录 如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代 use myhive; select * from teacher t full outer join course c on t.t_id = c.t_id; 多表连接 use myhive; select * from teacher t left join course c on t.t_id = c.t_id left join score s on c.c_id = s.c_id left join stu1 on s.s_id = stu1.id; 排序 order by 全局排序 全局排序，只有一个reduce asc (ascend) 升序 （默认）、desc (descend) 降序 order by 子句在select语句的结尾 use myhive; -- 降序 select * from score s order by s_score desc; -- 聚合函数别名排序 select s_id, avg(s_score) avgscore from score group by s_id order by avgscore desc; sort by 局部排序 每个reducer内部进行排序，对全局结果集来说不是排序 use myhive; -- 设置参数 set mapreduce.job.reduces=3; -- 3个ReduceTask内部有序，而对于整个数据结果是无序的 select * from score s sort by s.s_score; distribute by 分区排序（MR） 类似MR中partition，采用hash算法，在map端将查询的结果中指定字段的hash值相同的结果分发到对应的reduce文件中 结合sort by使用 distribute by 语句要写在 sort by 语句之前 use myhive; set mapreduce.job.reduces=3; -- 期望3个分区文件，且内部有序 insert overwrite local directory '/home/hadoop/hivedatas/distribute' row format delimited fields terminated by '\\t' select * from score distribute by s_id sort by s_score; cluster by 桶排序 当 distribute by 和 sort by 字段相同时，可以使用 cluster by 方式代替 use myhive; insert overwrite local directory '/home/hadoop/hivedatas/cluster' row format delimited fields terminated by '\\t' select * from score cluster by s_score; Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-09 20:22:23 "},"src/bigdata/hbase/HBase集群安装部署.html":{"url":"src/bigdata/hbase/HBase集群安装部署.html","title":"HBase集群安装部署","keywords":"","body":"先决条件 安装对应版本的hadoop集群并启动 安装对应版本的zookeeper集群并启动 服务规划 IP HMaster 备份HMaster HRegionServer 192.168.1.100 node01 √ √ 192.168.1.101 node02 √ √ 192.168.1.102 node03 √ 安装 # 上传压缩包到node01 scp hbase-1.2.0-cdh5.14.2.tar.gz hadoop@192.168.2.100:/bigdata/soft # 解压缩 tar -xzvf hbase-1.2.0-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置 hbase-env.sh cd /bigdata/install/hbase-1.2.0-cdh5.14.2/conf vi hbase-env.sh export JAVA_HOME=/bigdata/install/jdk1.8.0_141 export HBASE_MANAGES_ZK=false hbase-site.xml vi hbase-site.xml hbase.rootdir hdfs://node01:8020/hbase hbase.cluster.distributed true hbase.master.port 16000 hbase.zookeeper.quorum node01,node02,node03 hbase.zookeeper.property.clientPort 2181 hbase.zookeeper.property.dataDir /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas zookeeper.znode.parent /hbase regionservers vi regionservers 指定HBase集群的从节点；原内容清空，添加如下三行 node01 node02 node03 back-masters 创建back-masters配置文件，包含备份HMaster节点的主机名，每个机器独占一行，实现HMaster的高可用 vi backup-masters 将node02作为备份的HMaster节点 node02 分发 cd /bigdata/install scp -r hbase-1.2.0-cdh5.14.2/ node02:$PWD scp -r hbase-1.2.0-cdh5.14.2/ node03:$PWD 创建软连接 注意：三台机器均做如下操作 因为HBase集群需要读取hadoop的core-site.xml、hdfs-site.xml的配置文件信息，所以我们三台机器都要执行以下命令，在相应的目录创建这两个配置文件的软连接 ln -s /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml /bigdata/install/hbase-1.2.0-cdh5.14.2/conf/core-site.xml ln -s /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hdfs-site.xml /bigdata/install/hbase-1.2.0-cdh5.14.2/conf/hdfs-site.xml 添加HBase环境变量 注意：三台机器均做如下操作 sudo vi /etc/profile 文件末尾添加如下内容 export HBASE_HOME=/bigdata/install/hbase-1.2.0-cdh5.14.2 export PATH=$PATH:$HBASE_HOME/bin 重新编译/etc/profile使环境变量立即生效 source /etc/profile HBase启动和停止 启动 # node01 执行 # 启动 hdfs start-dfs.sh # node01 node02 node03 分别执行 # 启动 zookeeper # 检查 zkServer.sh status zkServer.sh start # node01 执行 # 启动 hbase start-hbase.sh 停止 # node01执行 # 停止 hbase stop-hbase.sh # node01 node02 node03 分别执行 # 停止 zookeeper zkServer.sh stop # node01 执行 # 停止 hdfs stop-dfs.sh 访问web页面 http://node01:60010 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-21 15:46:35 "},"src/bigdata/hbase/大数据数据库之HBase.html":{"url":"src/bigdata/hbase/大数据数据库之HBase.html","title":"大数据数据库之HBase","keywords":"","body":"核心概念 概念 HBase基于Google的BigTable论文，是建立的HDFS之上，提供高可靠性、高性能、列存储、可伸缩、实时读写的分布式数据库系统。 在需要实时读写随机访问超大规模数据集时，可以使用HBase。 特点 海量存储 可以存储大批量的数据 列式存储 HBase表的数据是基于列族进行存储的，列族是在列的方向上的划分 极易扩展 底层依赖HDFS，当磁盘空间不足的时候，只需要动态增加DataNode节点就可以了 可以通过增加服务器来对集群的存储进行扩容 高并发 支持高并发的读写请求 稀疏 稀疏主要是针对HBase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的 数据的多版本 HBase表中的数据可以有多个版本值，默认情况下是根据版本号去区分，版本号就是插入数据的时间戳 数据类型单一 所有的数据在HBase中是以字节数组进行存储 数据模型 rowkey 行键 table的主键，table中的记录按照rowkey的字典序进行排序 rowkey可以是任意字符串，最大长度是 64KB，实际应用中长度一般为 10-100bytes。 column family 列族 HBase表中的每个colum都归属于某个column family column family是表的schema的一部分（而column不是），即建表时至少指定一个列族 timestamp 时间戳 可以对表中的cell多次赋值，每次赋值操作的timestamp可看成cell值的版本号version numbe 一个cell可以有多个版本的值 column qualifier 列限定 列限定是表的某一列族下的一个列，用 列族名:列名 表示 属于某一个column family，类似于mysql当中创建的具体的列 cell 单元格 根据 {rowkey, column(=family+qualifier), timestamp} 可以映射到一个对应的cell，cell是HBase存储数据的具体地址 cell中的数据是没有类型的，全部是以字节数组进行存储 整体架构 Client Client是操作HBase集群的入口 对于管理类的操作，如表的增、删、改，Client通过RPC与HMaster通信完成 对于表数据的读写操作，Client通过RPC与HRegionServer通信读写数据 Client类型 HBase shell Java编程接口 Thrift、Avro、Rest等 ZooKeeper集群 实现了HMaster的高可用，多HMaster间进行主备选举 保存了HBase的元数据信息meta表，提供了HBase表中region寻址入口数据 对HMaster和HRegionServer实现了监控 ZooKeeper如何协调HMaster和HRegionServer工作？ 多HMaster会竞争创建ephemeral节点，而Zookeeper决定谁是第一个作为在线的HMaster，保证线上只有一个 HMaster。在线HMaster（active HMaster）会给Zookeeper发送心跳，不在线的待机HMaster（inactive HMaster）会监听active HMaster可能出现的故障并随时准备上线。 每个HRegionServer都会创建一个ephemeral 节点。HMaster会监控这些节点来发现可用的HRegionServer，同样它也会监控这些节点是否出现故障。 如果有一个HRegionServer或者HMastet出现故障或各种原因导致发送心跳失败，它们与Zookeeper的session就会过期，这个ephemeral节点就会被删除下线，监听者们就会收到这个消息。Active HMaster监听的是HRegionServer下线的消息，然后会恢复故障的HRegionServer以及它所负责的Region数据。而Inactive HMaster关心的则是active HMaster下线的消息，然后竞争上线变成active HMaster。 HMaster HBase集群也是主从架构，HMaster是主角色 负责Table的管理工作，管理Client对Table的增删改操作 负责Region的管理工作 在Region分裂后，负责将新Region分配到指定的HRegionServer 管理HRegionServer间的负载均衡，迁移region分布 当HRegionServer宕机后，负责将其上的region迁移 监控集群中所有HRegionServer（从Zookeeper获取通知信息） HRegionServer HRegionServer是从角色，负责管理一系列的Region（大约可以管理1000个Region） 响应Client的读写数据请求，通常情况下HRegionServer同DataNode同机部署，这样就可以实现数据的本地化 切分在运行过程中变大的Region Region HBase集群中分布式存储的最小单元 一个Region对应一个Table表的部分数据，也可以是全部数据；Table表根据rowkey的范围被水平拆分为多个Region，每个Region都包含了这个Region的start key和end key之间的所有行，然后被分配给集群中的HRegionServer来管理 HBase shell 命令 # 进入 hbase shell 交互客户端 hbase shell 基本命令 help 显示帮助 # 显示帮助 help # 查看具体命令的帮助 help 'create' exit 退出 # 退出shell exit # 当输入语法错误，导致client不工作 >` create 创建表 # 创建user表，info和data列族 # 默认一个版本 create 'user', 'info', 'data' # 创建user1表，同时指定列族的版本数 create 'user1', {NAME => 'info', VERSIONS => 3}, {NAME => 'data'} put 插入和更新单行数据 # 插入数据 # row key为rk0001，列族info中添加名为name的列，值为zhangsan # column qualifier 在插入数据时指定 put 'user', 'rk0001', 'info:name', 'zhangsan' put 'user', 'rk0001', 'info:gender', 'female' put 'user', 'rk0001', 'info:age', 20 put 'user', 'rk0001', 'data:pic', 'picture' put 'user', 'rk0002', 'info:name', 'fanbingbing' put 'user', 'rk0002', 'info:gender', 'female' put 'user', 'rk0002', 'info:nationality', '中国' # 更新数据 # cell有数据则更新，无数据则插入；本质还是插入数据，只不过查询数据时，显示最新版本号的数据 put 'user', 'rk0001', 'info:name', 'lisi' get 查询单行数据 # 获取user表中row key为rk0001的所有 cell 信息 get 'user', 'rk0001' # 过滤列族 get 'user', 'rk0001', 'info' get 'user', 'rk0001', 'info', 'data' get 'user', 'rk0001', {COLUMN => ['info', 'data']} # 过滤列 get 'user', 'rk0001', 'info:age' get 'user', 'rk0001', 'info:age', \"info:name\" get 'user', 'rk0001', {COLUMN => ['info:name', 'data:pic']} # 过滤值 get 'user', 'rk0001', {FILTER => \"ValueFilter(=, 'binary:zhangsan')\"} # 过滤值包含中文 get 'user', 'rk0002', {FILTER => \"ValueFilter(=, 'binary:中国')\"} # 过滤列限定名称 # 列名称包含a get 'user', 'rk0001', {FILTER => \"QualifierFilter(=,'substring:a')\"} scan 全表扫描 # 查看user表的所有数据 scan 'user' # 过滤列族 scan 'user', {COLUMNS => 'info'} # RAW => true 已被标记为删除，但还没有被删除 # 不能包含列限定 scan 'user', {COLUMNS => 'info', RAW => true, VERSIONS => 5} scan 'user', {COLUMNS => ['info', 'data']} # 过滤列 scan 'user', {COLUMNS => 'info:name'} scan 'user', {COLUMNS => ['info:name', 'data:pic']} # 版本最新的5个 scan 'user', {COLUMNS => 'info:name', VERSIONS => 5} # 过滤列限定名称 scan 'user', {COLUMNS => ['info', 'data'], FILTER => \"(QualifierFilter(=,'substring:a'))\"} # 指定行键范围 [rk0001, rk0003) scan 'user', {COLUMNS => 'info', STARTROW => 'rk0001', ENDROW => 'rk0003'} # row key 模糊查询 scan 'user', {FILTER => \"PrefixFilter('rk')\"} # 指定时间戳范围 [1581477006014, 1581477079033) scan 'user', {TIMERANGE => [1581477006014, 1581477079033]} delete 删除数据 # 注意当有多个版本时，自上向下逐个删除（逻辑删除，标记类型为Delete）；如一个cell有三个版本：ts3、ts2、ts1，第一次删除ts3版本，get查询时会显示ts2版本的数据 # 可以通过 RAW => true 查询标记为删除，但没有被物理删除的数据 delete 'user', 'rk0001', 'info:name' # 指定删除版本号 delete 'user', 'rk0001', 'info:name', 1581489923991 管理命令 status 显示服务器状态 status 'node01' whoami 显示当前用户 whoami list 显示数据库 list count 统计表行数据 count 'user' describe 显示表结构信息 describe 'user' exists 显示表是否存在 exists 'user' is_enabled 显示表是否启用 / is_disabled 显示表是否禁用 # 显示表是否启用 is_enabled 'user' # 显示表是否禁用 is_disabled 'user' alter 修改表结构 # 增加列族 alter 'user', NAME => 'data' # 等价 alter 'user', 'data' # 删除指定列族 alter 'user', NAME => 'data', METHOD => 'delete' alter 'user', 'delete' => 'data' # 修改列族 # 列族info版本数修改为5 alter 'user', NAME => 'info', VERSIONS => 5 enable 启用表 / disable 禁用表 # 禁用表 disable 'user' # 启用表 enable 'user' drop 删除表 # 先禁用，再删除 disable 'user1' drop 'user1' truncate 清空表数据 # 禁用表 -> 删除表 -> 创建表 truncate 'user' 核心原理 HBase数据存储原理 Region 一个HRegionServer负责管理多个Region 初始情况下，一个表只有一个Region或直接对表预分区。当随着数据增大，一个Region会分裂为两个Region 一个Region只对应一个表，而一个表可以有多个Region Store 一个Region包含多个Store，而一个column family对应一个Store 如果一个表中只有一个column family，那么Region中只会有一个Store；如果一个表有N个column family，那么Region中会有N个Store 为什么column family不应设置过多，最少一个，最多不超过两个？ 当列族过多且数据不均或均匀时，Region分裂，不同的列族会分裂到多个Region上造成某一列族数据过少，导致查询此列族上的数据可能会跨越多个Region，查询效率降低。 每一个列族都有一个MemoStore，导致内存消耗过多。 当一个Region中的Store刷写缓存或压缩时，其他Store会跟着一同操作，导致IO频繁。 MemStore 一个Store仅包含一个MemStore 写缓存，在写入数据时，会先写入MemStore缓存，然后在把数据刷写到磁盘 HFile 一个Store包含多个HFile。其中，StoreFile是HFile的抽象，最后是以HFile数据结构（有序KeyValue）存储在HDFS上 当每次MemStore超过一个阈值时，就会溢写到磁盘，对应就是生成一个HFile文件 HBase读数据流程 Client与Zookeeper连接，获取Meta表的位置信息，即Meta表存储在哪一个HRegionServer上。 HBase集群只有一张Meta表（B Tree），此表只有一个Region。保存了系统中所有Region的位置信息。结构如下： Key：table, region start key, region id Value：region server 可以通过 scan 'hbase:meta' 来查看Meta表信息 Client与Meta表所在的HRegionServer连接，进而获取请求rowkey所在Region的位置信息。 在Client缓存Meta表的位置信息，以及rowkey所在Region的位置信息，后续请求直接使用Meta Cache即可。除非Region迁移导致缓存失效，则需要重新获取相关位置信息并更新Client的Meta Cache。 Client同rowkey所在Region的HRegionServer连接，查找并定位所在的Region。首先在MemStore查找数据；如果没有，再从BlockCache上查找；如果没有，再到HFile上进行查找。 MemStore是写缓存 BlockCache是读缓存，是 LRU（Least Recently Used）缓存。 从HFile读取到数据后，先写入到BlockCache中加快后续查找，然后再将结果返回给Client。 HBase写数据流程 Client与Zookeeper连接，获取Meta表的位置信息，即Meta表存储在哪一个HRegionServer上。 Client与Meta表所在的HRegionServer连接，进而获取请求rowkey所在Region的位置信息。 Client同rowkey所在Region的HRegionServer连接，查找并定位所在的Region。首先在HLog上预写日志，然后写入MemStore缓存。 HLog也称为WAL，意为Write ahead log。类似mysql中的binlog，用来做灾难恢复时用，HLog记录数据的所有变更，相当于MemStore的一份快照。一旦MemStore数据丢失，就可以从HLog中恢复。 预写日志和写入MemStore的顺序不可调换，否则内存数据一旦丢失将无法恢复。 HLog以SequenceFile的形式存储，修改和删除数据本质都是增加，并且是在文件末尾顺序追加，因此磁盘响应速度很快，同时也可以解决HRegionServer崩溃导致MemStore数据丢失的问题。 返回Client确认写入成功。之后便可以查询此数据。 当MemStore达到阈值后，会将数据刷写到磁盘持久化，生成相应的HFile文件。同时，将HLog中的历史数据删除。 HBase的flush机制 MemStore 中累积了足够多的的数据后，整个有序数据集就会被写入一个新的 HFile 文件到 HDFS 上。HBase 为每个 Column Family 都创建一个 HFile，里面存储了具体的 Cell，也即 KeyValue 数据。随着时间推移，HFile 会不断产生，因为 KeyValue 会不断地从 MemStore 中被刷写到硬盘上。 注意这也是为什么 HBase 要限制 Column Family 数量的一个原因。每个 Column Family 都有一个 MemStore；如果一个 MemStore 满了，所有的 MemStore 都会被刷写到硬盘。 同时，它也会记录最后写入的数据的最大序列号（sequence number），这样系统就能知道目前为止哪些数据已经被持久化了。最大序列号是一个 meta 信息，被存储在每个 HFile 中，来表示持久化进行到哪条数据了，应该从哪里继续。当 Region 启动时，这些序列号会被读取，取其中最大的一个，作为基础序列号，后面的新的数据更新就会在该值的基础上递增产生新的序列号。这个序列号还可以用于从HLog中的什么位置开始恢复数据。 flush触发条件 MemoStore级别限制 当Region中任意一个MemStore的大小达到了上限（hbase.hregion.memstore.flush.size，默认128MB），会触发MemStore刷新。 hbase.hregion.memstore.flush.size 134217728 region级别限制 当Region中所有MemoStore的大小总和达到了上限（hbase.hregion.memstore.block.multiplier hbase.hregion.memstore.flush.size，默认 2 128M = 256M），会触发MemStore刷新。 hbase.hregion.memstore.flush.size 134217728 hbase.hregion.memstore.block.multiplier 2 Region Server级别限制 当HRegionServer中所有MemStore的大小总和超过低水位阈值hbase.regionserver.global.memstore.size.lower.limit*hbase.regionserver.global.memstore.size（前者默认值0.95），HRegionServer开始强制flush；首先刷新MemStore最大的Region，再执行次大的，依次执行。 如写入速度大于flush的速度，导致总MemStore大小超过高水位阈值hbase.regionserver.global.memstore.size（默认为JVM内存的40%），此时HRegionServer会阻塞更新并强制执行flush，直到总MemStore大小低于低水位阈值。 hbase.regionserver.global.memstore.size.lower.limit 0.95 hbase.regionserver.global.memstore.size 0.4 HLog数量上限 当一个HRegionServer中HLog数量达到上限（可通过参数hbase.regionserver.maxlogs配置）时，系统会选取最早的一个 HLog对应的一个或多个Region进行flush。 定期刷新 默认周期为1小时，确保MemStore不会长时间没有持久化。为避免所有的MemStore在同一时间都进行flush导致的问题，定期的flush操作有20000左右的随机延时。 手动flush 用户可以通过shell命令 flush ‘tablename’ 或者 flush ‘region name’ 分别对一个表或者一个Region进行flush。 flush 流程 为了减少flush过程对读写的影响，将整个flush过程分为三个阶段： prepare阶段（写阻塞，内存操作，时间短）：遍历当前Region中所有的MemStore，将MemStore中当前数据集CellSkipListSet做一个快照snapshot；然后再新建一个CellSkipListSet。后期写入的数据都会写入新的CellSkipListSet中。prepare阶段需要加锁updateLock对写请求阻塞，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。 flush阶段（涉及到磁盘IO，耗时）：遍历所有MemStore，将prepare阶段生成的snapshot持久化为临时文件，临时文件会统一放到目录.tmp下。这个过程因为涉及到磁盘IO操作，因此相对比较耗时。 commit阶段（耗时）：遍历所有MemStore，将flush阶段生成的临时文件移到指定的ColumnFamily目录下，针对HFile生成对应的storefile和Reader，把storefile添加到HStore的storefiles列表中，最后再清空prepare阶段生成的snapshot。 HBase的compact机制 hbase为了防止小文件过多，以保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。在hbase中主要存在两种类型的compaction合并： minor compaction 小合并 major compaction 大合并 minor compaction 小合并 将Store中多个HFile合并为一个HFile 在这个过程中会选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，对于TTL过期数据、更新的数据、删除的数据仅仅只是做了标记，并没有进行物理删除。一次Minor Compaction的结果是更少并且更大的StoreFile。这种合并的触发频率很高。 minor compaction触发条件由以下几个参数共同决定 hbase.hstore.compactionThreshold 3 hbase.hstore.compaction.max 10 hbase.hstore.compaction.min.size 134217728 hbase.hstore.compaction.max.size 9223372036854775807 major compaction 大合并 合并Store中所有的HFile为一个HFile 将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL（time to live）过期数据、版本号超过设定版本号的数据。合并频率比较低，默认7天执行一次，并且性能消耗非常大（会遍历所有数据）。建议生产关闭，在应用空闲时间手动触发，防止出现在业务高峰期。 major compaction触发时间条件 hbase.hregion.majorcompaction 604800000 手动触发 # 使用major_compact命令 major_compact ‘tableName’ Region分裂 分裂流程 一开始每个 table 默认只有一个 Region。当一个 Region 逐渐变得大时，大小大于 hbase.hregion.max.filesize ，会分裂（split）成两个子 Region，每个子 Region 都包含了原来 Region 一半的数据，这两个子 Region 并行地在原来这个 HRegionServer 上创建，这个分裂动作会被报告给 HMaster。出于负载均衡的目的，HMaster 可能会将新的 Region 迁移给其它 HRegionServer。 hbase.hregion.max.filesize 10737418240 Splitting 一开始是发生在同一台 HRegionServer 上的，但是出于负载均衡的原因，HMaster 可能会将新的 Region 迁移 （注意是逻辑上的迁移，即将某个Region给另一个HRegionServer管理）到其他 HRegionServer，这会导致此时迁移到的HRegionServer需要访问离它比较远的 HDFS 数据，直到 major compaction 的到来，它会将那些远方的数据重新移回到离HRegionServer节点附近的地方。 分裂策略 手动指定 当一个table刚被创建的时候，Hbase默认分配一个Region给table。也就是说这个时候，所有的读写请求都会访问到同一个HRegionServer的同一个Region中，这个时候就达不到负载均衡的效果，集群中的其他HRegionServer就可能会处于空闲的状态。解决这个问题可以用pre-splitting，在创建table的时候就配置生成多个Region。 为什么要预分区？ 增加数据读写效率 负载均衡，防止数据倾斜 方便集群容灾调度Region 优化Map数量 预分区原理 每一个Region维护着startRowKey与endRowKey，如果加入的数据符合某个Region维护的rowKey范围，则该数据交给这个Region维护。 手动指定预分区 方式一 create 'person','info1','info2',SPLITS => ['1000','2000','3000','4000'] 共有5个Region | name | start key | end key | range | | ------- | --------- | ------- | ------------ | | Region1 | | 1000 | [, 1000) | | Region2 | 1000 | 2000 | [1000, 2000) | | Region3 | 2000 | 3000 | [2000, 3000) | | Region4 | 3000 | 4000 | [3000, 4000) | | Region5 | 4000 | | [4000, ) | 注意： 首行和尾行的rowkey是空串 Region的rowkey范围包括start key，不包括end key 方式二 将分区规则创建于文件中 cd /bigdata/install vi splits # 文件内容 aaa bbb ccc ddd shell执行命令 create 'student', 'info', SPLITS_FILE => '/bigdata/install/splits' 方式三 HexStringSplit会将数据从 “00000000” 到 “FFFFFFFF” 之间的数据长度按照n等分之后算出每一段开始rowkey和结束rowkey，以此作为拆分点。 create 'mytable', 'base_info', 'extra_info', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'} 系统自动 ConstantSizeRegionSplitPolicy 0.94版本前，HBase region的默认切分策略 当region中最大的store大小超过某个阈值(hbase.hregion.max.filesize=10G)之后就会触发切分，一个region等分为2个region。 但是在生产线上这种切分策略却有相当大的弊端： 切分策略对于大表和小表（指HFile文件大小）没有明显的区分。 阈值(hbase.hregion.max.filesize)设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，形成热点，这对业务来说并不是什么好事。 如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的region，这对于集群的管理、资源使用、failover来说都不是一件好事。 IncreasingToUpperBoundRegionSplitPolicy 0.94版本~2.0版本默认切分策略 总体看和ConstantSizeRegionSplitPolicy思路相同 一个region中最大的store大小大于设置阈值就会触发切分。 但是这个阈值并不像ConstantSizeRegionSplitPolicy是一个固定的值，而是会在一定条件下不断调整，调整规则和region所属表在当前regionserver上的region个数有关系. region split阈值的计算公式是： 设regioncount：是region所属表在当前regionserver上的region的个数 阈值 = regioncount^3 128M 2，当然阈值并不会无限增长，最大不超过MaxRegionFileSize（10G）；当region中最大的store的大小达到该阈值的时候进行region split 例如： 第一次split阈值 = 1^3 256 = 256MB 第二次split阈值 = 2^3 256 = 2048MB 第三次split阈值 = 3^3 256 = 6912MB 第四次split阈值 = 4^3 256 = 16384MB > 10GB，因此取较小的值10GB 后面每次split的size都是10GB了 也就是说前三次触发切分的阈值比较适合小表，避免热点问题，分发给三个HRegionServer管理。当满足第四次切分条件时，就需要满足大表条件。 特点 相比ConstantSizeRegionSplitPolicy，可以自适应大表、小表； 在集群规模比较大的情况下，对大表的表现比较优秀 但是，它并不完美，小表可能产生大量的小region，分散在各regionserver上（大合并后Region会迁移到其他的HRegionServer上） SteppingSplitPolicy 2.0版本默认切分策略 相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些 region切分的阈值依然和待分裂region所属表在当前regionserver上的region个数有关系 如果region个数等于1，切分阈值为flush size 128M * 2 否则为MaxRegionFileSize。 这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小region，而是适可而止。 KeyPrefixRegionSplitPolicy 根据rowKey的前缀对数据进行分区，这里是指定rowKey的前多少位作为前缀，比如rowKey都是16位的，指定前5位是前缀，那么前5位相同的rowKey在相同的region中。 DelimitedKeyPrefixRegionSplitPolicy 保证相同前缀的数据在同一个region中，例如rowKey的格式为：userideventtype_eventid，指定的delimiter为 ，则split的的时候会确保userid相同的数据在同一个region中。 DisabledRegionSplitPolicy 不启用自动拆分， 需要指定手动拆分 Region合并 Region的合并不是为了性能，而是出于维护的目的。如删除了大量的数据，这个时候每个Region都变得很小，存储多个Region浪费资源，这个时候可以把Region合并起来，进而可以减少一些Region服务器节点。 通过Merge类冷合并Region 创建一张hbase表 create 'test', 'info1', SPLITS => ['1000','2000','3000'] 关闭集群 stop-hbase.sh 执行命令 hbase org.apache.hadoop.hbase.util.Merge test region1 region2 通过online命令热合并Region 与冷合并不同的是，online_merge的传参是Region的hash值，而Region的hash值就是Region名称最后两个.之间的字符串部分 hbase shell 执行命令 merge_region 'region1_hash', 'region2_hash' 系统集成 与MapReduce集成 利用MapReduce的分布式计算，提高数据导入HBase表效率 与HRegionServer交互，通过集成HBase框架的TableMapper和TableReducer实现。 HBase表到HBase表 Hdfs文件到HBase表 写入HBase数据时，同直接调用API写数据流程类似，仍需要占用HRegionServer大量资源。 不与HRegionServer交互，通过MapReduce直接将数据输出为HBase识别的HFile文件格式，然后再加载到HBase表中。 hadoop jar 在集群运行程序时可能会找不到hbase类，则需要做如下配置 一次生效（建议） export HBASE_HOME=/bigdata/install/hbase-1.2.0-cdh5.14.2/ export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2/ export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp` 永久生效 ```bash # 修改hadoop-env.sh # add hbase lib if [ -z $HBASE_HOME ]; then export HADOOP_CLASSPATH=${HADOOP_CLASSPATH} else export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${HBASE_HOME}/lib'/*' fi # 配置即时生效 source hadoop-env.sh ``` 执行成功输出hdfs文件 /test/hbase/huser/cf/21dc0757cbfe41a7bb8552818d0478ba 会生成目标列族 cf 下的HFile HBase在hdfs存储表默认在 /hbase/data/default 目录下 如/hbase/data/default/user/36c7b176416c41bb1534676a2b50fdf9/info/03ccf327a86649ba9fee7ed734eafe56 user 为表名 36c7b176416c41bb1534676a2b50fdf9 为RegionId info 为列族名 03ccf327a86649ba9fee7ed734eafe56 是HFile 加载到表。清空 /test/hbase/huser/cf 目录下数据，转移到表相应Region的列族下 /hbase/data/default/user1/031ed91d57c19615d009b21fde809f57/cf/ ae8b0ffabb374946922ef704b9f4a918_SeqId_5_ 与Hive集成 概述 Hive 数据仓库 Hive的本质相当于将HDFS中已经存储的文件在Mysql中做了一个映射，以方便使用HQL去管理查询 用于数据分析、清洗 Hive适用于离线的数据分析和清洗，延迟较高 基于HDFS、MapReduce Hive存储的数据依旧在DataNode上，编写的HQL语句最终转换为MapReduce代码执行 HBase 数据库 是一种面向列存储的非关系型数据库 用于存储结构化和非结构化的数据 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作 基于HDFS 数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理 延迟较低，接入在线业务使用 面对大量的企业数据，HBase可以支持单表大量数据的存储，同时提供了高效的数据访问速度 总结：Hive和HBase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，HBase是一种在Hadoop之上的 NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。 整合配置 HBase jar建立软连接到Hive的lib目录下 node03执行 ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar 修改配置 node03执行 hive-site.xml hive.zookeeper.quorum node01,node02,node03 hbase.zookeeper.quorum node01,node02,node03 hive-env.sh export HBASE_HOME=/bigdata/install/hbase-1.2.0-cdh5.14.2/ Hive分析结果保存到HBase表 Hive创建数据库和表 create database course; use course; create external table if not exists course.score(id int, cname string, score int) row format delimited fields terminated by '\\t' stored as textfile; 准备数据 hive-hbase 1 zhangsan 80 2 lisi 60 3 wangwu 30 4 zhaoliu 70 Hive加载数据 load data local inpath '/home/hadoop/hivedatas/hive-hbase' into table score; select * from score; 创建Hive内部表与HBase映射 -- hbase.columns.mapping hbase表的column要和hive表的field一一对应 create table course.hbase_score(id int, cname string, score int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties(\"hbase.columns.mapping\" = \":key, cf:name, cf:score\") tblproperties(\"hbase.table.name\" = \"hbase_score\"); 向Hive内部表插入数据 insert overwrite table course.hbase_score select id, cname, score from course.score; 向HBase表插入数据 # 如果没有向hive映射的field赋值，则为null put 'hbase_score', '10', 'cf:name', 'rain' put 'hbase_score', '10', 'cf:score', 10 总结 存储 数据存储在HBase端，节省存储空间。数据量小的情况下存储在MemStore中，执行 flush 'hbase_score' 刷写到磁盘 同步 当向Hive内部表或HBase表插入数据时，两边都会同步数据 删除 删除Hive内部表，HBase映射表同步删除；但删除HBase映射表，Hive内部表不会同步删除，但查询时会提示HBase映射表不存在 Hive外部表映射HBase已存表进行分析 创建Hive外部表映射HBase表 CREATE external TABLE course.hbase2hive(id int, name string, age int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key, info:name, info:age\") TBLPROPERTIES(\"hbase.table.name\" =\"user\"); 查询Hive外部表 select * from course.hbase2hive; 向Hive外部表插入数据 insert into table hbase2hive values(00000012, 'cat', 2); 向Hbase表插入数据 put 'user','00000011','info:name','wizard' 总结 存储 因为Hbase表映射的是Hive外部表，所以数据存储在HBase端 同步 当向Hive外部表或HBase表插入数据时，两边都会同步数据 删除 删除Hive外部表，不影响Hbase表；删除Hbase表，不会删除Hive外部表，但查询时会提示HBase表不存在，需要手动删除 HBase 的 RowKey 设计 设计原则 长度原则 RowKey 是一个二进制字节流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长 建议尽可能短（提高检索效率），但也不能太短，否则 RowKey 前缀重复的概率增大 设计过长会降低 MemStore 内存的利用率（key/value，RowKey是key的一部分）和HFile存储数据的效率 散列原则 建议将 RowKey 的高位作为散列字段，这样将提高数据均衡分布在每个 RegionServer ，以实现负载均衡 如果没有散列字段，首字段直接是时间信息。所有的数据都会集中在一个 RegionServer 上，这样在数据检索的时候负载会集中在个别的 RegionServer 上，造成热点问题，会降低查询效率 唯一原则 必须在设计上保证其唯一性，RowKey 是按照字典顺序排序存储的。因此，设计 RowKey 的时候，要充分利用这个排序的特点，可以将经常读取的数据存储到一块，将最近可能会被访问的数据存储到一块 热点问题 检索 HBase 记录首先要通过 RowKey 来定位数据行。当大量的 Client 访问 HBase 集群的一个或少数几个节点，造成少数 RegionServer 的读/写请求过多、负载过大，而其他 RegionServer 负载却很小，就造成了“热点”现象。 解决方案 预分区 预分区的目的让表的数据可以均衡的分散在集群中，而不是默认只有一个 Region 分布在集群的一个节点上。 加盐 这里所说的加盐不是密码学中的加盐，而是在 RowKey 的前面增加随机数，具体就是给 RowKey 分配一个随机前缀以使得它和之前的 RowKey 的前缀不同 哈希 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让 Client 重构完整的 RowKey，可以使用 get 操作准确获取某一个行数据，如 rowkey=MD5(username).subString(0,10)+时间戳 反转 反转固定长度或者数字格式的 RowKey，这样可以使得 RowKey 中经常改变的部分（最没有意义的部分）放在前面。可以有效的随机 RowKey，但牺牲了RowKey 的有序性。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-22 11:55:02 "},"src/bigdata/flume/Flume实战.html":{"url":"src/bigdata/flume/Flume实战.html","title":"Flume实战","keywords":"","body":"基本介绍 概述 Flume 是一个分布式、可靠、高可用的海量日志采集、聚合和传输的系统。 Flume 可以采集文件，socket数据包、文件、文件夹、kafka 等各种形式源数据，又可以将采集到的数据（下沉sink）输出到HDFS、hbase、hive、kafka等众多外部存储系统中。 一般的采集需求，通过对 Flume 的简单配置即可实现。Flume 针对特殊场景也具备良好的自定义扩展能力，因此，flume 可以适用于大部分的日常数据采集场景。 运行机制 Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成。每一个 agent 相当于一个数据传递员，内部有三个组件： Source：采集组件，用于跟数据源对接，以获取数据 Sink：下沉组件，用于往下一级agent传递数据或者往最终存储系统传递数据 Channel：传输通道组件，用于从source将数据传递到sink 安装部署 安装 node03执行 scp flume-ng-1.6.0-cdh5.14.2.tar.gz hadoop@node03:/bigdata/soft tar -zxvf flume-ng-1.6.0-cdh5.14.2.tar.gz -C /bigdata/install/ 配置 flume-env.sh cd /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf cp flume-env.sh.template flume-env.sh vi flume-env.sh # 增加java环境变量 export JAVA_HOME=/bigdata/install/jdk1.8.0_141 /etc/profile sudo vi /etc/profile # 增加flume环境变量 export FLUME_HOME=/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin export PATH=$PATH:$FLUME_HOME/bin # 立即生效 source /etc/profile 实战 采集网络端口数据并控制台打印 配置 netcat-memory-logger.conf # 定义这个agent中各组件的名字 a1.sources = r1 a1.sinks = k1 a1.channels = c1 # 描述和配置source组件：r1 a1.sources.r1.type = netcat a1.sources.r1.bind = 192.168.2.102 a1.sources.r1.port = 44444 # 描述和配置sink组件：k1 a1.sinks.k1.type = logger # 描述和配置channel组件，此处使用是内存缓存的方式 a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # 描述和配置source channel sink之间的连接关系 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 # -c conf 指定flume自身的配置文件所在目录 # -f conf/netcat-memory-logger.conf 指定描述的采集方案 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f netcat-memory-logger.conf -n a1 -Dflume.root.logger=INFO,console 测试 # 安装telnet客户端模拟数据发送 sudo yum -y install telnet # 测试 # control+] 回到telnet命令窗口 # quit 退出 telnet node03 44444 采集目录文件到HDFS 配置 spooldir.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source # 注意，不能向监控目中重复放同名文件 a1.sources.r1.type = spooldir a1.sources.r1.spoolDir = /home/hadoop/flumedatas/spooldir a1.sources.r1.fileHeader = true # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://node01:8020/test/flume/spooldir/files/%y-%m-%d/%H%M/ a1.sinks.k1.hdfs.filePrefix = events- # ==== # 时间戳是否需要四舍五入，默认false，true影响所有基于时间的转义字符，除了%t a1.sinks.k1.hdfs.round = true # 时间戳四舍五入的倍数，小于当前时间 a1.sinks.k1.hdfs.roundValue = 10 # 时间戳四舍五入的单位，默认秒 a1.sinks.k1.hdfs.roundUnit = minute # 以上为每隔10分钟产生一个文件 # ==== # 触发滚动文件等待时间（秒），默认30，0不会基于时间滚动文件 # a1.sinks.k1.hdfs.rollInterval = 3 a1.sinks.k1.hdfs.rollInterval = 0 # 触发滚动文件的文件大小（字节），默认1024，0不会基于文件大小滚动文件 # a1.sinks.k1.hdfs.rollSize = 20 a1.sinks.k1.hdfs.rollSize = 0 # 触发滚动文件的事件数量（最小传输单位），默认10，0不会基于事件数量滚动文件 # a1.sinks.k1.hdfs.rollCount = 5 a1.sinks.k1.hdfs.rollCount = 0 # ==== # 刷写到HDFS前的事件数量，默认100 a1.sinks.k1.hdfs.batchSize = 1 # ==== a1.sinks.k1.hdfs.useLocalTimeStamp = true # 生成的文件类型，默认 SequenceFile，可用 DataStream 代替，为普通文本 a1.sinks.k1.hdfs.fileType = DataStream # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 Source：spooldir 监视一个目录（flume部署到数据源一端），只要目录中出现新文件，就会采集文件中的内容 采集完成的文件，会被agent自动添加一个后缀：COMPLETED 所监视的目录中不允许重复出现相同文件名的文件（出错后不会继续运行） Sink：hdfs round、roundValue、roundUnit 控制多长时间生成一个文件，可以控制hdfs上小文件的数量。 rollInterval、rollSize、rollCount 控制滚动生成文件的时间间隔、大小、事件数量，当全部设置为0时，不会基于此三项生成文件。 启动 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f spooldir.conf -n a1 -Dflume.root.logger=INFO,console 测试 spooldir.sh #!/bin/bash i=0 while true do echo $((++i)) >> /home/hadoop/flumedatas/spooldir/$i; sleep 10; done sh spooldir.sh 采集文件到HDFS 配置 taillog.conf agent1.sources = source1 agent1.sinks = sink1 agent1.channels = channel1 # Describe/configure tail -F source1 agent1.sources.source1.type = exec agent1.sources.source1.command = tail -F /home/hadoop/flumedatas/taillog/access_log # Describe sink1 agent1.sinks.sink1.type = hdfs agent1.sinks.sink1.hdfs.path = hdfs://node01:8020/test/flume/taillog/%y-%m-%d/%H-%M agent1.sinks.sink1.hdfs.filePrefix = access_log agent1.sinks.sink1.hdfs.maxOpenFiles = 5000 agent1.sinks.sink1.hdfs.batchSize = 100 agent1.sinks.sink1.hdfs.fileType = DataStream agent1.sinks.sink1.hdfs.writeFormat = Text agent1.sinks.sink1.hdfs.rollSize = 102400 agent1.sinks.sink1.hdfs.rollCount = 1000000 agent1.sinks.sink1.hdfs.roundValue = 10 agent1.sinks.sink1.hdfs.roundUnit = minute agent1.sinks.sink1.hdfs.useLocalTimeStamp = true # Use a channel which buffers events in memory agent1.channels.channel1.type = memory agent1.channels.channel1.keep-alive = 120 agent1.channels.channel1.capacity = 500000 agent1.channels.channel1.transactionCapacity = 600 # Bind the source and sink to the channel agent1.sources.source1.channels = channel1 agent1.sinks.sink1.channel = channel1 tail -F 首次启动时，只加载文件最后10行 常驻系统程序，监控文件末尾追加 Sink：hdfs 最下层文件夹是 小时-分钟 ，也就是说每分钟生成一个文件夹。观察测试数据发现，每分钟会有两个小文件，是因为 hdfs.rollInterval 默认30秒滚动生成新文件。 启动 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f taillog.conf -n agent1 -Dflume.root.logger=INFO,console 测试 taillog.sh #!/bin/bash while true do date >> /home/hadoop/flumedatas/taillog/access_log; sleep 0.5; done sh taillog.sh 断点续传 配置 taildir.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = TAILDIR # record the inode, the absolute path and the last position of each tailing file a1.sources.r1.positionFile = /home/hadoop/flumedatas/taildir_position.json # 可以有多个组，以空格分隔 a1.sources.r1.filegroups = f1 a1.sources.r1.filegroups.f1 = /home/hadoop/flumedatas/taildir/.*log.* # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://node01:8020/test/flume/taildir/files/%y-%m-%d/%H%M/ a1.sinks.k1.hdfs.filePrefix = events- a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = minute a1.sinks.k1.hdfs.rollInterval = 3 a1.sinks.k1.hdfs.rollSize = 5000 a1.sinks.k1.hdfs.rollCount = 50000 a1.sinks.k1.hdfs.batchSize = 5000 a1.sinks.k1.hdfs.useLocalTimeStamp = true a1.sinks.k1.hdfs.fileType = DataStream # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 Sink：TAILDIR 可以监控文件夹下的文件，也可以监控文件内容 断点续传功能，原理是taildir_position.json记录每一次生成事件的文件位置 启动 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f taildir.conf -n a1 -Dflume.root.logger=INFO,console 测试 # > 新增文件写入内容 # >> 追加内容到文件末尾 echo \"testlog1\" >> /home/hadoop/flumedatas/taildir/file.log echo \"testlog2\" >> /home/hadoop/flumedatas/taildir/file.log 级联Agent 安装 node03执行 scp -r apache-flume-1.6.0-cdh5.14.2-bin/ node02:$PWD 配置/etc/profile sudo vi /etc/profile # 增加flume环境变量 export FLUME_HOME=/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin export PATH=$PATH:$FLUME_HOME/bin # 立即生效 source /etc/profile 配置 node02 tail-avro-avro-logger.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/hadoop/flumedatas/taillog/access_log # Describe the sink # 数据发送 a1.sinks.k1.type = avro a1.sinks.k1.hostname = node03 a1.sinks.k1.port = 4141 a1.sinks.k1.batch-size = 10 # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 node03 avro-hdfs.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source # 数据接收 a1.sources.r1.type = avro a1.sources.r1.bind = node03 a1.sources.r1.port = 4141 # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://node01:8020/test/flume/avro/hdfs/%y-%m-%d/%H%M/ a1.sinks.k1.hdfs.filePrefix = events- a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = minute a1.sinks.k1.hdfs.rollInterval = 3 a1.sinks.k1.hdfs.rollSize = 20 a1.sinks.k1.hdfs.rollCount = 5 a1.sinks.k1.hdfs.batchSize = 1 a1.sinks.k1.hdfs.useLocalTimeStamp = true a1.sinks.k1.hdfs.fileType = DataStream # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 # node03 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f avro-hdfs.conf -n a1 -Dflume.root.logger=INFO,console # node02 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f tail-avro-avro-logger.conf -n a1 -Dflume.root.logger=INFO,console 测试 node02 taillog.sh while true do date >> /home/hadoop/flumedatas/taillog/access_log; sleep 0.5; done sh taillog.sh failover 高可用 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-24 11:05:29 "},"src/database/mysql/CentOS7安装MySQL5.7.html":{"url":"src/database/mysql/CentOS7安装MySQL5.7.html","title":"CentOS7安装MySQL5.7","keywords":"","body":"安装MySQL CentOS 7中切换到root用户，安装mysql CentOS 7中默认安装有MariaDB，这个是MySQL的分支；但还是要安装MySQL，安装完成之后会直接覆盖掉MariaDB 安装在node03上 # 切换到root用户 su root # 安装wget cd /bigdata/soft/ yum -y install wget # 使用wget命令下载mysql的rpm包 # -i 指定输入文件 # -c 表示断点续传 wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm # 安装mysql yum -y install mysql57-community-release-el7-10.noarch.rpm # 安装mysql server yum -y install mysql-community-server 设置MySQL 启动服务 # 启动MySQL服务 systemctl start mysqld.service # 查看mysql启动状态 # active（running）表示mysql服务已启动 systemctl status mysqld.service 用临时密码登录 # 找出临时密码 # G;sZ/.i(G7Gt grep \"password\" /var/log/mysqld.log # 使用临时密码，登陆mysql客户端 mysql -uroot -p 修改密码 -- 设置密码策略为LOW，此策略只检查密码的长度 set global validate_password_policy=LOW; -- 设置密码最小长度 set global validate_password_length=4; -- 修改mysql的root用户，本地登陆的密码为root ALTER USER 'root'@'localhost' IDENTIFIED BY 'root'; -- 开启mysql的远程连接权限 grant all privileges on *.* to 'root'@'%' identified by 'root' with grant option; -- 即时生效 flush privileges; -- 退出 exit 卸载MySQL 使用root用户卸载mysql # 停止mysql服务 systemctl stop mysqld.service # 列出已安装的mysql相关的包 # 卸载完成后，用这两个命令再次检查 yum list installed mysql* # 或 rpm -qa | grep -i mysql # 卸载，命令后边依次添加上一步列出的包名，包名之间用空格分隔 rpm -e --nodeps 删除mysql残留文件 # 查看mysql相关目录 find / -name mysql # 删除上一步列出的目录 rm -rf # 删除文件 rm -rf /root/.mysql_history rm -f /var/log/mysqld.log Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-05 16:27:32 "},"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html":{"url":"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html","title":"GitBook&GitHub&Typora最佳实践","keywords":"","body":"概述 目前有众多的知识管理软件，对于比较私密的文档使用印象笔记，不仅支持Markdown这种轻量级的标记语言，也支持复杂的富文本语言，但是一些比较有用的功能则需要收费，比如查看历史版本、离线访问、容量限制。最近，使用了一下阿里的语雀，书写体验、排版和易操作方面确实属于不错的一款产品，可是，某一天断网了，不支持离线访问是硬伤。因此，想要寻找如下一款产品满足如下需求： 沉浸式的书写体验，不需要丰富的社交场景，也不需要过于酷炫的功能，只想安安静静的码字。 离线编辑，想什么时候访问自己做主。同时，同步是必不可少的。 支持版本管理。 不收费，也没有容量限制。想写多少写多少。 支持私有笔记。 支持同其他同事、同学沟通，因为大部分都是程序员，所以可以查看Markdown文件是必不可少的。 对笔记中的目录结构没有限制。 对笔记中关联的图像，本地和远程都可以访问，而且没有容量限制。 可以方便的导出其他格式，比如html，PDF等。 通过调研，使用GitHub&GitBook&Typora方案，可以完美的解决以上问题。 搭建环境 GitHub GitHub创建develop-stack项目，其中.gitignore选择gitbook 回到本地创建本地目录 mkdir note && cd note 下载项目到本地 git clone https://github.com/sciatta/develop-stack.git GitBook 安装nodejs brew install node 安装GitBook命令行工具 npm install -g gitbook-cli 在develop-stack目录执行 gitbook init 初始化Gitbook目录环境，生成两个重要文件README.md和SUMMARY.md Typora 编辑SUMMARY.md描述项目的目录和文件结构 在develop-stack目录执行 gitbook init 。GitBook会查找SUMMARY.md文件中描述的目录和文件，如果没有则会将其创建。注意，如果删除SUMMARY.md描述项目的目录和文件结构，执行 gitbook init 命令不会删除相应的目录或文件，需要手动维护。 GitBook 本地运行 gitbook serve --port 8088 开启GitBook服务。通过 http://localhost:8080 访问本地服务。在执行命令的同时会执行构建命令 gitbook build 生成 _book 目录。注意， _book 目录是临时目录，每次构建时全部重建。如果退出服务的话，执行 control + c 即可 本地构建 gitbook build ./ docs 。 GitHub 执行 git add 命令 git add -A 将文件提交到暂存区 执行 git commit 命令 git commit -m “test” 将文件提交到本地仓库 执行 git push 命令 git push origin master 将文件提交到远程仓库 GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Source选择 master branch/docs folder 。成功后显示 Your site is ready to be published at https://sciatta.github.io/develop-stack/ ，稍后即可访问GitHub Pages网站。 GitBook插件 在develop-stack目录创建book.json文件，配置插件后，运行 gitbook install 命令自动安装插件。 hide-element 隐藏默认gitbook左侧提示：Published with GitBook { \"plugins\": [ \"hide-element\" ], \"pluginsConfig\": { \"hide-element\": { \"elements\": [\".gitbook-link\"] } } } expandable-chapters gitbook默认目录没有折叠效果。 { \"plugins\": [ \"expandable-chapters\" ] } code 在代码区域的右上角添加一个复制按钮，点击一键复制代码。 { \"plugins\" : [ \"code\" ] } splitter 左侧目录和右侧文章可以拖动调节宽度。 { \"plugins\": [ \"splitter\" ] } search-pro 支持中英文。 { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ] } pageview-count 记录每个文章页面被访问的次数。本质是访问 https://hitcounter.pythonanywhere.com/ { \"plugins\": [ \"pageview-count\"] } tbfed-pagefooter 在每个文章下面标注版权信息和文章时间。 { \"plugins\": [ \"tbfed-pagefooter\" ], \"pluginsConfig\": { \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy sciatta.com 2020\", \"modify_label\": \"修订时间: \", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" } } } popup 点击可以在新窗口展示图片。 { \"plugins\": [ \"popup\" ] } auto-scroll-table 为避免表格过宽，增加滚动条。 { \"plugins\": [\"auto-scroll-table\"] } -sharing 去掉 GitBook 默认的分享功能。由于默认的一些推特，脸书都需要翻墙，所以将分享功能全部关闭。 \"plugins\": [ \"-sharing\" ] github-buttons 给 GitBook 添加 GitHub 的图标来显示 star 和 follow。 { \"plugins\": [ \"github-buttons\" ], \"pluginsConfig\": { \"github-buttons\": { \"buttons\": [{ \"user\": \"sciatta\", \"repo\": \"develop-stack\", \"type\": \"star\", \"count\": true, \"size\": \"small\" }] } } } github 在右上角显示 github 仓库的图标链接 { \"plugins\": [ \"github\" ], \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/sciatta\" } } } Google统计 添加Google统计 Google统计 https://analytics.google.com/ 在一个平台上可以全面分析业务数据，进而做出更明智的决策。在网站注册，获取跟踪ID { \"plugins\": [\"ga\"], \"pluginsConfig\": { \"ga\": { \"token\": \"UA-156491400-1\" } } } BaiDu 添加BaiDu统计 百度统计 https://tongji.baidu.com/ 在网站注册，获取跟踪ID { \"plugin\": [\"baidu-v3\"], \"pluginsConfig\": { \"baidu\": { \"token\": \"c6612709c010da681bbd4b785968a638\" } } } Donate Gitbook 捐赠打赏插件 { \"plugins\": [\"donate\"], \"pluginsConfig\": { \"donate\": { \"wechat\": \"/assets/wechat.jpg\", \"alipay\": \"/assets/alipay.jpg\", \"title\": \"\", \"button\": \"捐赠\", \"alipayText\": \" \", \"wechatText\": \" \" } } } anchors 标题带有 github 样式的锚点 { \"plugins\" : [ \"anchors\" ] } anchor-navigation-ex 页面内导航，一键回到顶部。 { \"plugins\": [\"anchor-navigation-ex\"], \"pluginsConfig\": { \"anchor-navigation-ex\": { \"showLevel\": true, \"associatedWithSummary\": false, \"printLog\": false, \"multipleH1\": true, \"mode\": \"float\", \"showGoTop\": true, \"float\": { \"floatIcon\": \"fa fa-navicon\", \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" }, \"pageTop\": { \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" } } } } sitemap 生成站点地图，便于爬虫抓取页面。可以通过http://www.sciatta.com/sitemap.xml 访问。 { \"plugins\": [\"sitemap\"], \"pluginsConfig\": { \"sitemap\": { \"hostname\": \"http://www.sciatta.com/\" } } } Typora设置 偏好设置 | 编辑器 | 图片插入 复制图片到 ./${filename}.assets 文件夹 优先使用相对路径 偏好设置 | 通用 | 启动选项 | 打开指定目录 选择工作目录 偏好设置 | 通用 | 侧边栏 侧边栏的大纲视图允许折叠和展开 绑定域名 获取GitHub Pages的IP地址 ping -c 3 sciatta.github.io 注意，ping的时候不需要加仓库的名称。 配置阿里云 进入阿里云解析列表，添加记录： 记录类型 主机记录 记录值 A @ 185.199.111.153 A www 185.199.111.153 配置GitHub Pages GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Custom domain：www.sciatta.com Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-29 11:46:49 "},"src/virtualization/vmware/VMware系统安装.html":{"url":"src/virtualization/vmware/VMware系统安装.html","title":"VMware系统安装","keywords":"","body":"Mac安装VMware 安装VMware_Fusion_Pro_11.5 注册码 7HYY8-Z8WWY-F1MAN-ECKNY-LUXYX 设置网络 偏好设置 | 网络 | 新建vmnet2自定义网络连接 选择：使用NAT 选择：将MAC主机连接到该网络 取消选择：通过DHCP在该网络上提供地址 修改子网 # 空格需用\\转义 cd /Library/Preferences/VMware\\ Fusion sudo vi networking answer VNET_2_HOSTONLY_SUBNET 192.168.2.0 修改网关 cd vmnet2 sudo vi nat.conf # NAT gateway address ip = 192.168.2.2 偏好设置 | 网络 | vmnet2 取消选择：将MAC主机连接到该网络 | 应用 选择：将MAC主机连接到该网络 | 应用 目的是为了使配置生效。 创建自定义虚拟机 选择操作系统 Linux | Centos 7 64 创建完成后设置 内存：2048 MB 硬盘：40 GB 网络适配器：vmnet2 启动磁盘：CD/DVD（即设置BIOS） 安装Centos7 设置中选择 CD/DVD（IDE） 选中连接CD/DVD驱动器 选择镜像CentOS-7-x86_64-DVD-1810.iso 启动 install Centos 7 language：english date & time：Asia/shanghai installation destination：automatic partitioning selected network & hostname：ens33 | on root password：root（太短，双击确认即可） Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 16:52:11 "},"src/virtualization/vmware/VMware部署Hadoop生态环境.html":{"url":"src/virtualization/vmware/VMware部署Hadoop生态环境.html","title":"VMware部署Hadoop生态环境","keywords":"","body":"环境准备 准备三台虚拟机 ip设置 vi /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=\"static\" IPADDR=192.168.2.100 NETMASK=255.255.255.0 GATEWAY=192.168.2.2 DNS1=192.168.2.2 准备三台linux机器，IP地址分别设置成为 第一台机器IP地址：192.168.2.100 第二台机器IP地址：192.168.2.101 第三台机器IP地址：192.168.2.102 关闭防火墙 root用户下执行 systemctl stop firewalld systemctl disable firewalld 关闭selinux root用户下执行 vi /etc/selinux/config SELINUX=disabled 更改主机名 vi /etc/hostname node01 第一台主机名更改为：node01 第二台主机名更改为：node02 第三台主机名更改为：node03 更改主机名与IP地址映射 vi /etc/hosts 192.168.2.100 node01 192.168.2.101 node02 192.168.2.102 node03 同步时间 定时同步阿里云服务器时间 yum -y install ntpdate crontab -e */1 * * * * /usr/sbin/ntpdate time1.aliyun.com 添加用户 三台linux服务器统一添加普通用户hadoop，并给以sudo权限，用于以后所有的大数据软件的安装 并统一设置普通用户的密码为 hadoop useradd hadoop passwd hadoop 为普通用户添加sudo权限 visudo hadoop ALL=(ALL) ALL 定义统一目录 定义三台linux服务器软件压缩包存放目录，以及解压后安装目录，三台机器执行以下命令，创建两个文件夹，一个用于存放软件压缩包目录，一个用于存放解压后目录 # root 用户执行 mkdir -p /bigdata/soft # 软件压缩包存放目录 mkdir -p /bigdata/install # 软件解压后存放目录 chown -R hadoop:hadoop /bigdata # 将文件夹权限更改为hadoop用户 安装JDK 使用hadoop用户来重新连接三台机器，然后使用hadoop用户来安装jdk软件。上传压缩包到第一台服务器的/bigdata/soft下面，然后进行解压，配置环境变量，三台机器都依次安装 ```bash # 分别上传jdk文件 scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.100:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.101:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.102:/bigdata/soft cd /bigdata/soft/ tar -zxf jdk-8u141-linux-x64.tar.gz -C /bigdata/install/ sudo vi /etc/profile #添加以下配置内容，配置jdk环境变量 export JAVA_HOME=/bigdata/install/jdk1.8.0_141 export PATH=:$JAVA_HOME/bin:$PATH # 立即生效 source /etc/profile ``` ## hadoop用户免密码登录 三台机器在hadoop用户下执行以下命令生成公钥与私钥 ```bash # 三台机器在hadoop用户下分别执行 ssh-keygen -t rsa # 三台机器拷贝公钥到node01 ssh-copy-id node01 # 在node01的hadoop用户下执行，将authorized_keys拷贝到node02和node03 # node01已经存在authorized_keys cd /home/hadoop/.ssh/ scp authorized_keys node02:$PWD scp authorized_keys node03:$PWD ``` # 安装hadoop ## CDH软件版本重新进行编译 ### 为何编译 CDH和Apache发布包不支持C程序库。本地库可以用于支持压缩算法和c程序调用。 ### 安装JDK 需要版本jdk1.7.0_80（1.8编译会出现错误） 安装maven # 解压缩 tar -zxvf apache-maven-3.0.5-bin.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export MAVEN_HOME=/bigdata/install/apache-maven-3.0.5 export MAVEN_OPTS=\"-Xms4096m -Xmx4096m\" export PATH=:$MAVEN_HOME/bin:$PATH # 立即生效 source /etc/profile 安装findbugs # 安装wget sudo yum install -y wget # 下载findbugs cd /bigdata/soft wget --no-check-certificate https://sourceforge.net/projects/findbugs/files/findbugs/1.3.9/findbugs-1.3.9.tar.gz/download -O findbugs-1.3.9.tar.gz # 解压findbugs tar -zxvf findbugs-1.3.9.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export FINDBUGS_HOME=/bigdata/install/findbugs-1.3.9 export PATH=:$FINDBUGS_HOME/bin:$PATH # 立即生效 source /etc/profile 安装依赖 sudo yum install -y autoconf automake libtool cmake sudo yum install -y ncurses-devel sudo yum install -y openssl-devel sudo yum install -y lzo-devel zlib-devel gcc gcc-c++ sudo yum install -y bzip2-devel 安装protobuf # 解压缩 tar -zxvf protobuf-2.5.0.tar.gz -C ../install/ # 执行 cd /bigdata/install/protobuf-2.5.0 ./configure # root 用户执行 make && make install 安装snappy # 解压缩 cd /bigdata/soft/ tar -zxf snappy-1.1.1.tar.gz -C ../install/ # 执行 cd ../install/snappy-1.1.1/ ./configure # root 用户执行 make && make install 编译 # 解压缩 tar -zxvf hadoop-2.6.0-cdh5.14.2-src.tar.gz -C ../install/ # 编译 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 # 编译不支持snappy压缩 mvn package -Pdist,native -DskipTests -Dtar # 编译支持snappy压缩 mvn package -DskipTests -Pdist,native -Dtar -Drequire.snappy -e -X 安装hadoop集群 安装环境服务部署规划 服务器IP HDFS HDFS HDFS YARN YARN 历史日志服务器 192.168.2.100 NameNode SecondaryNameNode DataNode ResourceManager NodeManager JobHistoryServer 192.168.2.101 DataNode NodeManager 192.168.2.102 DataNode NodeManager 上传压缩包并解压 重新编译之后支持snappy压缩的hadoop包上传到第一台服务器并解压 主机执行 scp hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz hadoop@192.168.2.100:/bigdata/soft node01执行 cd /bigdata/soft/ tar -zxvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C ../install/ 查看hadoop支持的压缩方式以及本地库 node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 bin/hadoop checknative 如果出现openssl为false，那么所有机器在线安装openssl su root yum -y install openssl-devel 修改配置文件 修改core-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi core-site.xml fs.defaultFS hdfs://node01:8020 hadoop.tmp.dir /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas io.file.buffer.size 4096 fs.trash.interval 10080 修改hdfs-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hdfs-site.xml dfs.hosts /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/accept_host dfs.hosts.exclude /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/deny_host --> dfs.namenode.secondary.http-address node01:50090 dfs.namenode.http-address node01:50070 dfs.namenode.name.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas dfs.datanode.data.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas dfs.namenode.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits dfs.namenode.checkpoint.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name dfs.namenode.checkpoint.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits dfs.replication 2 dfs.permissions false dfs.blocksize 134217728 修改hadoop-env.sh node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hadoop-env.sh export JAVA_HOME=/bigdata/install/jdk1.8.0_141 修改mapred-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi mapred-site.xml mapreduce.framework.name yarn mapreduce.job.ubertask.enable true mapreduce.jobhistory.address node01:10020 mapreduce.jobhistory.webapp.address node01:19888 修改yarn-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi yarn-site.xml yarn.resourcemanager.hostname node01 yarn.nodemanager.aux-services mapreduce_shuffle 修改slaves文件 node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi slaves node01 node02 node03 创建文件存放目录 node01 执行 mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits 安装包分发 node01 执行 cd /bigdata/install/ scp -r hadoop-2.6.0-cdh5.14.2/ node02:$PWD scp -r hadoop-2.6.0-cdh5.14.2/ node03:$PWD 配置hadoop的环境变量 三台机器执行 sudo vi /etc/profile export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2 export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 配置完成之后生效 source /etc/profile 集群启动 要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个集群。 注意：首次启动HDFS时，必须对其进行格式化操作。本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。 node01 执行 hdfs namenode -format 或者 hadoop namenode –format 单个节点逐一启动 # 在主节点上使用以下命令启动 HDFS NameNode: hadoop-daemon.sh start namenode # 在每个从节点上使用以下命令启动 HDFS DataNode: hadoop-daemon.sh start datanode # 在主节点上使用以下命令启动 YARN ResourceManager: yarn-daemon.sh start resourcemanager # 在每个从节点上使用以下命令启动 YARN nodemanager: yarn-daemon.sh start nodemanager # 以上脚本位于$HADOOP_PREFIX/sbin/目录下 # 如果想要停止某个节点上某个角色，只需要把命令中的start改为stop即可 脚本一键启动 如果配置了 etc/hadoop/slaves 和 ssh 免密登录，则可以使用程序脚本启动所有Hadoop两个集群的相关进程，在主节点所设定的机器上执行。 node01 执行 启动集群 start-dfs.sh start-yarn.sh mr-jobhistory-daemon.sh start historyserver 停止集群 stop-dfs.sh stop-yarn.sh mr-jobhistory-daemon.sh stop historyserver 浏览器查看启动页面 hdfs集群访问地址 http://192.168.2.100:50070/dfshealth.html#tab-overview yarn集群访问地址 http://192.168.2.100:8088/cluster jobhistory访问地址 http://192.168.2.100:19888/jobhistory Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-29 11:42:30 "},"src/virtualization/docker/Docker架构.html":{"url":"src/virtualization/docker/Docker架构.html","title":"Docker架构","keywords":"","body":"Docker架构 基础 什么是容器 ​ 作为容器，包含了开发、共享、部署的一切，包括代码，运行时，系统库，系统工具和相关的设置。因此，它可以快速运行，并可以在多台不同环境的计算机间无缝运行，避免解决各种因软件和硬件环境差异带来的各种开发和系统问题。 安装 ​ 登录DockerHub ​ 下载Docker.dmg，并安装 卸载 ​ 在【应用程序】卸载docker ​ 无法使用docker命令 架构 Docker使用的是client-server架构。Docker client同Docker daemon（守护进程）通信，Docker daemon负责构建、运行和分发Docker容器。Docker client和Docker daemon可以运行在同一个系统内，Docker client也可以连接到一个远程的Docker daemon。 ​ The Docker daemon ​ Docker daemon（dockerd）监听Docker API请求，管理Docker对象（镜像，容器，网络和卷）。一个daemon也可以同其他daemons通信来管理Docker服务。 ​ The Docker client ​ Docker client（docker）是Docker用户与Docker交互的主要方式。当您使用docker run等命令时，client会将这些命令发送给dockerd，dockerd会执行这些命令。docker命令使用docker API。Docker client可以与多个daemon通信。 ​ Docker registries ​ Docker registry存储Docker镜像。Docker Hub是一个公共的注册中心，任何人均可使用，Docker默认从Docker Hub中获取镜像。 ​ Docker objects ​ IMAGES 镜像 ​ 创建Docker容器的只读模板。为了创建自己的镜像，你需要创建一个Dockerfile，定义一些语法步骤，来创建和运行一个镜像。在Dockerfile中的每一个指令创建镜像中的一层。当你改变Dockerfile，然后重建镜像，仅仅改变的那些层被重建。 ​ CONTAINERS 容器 ​ 一个容器是一个镜像的运行实例。一个容器与其他容器，主机隔离。 ​ SERVICES 服务 ​ 服务允许跨多个Docker daemon来扩展容器，多个管理者和工作者作为一个集群工作。一个集群的每一个成员都是一个Docker daemon，这些daemon使用Docker API来通信。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 20:01:50 "},"src/virtualization/docker/Docker命令.html":{"url":"src/virtualization/docker/Docker命令.html","title":"Docker命令","keywords":"","body":"Docker命令 管理命令 docker builder docker checkpoint docker config docker container docker container run docker container run --publish 8000:8080 --detach --name tc ti:1.0 --detach: 在后台运行容器，打印容器ID --name: 给容器指定一个名称 --publish: 发布容器的端口到主机，要求Docker将主机8000端口的流量转发到容器的8080端口 在一个新容器中运行命令。其中，容器名称必须唯一。 docker container ls docker container ls -a -a: 显示所有容器，默认仅显示正在运行的容器 列举所有容器 docker container rm docker container rm --force tc --force: 强制删除正在运行的容器 删除一个或多个容器。 docker context docker image docker image build docker image build -t ti:1.0 . -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签 执行Dockerfile文件命令，构建镜像。成功后显示 Successfully tagged testubuntu:1.0。 docker image ls docker image ls 列举所有镜像 docker image rm docker image rm ti 删除本地镜像文件，rm后可以是镜像名称，也可以是镜像 ID。 docker image tag docker image tag ti:1.0 sciatta/ti:1.0 为镜像创建标签，创建的新标签指向的是原镜像。 在Docker Hub共享镜像，必须命名为/:的样式。 docker image pull docker image push docker image push sciatta/ti:1.0 上传镜像到仓库。 docker network docker node docker plugin docker secret docker service docker stack docker swarm docker system docker trust docker volume 命令 docker attach docker build docker commit docker cp docker create docker deploy docker diff docker events docker exec docker exec -it tc ps aux 在一个正在运行的容器内执行命令。 docker export docker history docker images docker import docker info docker inspect docker kill docker load docker login docker login 输入用户名和密码，登录到docker仓库。默认是Docker Hub。如果没有Docker ID（用户名），需要在https://hub.docker.com注册。 登录成功后显示Login Succeeded。 docker logout docker logs docker pause docker port docker ps docker pull docker push docker rename docker restart docker rm docker rmi docker run docker run -i -t sciatta/ti /bin/bash -i: 以交互模式运行容器 -t: 为容器重新分配一个伪输入终端 如果本机没有sciatta/ti镜像，从配置仓库pull镜像，同运行命令docker pull。 创建一个新的容器，同运行命令docker container create。 Docker为容器创建一个可读写的文件系统作为最后一层。允许一个正在运行的容器，创建和修改本机文件系统的文件、目录。 在没有指定网络选项时，Docker创建一个网络接口，连接容器到默认的网络，包括为容器分配一个IP地址。默认，容器可以使用主机的网络连接来连接到外部网络。 Docker启动容器，执行/bin/bash。因为容器以交互式运行，并且已经附加到终端 ，所以当日志输出到你的终端时，可以使用键盘输入。 当输入exit退出/bin/bash命令后，容器停止运行，但容器没有被删除。可以重新启动或者删除容器。 docker save docker search docker start docker stats docker stop docker tag docker top docker unpause docker update docker version docker wait Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:59:48 "},"src/virtualization/docker/Dockerfile参考.html":{"url":"src/virtualization/docker/Dockerfile参考.html","title":"Dockerfile参考","keywords":"","body":"Dockerfile参考 概述 Docker从Dockerfile文件中读取指令，自动构建镜像。 docker build 从Dockerfile和上下文构建镜像。构建上下文是在指定位置 PATH 或 URL 下的文件集合。其中，PATH是本地文件系统的目录，URL是Git仓库的位置。上下文的处理是递归的。 docker build . 以当前目录作为上下文。 构建过程是运行在 Docker daemon，而并不是CLI。构建过程的第一件事是发送整个上下文（递归）到daemon。因此，最好是把Dockerfile放置在一个空目录作为上下文，仅增加对构建需要的文件。为了使用上下文下的文件，可以使用COPY指令。为了提高构建性能，可以在上下文目录增加一个.dockerignore文件来排除文件和目录。 注释 指令不是大小写敏感的，但按照惯例，把指令全部写成大写来与参数作区分。Docker按顺序运行Dockerfile内的指令。一个Dockerfile必须以FROM指令开始（可在解析器指令、注释、全局范围参数之后）。 Docker会将以＃开头的行视为注释，除非该行是有效的解析器指令。一行中其他任何地方的＃标记均被视为参数。注释中不支持换行符。 # Comment RUN echo 'we are running some # of cool things' 第一个 # 为注释；第二个 # 被视为参数，正常输出 we are running some # of cool things 。 解析器指令 解析器指令是可选的，影响Dockerfile后续行的执行。解析器指令不会增加层，也不会显示为一个构件步骤。解析器指令被写成一个特殊的注释# directive=value。处理完注释，空行或构件指令后，Docker不再寻找解析器指令。 而是将格式化为解析器指令的任何内容都视为注释，并且不会尝试验证它是否可能是解析器指令。 因此，所有解析器指令必须位于Dockerfile的最顶部。 解析器指令不是大小写敏感的，但按照惯例，全部写成小写。约定还应在任何解析器指令之后包含一个空白行。 解析器指令不支持换行符。而非换行符的空格字符是支持的。 syntax # syntax=[remote image reference] escape # escape=\\ (backslash) 转义指令设置Dockerfile中的转义字符，如果没有指定，默认是 \\ 。 .dockerignore 文件 docker CLI 发送上下文到 docker daemon之前，会首先在上下文的root目录查找.dockerignore文件。如果存在，CLI会修改上下文，排除同模式匹配的文件和目录。这样可以避免发送不必要的大或敏感文件和目录到daemon。如果需要这些文件或目录的话，可以使用 ADD 或 COPY 指令。 CLI将.dockerignore文件解释为以换行符分隔的模式列表，类似于Unix shell的文件组。为了匹配，上下文的根被认为是工作目录和根目录。例如，模式 /foo/bar 和 foo/bar 都在PATH或位于URL的git仓库根目录的foo子目录中排除名为bar的文件或目录。 如果.dockerignore文件中第一行以 # 开始 ，则被看做是注释，在CLI解释之前被忽略。 # 注释 # comment注释 * 匹配任意字符 /temp匹配在root的直接子目录中，以temp为前缀的文件或目录 //temp* 匹配在root的二级子目录中，以temp为前缀的文件或目录 ** 匹配任意数量的目录，包括0个 */.go排除所有目录以.go结尾的文件，包括root目录 ! 排除例外 !README.md包含文件README.md ? 匹配一个字符 temp?匹配在root中以一个字符扩展temp的所有文件或目录 FROM FROM [AS ] FROM [:] [AS ] FROM [@] [AS ] 一个有效的Dockerfile文件必须包含一个 FROM 指令。 ARG 是唯一一个可以先于FROM的指令。 FROM可以在单个Dockerfile中多次出现，以创建多个映像或将一个构建阶段用作对另一个构建阶段的依赖。 只需在每个新的FROM指令之前记录一次提交输出的最后一个镜像ID。 每个FROM指令清除由先前指令创建的任何状态。 可以增加 AS 到 FROM 指令，为一个新的构建阶段提供一个可选的名称。名称可以用于后续 FROM 和 COPY --from= 指令，来在这个阶段引用镜像。 tag 和 digest 值是可选的。如果省略，构建器默认latest tag。如果没有找到tag，则返回一个错误。 ARG VERSION=latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION VERSION定义先于FROM指令，因此在FROM指令之后，ARG不可以用于其他指令。在构建阶段如果需要使用ARG的默认值，则使用ARG指令不赋予值即可。 RUN RUN 以shell方式运行，默认 /bin/sh -c； 如：RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME' RUN [\"executable\", \"param1\", \"param2\"] 以exec方式。注意双引号，JSON格式; 如：RUN [\"/bin/bash\", \"-c\", \"echo hello\"] 在当前镜像的顶层创建一个新层来执行命令，然后提交结果。 不像shell方式，以exec方式方式并不会调用命令shell。也就是说shell处理将不会发生。例如 RUN [ \"echo\", \"$HOME\" ]在 $HOME上将不会进行变量替换。如果需要进行shell处理，则以shell形式，或直接执行shell，如：RUN [ \"sh\", \"-c\", \"echo $HOME\" ]。会进行环境变量扩展，而不是docker。 CMD CMD [\"executable\",\"param1\",\"param2\"] exec方式 CMD [\"param1\",\"param2\"] ENTRYPOINT的默认参数 CMD command param1 param2 shell方式 在Dockerfile中仅可以有一条 CMD 指令。如果有多条，仅有最后一条有效。 主要目的为正在执行容器提供默认执行方式。当运行镜像时，CMD指令被执行。 运行 docker run 指定参数，将覆盖 CMD 中的默认相同参数。 在构建阶段，RUN 执行一个命令，然后提交结果；而 CMD 并不在构建阶段执行，针对镜像的预期指定命令，即在容器执行时执行。 LABEL LABEL = = = ... 为镜像增加元数据。一个LABEL是一个 key-value 对。为了在LABEL的value中包括空字符，需要使用引号和反斜杠。 一个镜像可以有多个LABEL；可以指定多个LABEL在一行。 LABEL可以包括在父镜像中，被用于继承。如果LABEL重复，后者覆盖前者。使用 docker inspect 查看LABEL。 LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" MAINTAINER (deprecated) MAINTAINER 生成镜像的作者。LABEL比MAINTAINER更灵活，建议使用LABEL替换。 例如 LABEL maintainer=\"SvenDowideit@home.org.au\" EXPOSE EXPOSE [/...] 在运行时，容器监听的指定网络端口。可以指定端口监听 TCP 或是 UDP，如果协议没有指定，则TCP是默认协议。 EXPOSE 指令实际并没有发布端口，它的主要功能是在构建镜像者和运行容器者之间作为一种规约，也就是预期要发布的端口。当运行容器时，实际要发布端口的话，使用 -p 标志发布并映射一个或多个端口，或 -P 发布所有 EXPOSE 的端口并映射到高阶端口。 EXPOSE 80/udp 默认TCP协议，可以指定协议为UDP。 不管 EXPOSE 如何设置，都可以在运行时使用 -p 标志覆盖。 ENV ENV ENV = ... 在构建阶段，环境变量可以像使用变量一样用于后续的指令中，被Dockerfile所解析。环境变量可以在Dockerfile中以$variable_name 或 ${variable_name} 方式来引用。 如果在变量前面加上转义字符 \\ ，则按字面字符解释。如 $foo 或者 ${foo}，被分别解释为 $foo 和 ${foo} 。 环境变量在运行时持续存在。可以使用docker inspect 查看。 可以在一个command中设置一个环境变量，使用 RUN = 。 ADD ADD [--chown=:] ... ADD [--chown=:] [\"\",... \"\"] 1、路径包括空字符 2、--chown 只支持UNIX系统 从复制文件，目录或者远程 URL，到镜像文件系统。 可以指定多个资源，如果它们是文件或目录，则将其路径解释为相对于构建上下文的路径。 ADD hom* /mydir/ ADD hom?.txt /mydir/ 可以是一个绝对路径，也可以是一个相对 WORKDIR 的相对路径。 ADD test relativeDir/ # adds \"test\" to `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # adds \"test\" to /absoluteDir/ 路径必须在构建上下文内部；不可使用../something /something，因为docker构建的第一步（FROM）已经把上下文目录发送到docker daemon。 如果是一个URL，并且没有以斜杠结束，则文件从URL下载并复制到。 如果是一个URL，并且以斜杠结束，则文件名按照URL进行推导，文件下载为/。如 ADD http://example.com/foobar / 将创建文件 /foobar 。 如果是一个目录，则目录的所有内容被复制，包括文件系统的元数据。注意，目录本身不会被复制，仅是内容。 如果是一个本地tar压缩格式，则被解压缩为一个目录。来自远程URL的资源不被解压缩。 如果多个资源被指定，或者是目录，或者是通配符形式，则必须是一个目录，必须以 反斜杠结尾。 如果不存在，路径所有缺失的目录被创建。 COPY COPY [--chown=:] ... COPY [--chown=:] [\"\",... \"\"] 从复制文件，目录，到容器文件系统。 ENTRYPOINT ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec form, preferred) ENTRYPOINT command param1 param2 (shell form) 配置容器，作为可执行方式运行。 以exec方式运行，命令行参数会追加到 docker run ENTRYPOINT之后，并覆盖CMD指定的元素。可以搭配 CMD 命令使用，一般是变参会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参。可以使用ENTRYPOINT的exec方式，作为稳定默认命令和参数。然后使用两种形式的CMD设置更可能更改的其他默认值。 FROM nginx ENTRYPOINT [\"nginx\", \"-c\"] # 定参 CMD [\"/etc/nginx/nginx.conf\"] # 变参 docker run nginx:test 不传参运行，启动主进程 nginx -c /etc/nginx/nginx.conf docker run nginx:test -c /etc/nginx/new.conf 传参运行，容器内会默认运行 nginx -c /etc/nginx/new.conf 允许向ENTRYPOINT传参。如 docker run -d ，将 -d 传入到ENTRYPOINT。可以使用 docker run --entrypoint 覆盖 ENTRYPOINT指令。 以Shell方式运行，可防止使用任何 CMD 或 run 命令行参数，但缺点是ENTRYPOINT将作为/bin/sh -c 的子命令启动，该子命令不传递信号。这意味着executable将不是容器的PID 1，并且不会接收Unix信号，因此executable将不会从 docker stop 接收到信号。 多个ENTRYPOINT指令，只有最后一个有效。 ENTRYPOINT和CMD交互 No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUME VOLUME [\"/var/log/\"] VOLUME /var/log or VOLUME /var/log /var/db 创建一个指定名称的挂载点，可以将主机或其他容器的目录挂载到容器中。当容器删除时，数据被持久化而不会丢失。 主机目录是在容器运行时声明的：主机目录（挂载点）从本质上说是依赖于主机的。 这是为了保证镜像的可移植性，因为不能保证给定的主机目录在所有主机上都可用。 因此，您无法从Dockerfile中挂载主机目录。VOLUME 指令不支持指定host-dir参数。 当创建或运行容器时，必须指定挂载点。 USER USER [:] USER [:] USER指令设置运行镜像时要使用的用户名（或UID）以及可选的用户组（或GID），以及Dockerfile中跟随该镜像的所有RUN，CMD和ENTRYPOINT指令。 如果用户没有组，则该镜像（或接下来指令）将用root组运行。 WORKDIR WORKDIR /path/to/workdir WORKDIR指令为Dockerfile中跟在其后的所有RUN，CMD，ENTRYPOINT，COPY和ADD指令设置工作目录。 如果WORKDIR不存在，即使后续的Dockerfile指令中未使用，WORKDIR也将被创建。 WORKDIR指令可在Dockerfile中多次使用。 如果提供了相对路径，则它将相对于上一个WORKDIR指令的路径。 WORKDIR /a WORKDIR b WORKDIR c RUN pwd 最后输出的WORKDIR是 /a/b/c 。 ARG ARG [=] ARG指令定义的变量，用户可以在构建阶段，通过 docker build 指令使用 --build-arg = 覆盖已定义的变量。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:58:59 "},"src/os/linux/Linux命令.html":{"url":"src/os/linux/Linux命令.html","title":"Linux命令","keywords":"","body":"Linux命令 A B bye C cat Concatenate FILE(s), or standard input, to standard output. cat file1 file2 > file3 1、将file1内容和file2内容输出到file3 2、“命令 > 文件”将标准输出重定向到一个文件中（清空原有文件的数据） 3、“命令 >> 文件”将标准输出重定向到一个文件中（追加到原有内容的后面） cat -n file1 file2 > file3 由1开始对所有输出的行编号，包括空行 cat -b file1 file2 > file3 由1开始对所有输出的行编号，不包括空行 cat 拷贝标准输入到标准输出 cat file1 拷贝file1到标准输出 cat file1 file2 拷贝file1和file2到标准输出 cat /dev/null > file1 1、清空file1的内容 2、dev/null：在类Unix系统中，/dev/null称空设备，它丢弃一切写入其中的数据（但报告写入操作成功）， 读取它则会立即得到一个EOF cat file1 > /dev/null 不会得到任何信息，因为将本来该通过标准输出显示的文件信息重定向到了/dev/null中 chgrp chmod chown cp Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. -a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。 -d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。 -f：覆盖已经存在的目标文件而不给出提示。 -i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答\"y\"时目标文件将被覆盖。 -p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。 -r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。 -l：不复制文件，只是生成链接文件。 cp -r test test1 复制test目录下的内容到test1 cd dirName：要切换的目标目录。可为绝对路径或相对路径。\"~\" 表示为 home 目录，\".\" 则是表示目前所在的目录，\"..\" 则表示目前目录位置的上一层目录。 cd a/b 切换当前工作目录为目标目录 cd 若目录名称省略，则变换至使用者的 home 目录 D df Show information about the file system on which each FILE resides, or all file systems by default. df -h 显示文件系统的磁盘使用情况统计 Filesystem Size Used Avail Use% Mounted on 显示文件系统、挂载点等信息 du Summarize disk usage of each FILE, recursively for directories. du -h 只显示当前目录和子目录的大小 du -h * 显示当前目录的文件大小，子目录，以及递归子目录的大小（子目录下文件不显示大小） du -sh * 仅显示当前目录的子目录和文件的总计大小 E F find search for files in a directory hierarchy find . -name \"file*\" 查找当前目录和子目录下所有匹配样式file*的文件或目录，输出路径 find . -name \"*.lastUpdated\" | xargs rm -rf tp G grep Search for PATTERN in each FILE or standard input. grep java file* 在当前目录文件名为file前缀的文件中，查找包含java的文件，并输出行内容 grep -rn java . 递归方式查找当前目录和其子目录 grep -n '2019-10-24 00:01:11' *.log H I J K L less Less is a program similar to more, but which allows backward movement in the file as well as forward movement. Also, less does not have to read the entire input file before starting, so with large input files it starts up faster than text editors like vi. G 移动到最后一行 g 移动到第一行 /字符串 向下搜索“字符串” ?字符串 向上搜索“字符串”的功能 n 重复前一个搜索（与 / 或 ? 有关） N 反向重复前一个搜索（与 / 或 ? 有关） Enter（j） 向下移动一行 k 向上移动一行 空格键（d） 向下翻一页 b 向上翻一页 q（ZZ） 退出less命令 ps -ef |less ps查看进程信息并通过less分页显示 history | less 查看命令历史使用记录并通过less分页显示 less file1 file2 1、浏览多个文件 2、输入 :n 切换到file2 :p 切换到file1 ls ls -al 默认当前目录，显示所有文件和目录，包括隐藏档 M more 空格键 向下翻一页 b 向上翻一页 more -s file1 如有连续两行以上空白行则以一行空白行显示 more +5 file1 从第5行开始显示 mv mv file1 file 移动并改名 mv ok/ no ok和no都是目录。当no不存在时，则将ok改名为no；当no存在时，将ok目录转移到no目录。 mv ok/* no ok和no都是目录。当no不存在时，失败；当no存在时，将ok目录所有内容转移到no目录。 mkdir Create the DIRECTORY(ies), if they do not already exist. mkdir [-p] dirName -p 确保目录名称存在，不存在的就建一个。 mkdir -p a/b 在当前工作目录下创建目录，父目录不存在，则一起创建 N O P pwd Print the full filename of the current working directory. pwd [--help][--version] --help 在线帮助。 --version 显示版本信息。 pwd 输出当前工作路径 Q R rcp rm Remove (unlink) the FILE(s). rm [options] name... -i 删除前逐一询问确认。 -f 即使原档案属性设为唯读，亦直接删除，无需逐一确认。 -r 将目录及以下之档案亦逐一删除。 rm services rm: remove regular file ‘services’? n 删除文件需要询问 rm -r test 删除目录 rm -rf test 强制删除所有文件和目录（即使只读），不需要询问 S ssh ssh root@172.16.92.132 1、登录远程服务器，接着输入密码 2、iTerm远程连接centos服务器，显示命令帮助为中文，而centos服务器内部可以显示英文。 centos语言为US，iTerm语言为CN，因此需要修改iTerm。 1）vim ~/.zshrc 2）末尾添加 export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8 3）source ~/.zshrc scp T touch touch services -rw-r--r--. 1 root root 670293 Dec 31 17:10 services -rw-r--r--. 1 root root 670293 Dec 31 21:07 services 文件存在，更新修改时间为当前时间 touch file1 文件不存在，创建空文件 U V W which Write the full path of COMMAND(s) to standard output. which [文件...] -n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 -p 与-n参数相同，但此处的包括了文件的路径。 -w 指定输出时栏位的宽度。 -V 显示版本信息。 which bash which指令会在环境变量$PATH设置的目录里查找符合条件的文件 which -a bash 打印所有匹配的路径（不仅只有第一个） wc Print newline, word, and byte counts for each FILE, and a total line ifmore than one FILE is specified. With no FILE, or when FILE is -,read standard input. A word is a non-zero-length sequence of charactersdelimited by white space. wc [-clw][--help][--version][文件...] -c或--bytes 只显示Bytes数。 -m或--chars 只显示字符数。 -l或--lines 只显示行数。 -w或--words 只显示单词数。 --help 在线帮助。 --version 显示版本信息。 wc file1 file2 2 3 17 file1 3 4 18 file2 5 7 35 total 行数 单词数 字节数，以及总统计值 X Y Z Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:38:51 "},"src/os/linux/iTerm的使用.html":{"url":"src/os/linux/iTerm的使用.html","title":"iTerm的使用","keywords":"","body":"iTerm的使用 快捷键 水平分隔当前屏幕 shift+command+D 垂直分隔当前屏幕 command+D 向所有TAB的所有窗口广播命令 shift+command+i 向当前TAB的所有窗口广播命令 option+command+i 将窗口分组广播命令 1、方法一 所有窗口分组 option+command+i 单独一个窗口分组 shift+control+option+command+i 2、方法二 单独一个窗口分组 shift+control+option+command+i 其他窗口分在一组 shift+control+option+command+i 恢复窗口分组 shift+option+command+i Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:57:55 "},"src/ide/intellijidea/IDEA配置开发环境.html":{"url":"src/ide/intellijidea/IDEA配置开发环境.html","title":"IDEA配置开发环境","keywords":"","body":"Java环境 配置JDK环境变量 vi ~/.zshrc export SCALA_HOME=/Users/yangxiaoyu/work/install/scala-2.11.8 export PATH=$SCALA_HOME/bin:$PATH # 即时生效 source ~/.zshrc 配置JDK集成开发环境 项目右键 | open module settings | sdks | + jdk 指定 jdk home path 项目右键 | open module settings | project 指定sdk 项目右键 | open module settings | modules | language level: 8 Preferences | Build, Execution, Deployment | compiler | java compiler module 设置为8 project bytecode version 设置为8 Scala环境 配置SDK环境变量 tar -zxvf scala-2.11.8.tgz -C ../install vi ~/.zshrc export SCALA_HOME=/Users/yangxiaoyu/work/install/scala-2.11.8 export PATH=$SCALA_HOME/bin:$PATH # 即时生效 source ~/.zshrc 配置SDK集成开发环境 下载 Scala 插件 Preferences | plugins | Scala 新建 pom module 新建 jar module 指定SDK File | Project Structure | Libraries | + | Scala SDK | 选择 scala-2.11.8 项目右键 | open module settings | modules | hadoop-scala-example | Dependencies | + | Libraries | 选择 scala-sdk-2.11.8 混编Scala src/main 新建 Directory | scala Mark Directory as | Sources Root 打包运行 maven | package scala -classpath /Users/yangxiaoyu/Desktop/hadoop-scala-example-1.0-SNAPSHOT.jar First 下载 Scala 反编译插件 Preferences | plugins | CFR Decompile Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-27 17:07:40 "},"src/ide/intellijidea/IDEA创建GitHub项目.html":{"url":"src/ide/intellijidea/IDEA创建GitHub项目.html","title":"IDEA创建GitHub项目","keywords":"","body":"GitHub空项目 在GitHub上创建空项目，如：hadoop-main IDEA创建project check out from version control | 选择 git 指定URL https://github.com/sciatta/hadoop-main.git 和本地目录 | clone | 选择 create project from existing sources 后续默认选择 初始化project git flow init 后续在develop分支开发 IDEA创建module 创建 parent pom module hadoop-main | new module | maven groupid: com.sciatta.hadoop artifact: hadoop-main module name: hadoop-main # hadoop-main根目录作为module parent content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main # 修改pom.xml pom 创建child pom module hadoop-main | new module | maven parent: hadoop-main add as module to: hadoop-main artifact: hadoop-hdfs module name: hadoop-hdfs content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs # 修改pom.xml pom 创建child jar module hadoop-hdfs | new module | maven parent: hadoop-hdfs add as module to: hadoop-hdfs artifact: hadoop-hdfs-example module name: hadoop-hdfs-example content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs/hadoop-hdfs-example # 修改pom.xml jar Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 20:17:03 "},"src/cloudplatform/vultr/Shadowsocks服务端配置.html":{"url":"src/cloudplatform/vultr/Shadowsocks服务端配置.html","title":"Shadowsocks服务端配置","keywords":"","body":"注册 登录 https://www.vultr.com/ 注册用户 新建Server Products | Deploy new server Server Location Australia | Sydney Server Type 64 bit OS | CentOS | 7 x64 管理Server 登录 ping -c 3 IP Address 查看延时情况 ssh root@IP Address UI界面 copy password 复制到控制台登录 （不需要修改root密码） 安装wget 视情况安装 yum -y install wget 初始化目录 mkdir -p /export/ && cd /export/ 下载脚本 # 下载 wget https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh # 修改权限 chmod +x shadowsocks.sh 运行脚本 cd /export/ # 当出现 Enjoy it! 表示安装成功 ./shadowsocks.sh 2>&1 | tee shadowsocks.log # ==== # 密码 # 端口号 # 加密（可以使用默认aes-256-gcm） # ==== 检查服务 ps -ef | grep ssserver 停止服务 ssserver -c /etc/shadowsocks.json -d stop 启动服务 ssserver -c /etc/shadowsocks.json -d start Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-25 16:47:06 "}}