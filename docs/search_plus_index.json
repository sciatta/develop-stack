{"./":{"url":"./","title":"前言","keywords":"","body":"每一个人的成功都不是偶然的，其中存在的必然。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/language/java/jvm/JVM核心技术.html":{"url":"src/language/java/jvm/JVM核心技术.html","title":"JVM核心技术","keywords":"","body":"概述 Java是一种面向对象、静态类型、编译执行，有 VM/GC 和运行时、跨平台的高级语言。 字节码 Java bytecode由单字节指令组成，理论上最多支持256个操作码（opcode）。实际Java只使用了200左右的操作码。还有一些操作码则保留给调试操作。 栈操作指令，包括与局部变量交互的指令 程序流程控制指令 对象操作指令，包括方法调用指令 算术运算及类型转换指令 字节码运行时结构 JVM是一台基于栈的计算机器。每个线程都有一个独属于自己的线程栈，用于存储栈帧。每一次方法调用，JVM都会自动创建一个栈帧。栈帧由操作数栈，局部变量表以及一个Class引用组成。Class引用指向当前方法在运行时常量池中对应的Class。 所有指令的起点都是操作数栈，在其上进行运算，临时结果继续入栈；而最终结果可以保存在本地变量表中，用于其他指令读取。 字节码分析 HelloByteCode 源码 public class HelloByteCode { public static void main(String[] args) { HelloByteCode helloByteCode = new HelloByteCode(); } } 分析 # 反编译 HelloByteCode.class javap -c -verbose HelloByteCode # 输出 Classfile /Users/yangxiaoyu/work/bigdata/project/hadoop-dev/hadoop-java/hadoop-java-jvm/target/classes/com/sciatta/hadoop/java/jvm/bytecode/HelloByteCode.class Last modified 2020-10-18; size 499 bytes MD5 checksum 39bee97c80c36566884eb74e401f2321 Compiled from \"HelloByteCode.java\" public class com.sciatta.hadoop.java.jvm.bytecode.HelloByteCode minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPER Constant pool: #1 = Methodref #4.#19 // java/lang/Object.\"\":()V #2 = Class #20 // com/sciatta/hadoop/java/jvm/bytecode/HelloByteCode #3 = Methodref #2.#19 // com/sciatta/hadoop/java/jvm/bytecode/HelloByteCode.\"\":()V #4 = Class #21 // java/lang/Object #5 = Utf8 #6 = Utf8 ()V #7 = Utf8 Code #8 = Utf8 LineNumberTable #9 = Utf8 LocalVariableTable #10 = Utf8 this #11 = Utf8 Lcom/sciatta/hadoop/java/jvm/bytecode/HelloByteCode; #12 = Utf8 main #13 = Utf8 ([Ljava/lang/String;)V #14 = Utf8 args #15 = Utf8 [Ljava/lang/String; #16 = Utf8 helloByteCode #17 = Utf8 SourceFile #18 = Utf8 HelloByteCode.java #19 = NameAndType #5:#6 // \"\":()V #20 = Utf8 com/sciatta/hadoop/java/jvm/bytecode/HelloByteCode #21 = Utf8 java/lang/Object { public com.sciatta.hadoop.java.jvm.bytecode.HelloByteCode(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 # 4、从本地变量表中拿出第0个元素，this引用入栈 # -> this 0: aload_0 # 5、this出栈，调用父类的初始化方法 # -> 1: invokespecial #1 // Method java/lang/Object.\"\":()V # 6、调用结束，没有返回值；方法结束时，操作数栈必须为空 # -> 4: return LineNumberTable: line 8: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/sciatta/hadoop/java/jvm/bytecode/HelloByteCode; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=1 # 1、在堆中为HelloByteCode分配内存空间，其引用入栈 # -> ref 0: new #2 // class com/sciatta/hadoop/java/jvm/bytecode/HelloByteCode # 2、复制栈顶元素 # -> ref ref 3: dup # 3、ref 出栈，调用该实例的初始化方法，为HelloByteCode构造新的栈帧 # -> ref 4: invokespecial #3 // Method \"\":()V # 7、ref 出栈，保存引用到本地变量表中第1个元素 # -> 7: astore_1 # 8、调用结束 # -> 8: return LineNumberTable: line 10: 0 line 11: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 args [Ljava/lang/String; 8 1 1 helloByteCode Lcom/sciatta/hadoop/java/jvm/bytecode/HelloByteCode; } MovingAverage 源码 public class MovingAverage { private int count = 0; private double sum = 0.0D; public void submit(double value) { this.count++; sum += value; } public double getAvg() { if (0 == count) return sum; return sum / count; } public static void main(String[] args) { MovingAverage movingAverage = new MovingAverage(); int num1 = 1; int num2 = 2; movingAverage.submit(num1); movingAverage.submit(num2); double avg = movingAverage.getAvg(); } } 分析 # 反编译 javap -c -verbose MovingAverage # 输出 Classfile /Users/yangxiaoyu/work/bigdata/project/hadoop-dev/hadoop-java/hadoop-java-jvm/target/classes/com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.class Last modified 2020-10-18; size 930 bytes MD5 checksum 6f577201cd5747c22b3ae11db0aba16d Compiled from \"MovingAverage.java\" public class com.sciatta.hadoop.java.jvm.bytecode.MovingAverage minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPER Constant pool: #1 = Methodref #8.#36 // java/lang/Object.\"\":()V #2 = Fieldref #4.#37 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.count:I #3 = Fieldref #4.#38 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.sum:D #4 = Class #39 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage #5 = Methodref #4.#36 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.\"\":()V #6 = Methodref #4.#40 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.submit:(D)V #7 = Methodref #4.#41 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.getAvg:()D #8 = Class #42 // java/lang/Object #9 = Utf8 count #10 = Utf8 I #11 = Utf8 sum #12 = Utf8 D #13 = Utf8 #14 = Utf8 ()V #15 = Utf8 Code #16 = Utf8 LineNumberTable #17 = Utf8 LocalVariableTable #18 = Utf8 this #19 = Utf8 Lcom/sciatta/hadoop/java/jvm/bytecode/MovingAverage; #20 = Utf8 submit #21 = Utf8 (D)V #22 = Utf8 value #23 = Utf8 getAvg #24 = Utf8 ()D #25 = Utf8 StackMapTable #26 = Utf8 main #27 = Utf8 ([Ljava/lang/String;)V #28 = Utf8 args #29 = Utf8 [Ljava/lang/String; #30 = Utf8 movingAverage #31 = Utf8 num1 #32 = Utf8 num2 #33 = Utf8 avg #34 = Utf8 SourceFile #35 = Utf8 MovingAverage.java #36 = NameAndType #13:#14 // \"\":()V #37 = NameAndType #9:#10 // count:I #38 = NameAndType #11:#12 // sum:D #39 = Utf8 com/sciatta/hadoop/java/jvm/bytecode/MovingAverage #40 = NameAndType #20:#21 // submit:(D)V #41 = NameAndType #23:#24 // getAvg:()D #42 = Utf8 java/lang/Object { public com.sciatta.hadoop.java.jvm.bytecode.MovingAverage(); descriptor: ()V flags: ACC_PUBLIC Code: stack=3, locals=1, args_size=1 # -> this 0: aload_0 # -> 1: invokespecial #1 // Method java/lang/Object.\"\":()V # -> this 4: aload_0 # -> this 0 5: iconst_0 # 栈顶的两个元素一起出栈，设置对象的值 # -> 6: putfield #2 // Field count:I # -> this 9: aload_0 # -> this 0 10: dconst_0 # -> 11: putfield #3 // Field sum:D # -> 14: return LineNumberTable: line 8: 0 line 9: 4 line 10: 9 LocalVariableTable: Start Length Slot Name Signature 0 15 0 this Lcom/sciatta/hadoop/java/jvm/bytecode/MovingAverage; public void submit(double); descriptor: (D)V flags: ACC_PUBLIC Code: stack=5, locals=3, args_size=2 # -> this 0: aload_0 # -> this this 1: dup # -> this value 2: getfield #2 // Field count:I # -> this value 1 5: iconst_1 # -> this result 6: iadd # -> 7: putfield #2 // Field count:I 10: aload_0 11: dup 12: getfield #3 // Field sum:D 15: dload_1 16: dadd 17: putfield #3 // Field sum:D 20: return LineNumberTable: line 13: 0 line 14: 10 line 15: 20 LocalVariableTable: Start Length Slot Name Signature 0 21 0 this Lcom/sciatta/hadoop/java/jvm/bytecode/MovingAverage; 0 21 1 value D public double getAvg(); descriptor: ()D flags: ACC_PUBLIC Code: stack=4, locals=1, args_size=1 0: iconst_0 1: aload_0 2: getfield #2 // Field count:I 5: if_icmpne 13 8: aload_0 9: getfield #3 // Field sum:D 12: dreturn 13: aload_0 14: getfield #3 // Field sum:D 17: aload_0 18: getfield #2 // Field count:I 21: i2d 22: ddiv 23: dreturn LineNumberTable: line 18: 0 line 19: 13 LocalVariableTable: Start Length Slot Name Signature 0 24 0 this Lcom/sciatta/hadoop/java/jvm/bytecode/MovingAverage; StackMapTable: number_of_entries = 1 frame_type = 13 /* same */ public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=6, args_size=1 # -> ref 0: new #4 // class com/sciatta/hadoop/java/jvm/bytecode/MovingAverage # -> ref ref 3: dup # -> ref 4: invokespecial #5 // Method \"\":()V # -> 7: astore_1 # -> 1 8: iconst_1 # -> 9: istore_2 # -> 2 10: iconst_2 # -> 11: istore_3 # -> ref 12: aload_1 # -> ref 1 13: iload_2 # 取出栈顶整数1，转换成double后，再压入栈 # -> ref 1 14: i2d # 从ref开始全部出栈 # -> 15: invokevirtual #6 // Method submit:(D)V 18: aload_1 19: iload_3 20: i2d 21: invokevirtual #6 // Method submit:(D)V 24: aload_1 25: invokevirtual #7 // Method getAvg:()D 28: dstore 4 30: return LineNumberTable: line 23: 0 line 24: 8 line 25: 10 line 26: 12 line 27: 18 line 28: 24 line 29: 30 LocalVariableTable: Start Length Slot Name Signature 0 31 0 args [Ljava/lang/String; 8 23 1 movingAverage Lcom/sciatta/hadoop/java/jvm/bytecode/MovingAverage; 10 21 2 num1 I 12 19 3 num2 I 30 1 4 avg D } SourceFile: \"MovingAverage.java\" ForLoop 源码 public class ForLoop { public static void main(String[] args) { int[] numbers = {1, 2, 3}; MovingAverage movingAverage = new MovingAverage(); for (int number : numbers) { movingAverage.submit(number); } double avg = movingAverage.getAvg(); } } 分析 javap -c -verbose ForLoop # output Classfile /Users/yangxiaoyu/work/bigdata/project/hadoop-dev/hadoop-java/hadoop-java-jvm/target/classes/com/sciatta/hadoop/java/jvm/bytecode/ForLoop.class Last modified 2020-10-18; size 844 bytes MD5 checksum 3d7b2aa7f6295d94dcc4821eb793e1f4 Compiled from \"ForLoop.java\" public class com.sciatta.hadoop.java.jvm.bytecode.ForLoop minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPER Constant pool: #1 = Methodref #7.#33 // java/lang/Object.\"\":()V #2 = Class #34 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage #3 = Methodref #2.#33 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.\"\":()V #4 = Methodref #2.#35 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.submit:(D)V #5 = Methodref #2.#36 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.getAvg:()D #6 = Class #37 // com/sciatta/hadoop/java/jvm/bytecode/ForLoop #7 = Class #38 // java/lang/Object #8 = Utf8 #9 = Utf8 ()V #10 = Utf8 Code #11 = Utf8 LineNumberTable #12 = Utf8 LocalVariableTable #13 = Utf8 this #14 = Utf8 Lcom/sciatta/hadoop/java/jvm/bytecode/ForLoop; #15 = Utf8 main #16 = Utf8 ([Ljava/lang/String;)V #17 = Utf8 number #18 = Utf8 I #19 = Utf8 args #20 = Utf8 [Ljava/lang/String; #21 = Utf8 numbers #22 = Utf8 [I #23 = Utf8 movingAverage #24 = Utf8 Lcom/sciatta/hadoop/java/jvm/bytecode/MovingAverage; #25 = Utf8 avg #26 = Utf8 D #27 = Utf8 StackMapTable #28 = Class #20 // \"[Ljava/lang/String;\" #29 = Class #22 // \"[I\" #30 = Class #34 // com/sciatta/hadoop/java/jvm/bytecode/MovingAverage #31 = Utf8 SourceFile #32 = Utf8 ForLoop.java #33 = NameAndType #8:#9 // \"\":()V #34 = Utf8 com/sciatta/hadoop/java/jvm/bytecode/MovingAverage #35 = NameAndType #39:#40 // submit:(D)V #36 = NameAndType #41:#42 // getAvg:()D #37 = Utf8 com/sciatta/hadoop/java/jvm/bytecode/ForLoop #38 = Utf8 java/lang/Object #39 = Utf8 submit #40 = Utf8 (D)V #41 = Utf8 getAvg #42 = Utf8 ()D { public com.sciatta.hadoop.java.jvm.bytecode.ForLoop(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"\":()V 4: return LineNumberTable: line 8: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/sciatta/hadoop/java/jvm/bytecode/ForLoop; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=4, locals=7, args_size=1 # -> 3 0: iconst_3 # -> arrayref 1: newarray int # -> arrayref arrayref 3: dup # -> arrayref arrayref 0 4: iconst_0 # -> arrayref arrayref 0 1 5: iconst_1 # 在数组array指定index保存value # -> arrayref 6: iastore 7: dup 8: iconst_1 9: iconst_2 10: iastore 11: dup 12: iconst_2 13: iconst_3 # -> arrayref 14: iastore # -> 15: astore_1 16: new #2 // class com/sciatta/hadoop/java/jvm/bytecode/MovingAverage 19: dup 20: invokespecial #3 // Method com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.\"\":()V 23: astore_2 # -> arrayref 24: aload_1 # -> 25: astore_3 # -> arrayref 26: aload_3 # 数组长度入栈 # -> 3 27: arraylength # -> 28: istore 4 # -> 0 30: iconst_0 # -> 31: istore 5 # -> 0 33: iload 5 # -> 0 3 35: iload 4 # 取出两个操作数 v1=0 v2=3 如果 v1>=v2 则指令跳转到指定index 59位置执行；否则执行后续指令 # -> 37: if_icmpge 59 # -> 每次都取出堆中的数组，线程不安全 # -> arrayref 40: aload_3 # -> arrayref 0 41: iload 5 # -> 弹出数组和索引，取值后入栈 # -> 1 43: iaload # -> 44: istore 6 # -> maref 46: aload_2 # -> maref 1 47: iload 6 # -> maref 1.0 49: i2d # -> 50: invokevirtual #4 // Method com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.submit:(D)V # 数组index=5加1 # -> 53: iinc 5, 1 # 调整到33，继续执行 56: goto 33 59: aload_2 60: invokevirtual #5 // Method com/sciatta/hadoop/java/jvm/bytecode/MovingAverage.getAvg:()D 63: dstore_3 64: return LineNumberTable: line 10: 0 line 11: 16 line 13: 24 line 14: 46 line 13: 53 line 17: 59 line 18: 64 LocalVariableTable: Start Length Slot Name Signature 46 7 6 number I 0 65 0 args [Ljava/lang/String; 16 49 1 numbers [I 24 41 2 movingAverage Lcom/sciatta/hadoop/java/jvm/bytecode/MovingAverage; 64 1 3 avg D StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 33 locals = [ class \"[Ljava/lang/String;\", class \"[I\", class com/sciatta/hadoop/java/jvm/bytecode/MovingAverage, class \"[I\", int, int ] stack = [] frame_type = 248 /* chop */ offset_delta = 25 } SourceFile: \"ForLoop.java\" 类加载器 类的声明周期 加载：找Class文件 验证：验证格式、依赖 准备：静态字段、方法表 解析：符合解析为引用 初始化：静态变量、静态代码块 使用 卸载 类的加载时机 在首次主动使用时初始化 当虚拟机启动时，初始化用户指定的主类，就是启动执行main方法所在的类 当遇到用以新建目标类实例的new指令时，初始化new指令的目标类，就是new一个类的时候要初始化； 当遇到调用静态方法的指令时，初始化该静态方法所在的类； 当遇到访问静态字段的指令时，初始化该静态字段所在的类； 子类的初始化会触发父类的初始化； 如果一个接口定义了default方法，那么直接实现或者间接实现该接口的类的初始化，会触发该接口的初始化； 使用反射API对某个类进行反射调用时，初始化这个类，反射调用要么是已经有实例了，要么是静态方法，都需要初始化； 当初次调用MethodHandle实例时，初始化该MethodHandle指向的方法所在的类。 不会初始化（可能会加载） 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化； 定义对象数组，不会触发该类的初始化； 常量在编译期间会存入调用类的常量池中，本质上没有直接引用定义常量的类，不会触发定义常量所在的类； 通过类名获取Class对象，不会触发类的初始化，Hello.class不会让Hello类初始化； 通过Class.forName加载指定类时，如果指定参数initialize为false时，也不会触发类初始化，其实这个参数是告诉虚拟机，是否对类进行初始化。Class.forName(\"jvm.Hello\")默认会加载Hello类； 通过ClassLoader默认的loadClass方法，也不会触发初始化动作。（加载，但不初始化）。 三类类加载器 BootstrapClassLoader 启动类加载器，其是JVM核心的一部分，由native code实现，没有java引用。当某一个ClassLoader的parent为null时，其parent就是BootstrapClassLoader；加载目录 $JAVA_HOME/jre/lib 和 $JAVA_HOME/jre/classes ，可以通过 -Xbootclasspath 指定路径 ExtClassLoader 扩展类加载器，加载目录 $JAVA_HOME/jre/lib/ext 添加引用类放到JDK的lib/ext下 或 -Djava.ext.dirs 指定路径 AppClassLoader 应用类加载器，加载目录默认是 . 当前目录 class文件放到当前路径下 或 -classpath 或 -cp 指定路径 sun.misc.Launcher JVM的入口类，构造函数负责初始化类加载器的层次结构 加载ExtClassLoader，其继承URLClassLoader，但注意ExtClassLoader的parent ClassLoader是null 加载AppClassLoader，其继承URLClassLoader，AppClassLoader的parent ClassLoader是ExtClassLoader；设置为当前线程的上下文类加载器 双亲委托 当加载一个class时，首先从指定ClassLoader中查找，如AppClassLoader的缓存中查找，若找到则直接返回；否则，向上查找父ClassLoader，如ExtClassLoader和BootstrapClassLoader的缓存 若缓存中没有找到，则自上至下委托给父ClassLoader查找class 所有class延迟加载 添加引用类的方式 放到 JDK 的 lib/ext 下，或 -Djava.ext.dirs java -cp / classpath，或 class 文件放到当前路径 自定义ClassLoader加载 拿到当前执行类的ClassLoader，反射调用 addUrl 方法添加 Jar 或路径（JDK9无效） 内存模型 内存结构 每个线程都只能访问自己的线程栈 每个线程不能访问（看不见）其他线程的局部变量 所有原生类型（指基本类型：byte、short、int、long、float、double、char 和 boolean）的局部变量都存储在线程栈中，因此对其他线程时不可见的 线程可以将一个原生变量值的副本传给另一个线程，但不能共享原生局部变量本身 堆内存中包含java代码中创建的所有对象，不管是哪个线程创建的。其中也包括包装类型（如 Byte、Integer、Long等） 不管是创建一个对象并将其赋值给局部变量（栈中局部变量槽位保存的是对象的引用地址），还是赋值给另一个对象的成员变量，创建的对象都会保存到堆内存中 对象的成员变量与对象本身一起存储在堆上，不管成员变量的类型是原始数值，还是对象引用 类的静态变量和类定义一样都保存在堆中 总结 方法中使用的原生数据类型和对象引用地址在栈上储存；对象、对象成员、类定义、静态变量在堆上 堆内存又称“共享堆”，堆中的所有对象，可以被线程访问，只要拿到对象的引用地址 如果一个线程可以访问某个对象时，也就可以访问该对象的成员变量 如果两个线程同时调用某个对象的同一方法，它们都可以访问到这个对象的成员变量，但每个线程的局部变量副本是独立的（各有各的局部变量表） 内存整体结构 JVM启动参数 具体Java8启动参数说明可以查看 https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html 内存参数关系 GC原理 一般原理 对象是否可用 引用计数 无法解决循环引用的问题 可达性分析 判断对象是否可达。通过一系列“GC Roots”对象作为起点进行搜索，如果“GC Roots”和一个对象没有可达路径，则该对象是不可达的。对于跨代引用，比如在老年代中没有“GC Roots”引用，但在年轻代中有引用关系，这一类关系记录在RememberSet中。注意此阶段暂停的时间，与堆内存的大小，对象的总数没有直接关系，而是由存活对象的数量决定。所以增加堆内存的大小并不会直接影响标记阶段占用的时间。 以下对象被认为是“GC Roots”对象： 当前正在执行的方法里的局部变量和输入参数 活动线程 所有类的静态字段 JNI引用 判断对象是否可回收： 可达性分析 当对象没有重写finalize()方法或finalize()方法被调用过，JVM认为该对象不可以被救活，因此需要回收该对象 垃圾回收算法 Mark-Sweep（标记清除算法） 算法：标记阶段标记出所有要回收的对象；清除阶段清除被标记对象的空间。 优缺点：实现简单，容易产生碎片。 Mark-Copy（标记复制算法） 算法：将可用内存划分为大小相等的两块，每次只使用其中的一块。当进行垃圾回收时，把其中存活的对象全部负责到另一块中，然后把已使用的空间一次清除。（遍历“GC Roots”的过程就可以向另一块复制） 优缺点：不容易产生碎片；当有大量存活对象时，空间利用率低 Mark-Compact（标记整理算法） 算法：先标记存活对象，然后把存活对象向一边移动，然后清理边界以外的内存（先遍历“GC Roots”标记存活对象，然后再向一边移动；遍历的过程不可移动，否则可能会替换未遍历的对象）。 优缺点：不容易产生碎片；内存利用率高；存活对象少且较分散时，移动次数多，效率低 分代收集算法 分代假设：大部分新生对象很快无用；存活较长时间的对象，可能存活更长时间。 算法： 由于新生代每次垃圾回收都要回收大部分对象，因此采用Coping算法。新生代分成一块较大的Eden空间和两块较小的Survivor空间。每次只使用Eden和其中一块Survivor空间。当垃圾回收时，把存活对象放到未使用的Survivor空间，清空Eden和之前使用过的Survivor空间。 由于老年代每次只回收少量对象，因此采用Mark-Compact算法。 垃圾回收过程 Minor GC 新创建对象被分配到Eden，两个Survivor是空的 当Eden空间第一次被填满了，触发Minor GC；将Eden标记存活的对象复制到S0，对象的age+1，然后清空Eden 接着下一次Eden空间被填满了，触发Minor GC；此时是将Eden标记存活的对象复制到S1，对象的age+1；同时，S0存活的对象也会复制到S1，对象的age+1。最后将Eden和S0全部清空。注意，此时S1中包括S0和Eden的存活对象，他们的age是不相同的 Minor GC不断重复，S0和S1的角色也会随之切换。 在一次Minor GC后，当对象的age（基本是Survivor区的对象）达到某一个阈值，则此对象会从年轻代晋升到老年代，同时对象的age+1。之后，会有源源不断的对象晋升到老年代。 Full GC 如果创建一个大对象，Eden区放不下这个大对象，会直接保存在老年代，如果老年代空间也不足，就会触发Full GC。 如果有持久代空间的话，系统当中需要加载的类，调用的方法很多，同时持久代当中没有足够的空间，就触发一次Full GC。 在发生Minor GC之前，虚拟机会先检查老年代的最大的连续内存空间是否大于新生代的所有对象的空间，如果这个条件成立，Minor GC是安全的。如果不成立虚拟机会查看HanlerPromotionFailure设置值是否允许担当失败，如果允许，那么会继续检查老年代最大可用的连续内存空间是否大于历次晋级到老年代对象的平均大小，如果大于就尝试一次Minor GC， 如果小于，或者HanlerPromotionFailure 不愿承担风险就要进行一次Full GC 。 promotion failure发生在Young GC，如果Survivor区当中存活对象的年龄达到了设定值，会就将Survivor区当中的对象拷贝到老年代，如果老年代的空间不足，就会发生promotion failure， 接下去就会发生Full GC。 显式调用System.gc。但不会马上触发Full GC。 常用GC GC分类 串行GC（Serial GC）/ ParNewGC -XX:+UseSerialGC Serial + Serial Old 串行GC对年轻代使用Mark-Copy算法，对老年代使用Mark-Compact算法。两者都是单线程（STW+单线程）的垃圾收集器，不能进行并行处理，所以都会触发STW（全线暂停），停止所有应用线程。因此这种GC算法无法利用多核CPU，不管多少CPU内核，JVM GC时，只能使用单个核心。 CPU利用率高，暂停时间长。适用于几百MB堆内存的JVM，而且是单核CPU比较有用。 -XX:+UseParNewGC ParNew + Serial Old 改进版本的Serial GC，可以配合CMS使用。当使用 -XX:+UseConcMarkSweepGC 时，该选项自动可用。 并行GC（Parallel GC） -XX:+UseParallelGC（JDK8默认；注意如果是单核线程会退化为Mark Sweep Compact GC） -XX:+UseParallelOldGC -XX:+UseParallelGC -XX:+UseParallelOldGC 以上等价 Parallel Scavenge + Parallel Old 年轻代和老年代的垃圾回收都会触发STW事件。年轻代使用Mark-Copy算法；老年代使用Mark-Sweep-Compact算法。-XX:ParallelGCThreads=threads 来指定GC线程数，默认值是CPU核数（因为是STW，GC多线程处理）。 并行垃圾收集器使用于多核服务器，主要目标是增加吞吐量（应用程序线程用时占程序总用时的比例）。因为对系统资源的有效使用，能达到更高的吞吐量。 GC期间，所有CPU并行（STW+多线程）清理，总暂停时间更短 在两次GC周期的间隔期，没有GC线程运行，不会消耗任何系统资源 并发CMS GC Mostly Concurrency Mark and Sweep Garbage Collector -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseConcMarkSweepGC 以上等价 ParNew + CMS + Serial Old（备用） 对年轻代采用并行STW方式的Mark-Copy算法；对老年代使用并发（NO ALL STW+多线程）的Mark-Sweep算法。设计目标是避免老年代垃圾收集时出现长时间卡顿： 不对老年代进行整理，而是使用空闲列表来管理内存空间的回收 在Mark-Sweep阶段的大部分工作和应用线程一起并发执行 默认情况下CMS使用的并发线程数等于CPU核心数的1/4（因为是NO ALL STW，GC和应用多线程同时处理，需要注意因为是并发，所以GC线程会和应用线程争抢CPU时间）。如果服务器是多核CPU，并且主要调优的目标是降低GC停顿导致的系统延迟，则使用CMS是明智选择。CMS也有一些缺点，其中最大的问题是老年代碎片问题，在某些情况下GC会造成不可预测的暂停时间，特别是堆内存较大的情况下。 六个阶段 Initial Mark 初始标记（STW）：只标记与“GC ROOTS”连接的根对象（所以速度非常快） Concurrent Mark 并发标记：并发标记遍历老年代所有存活对象 Concurrent Preclean 并发预清理：前一阶段并发处理，所以标记完成后，仍然可能会出现引用变化，所以用卡片标记“脏”区 Final Remark 最终标记（STW）：对“脏“区做最后修正，完成老年代所有存活对象的标记 Concurrent Sweep 并发清除：删除不再使用的对象，回收占用的内存空间 Concurrent Reset 并发重置：重置CMS算法相关的内部数据，为下一次GC循环做准备 G1 GC Garbage First -XX:+UseG1GC（JDK9默认） -XX:G1NewSizePercent 初始年轻代占整个Heap的大小，默认5% -XX:G1MaxNewSizePercent 最大年轻代占整个Heap的大小，默认60% -XX:G1HeapRegionSize 按照平均堆的大小 (MinHeapSize+MaxHeapSize)/2 划分为2048个目标区块，取同最小RegionSize比较的最大值，但必须满足有效范围1MB~32MB，即不能小于1MB -XX:ParallelGCThreads=n 设置STW阶段并行的worker线程数量。 如果逻辑处理器小于等于8，则默认值n等于逻辑处理器的数量 如果逻辑处理器大于8，则默认值n等于处理器数量的 5/8+3。在大多数情况下都是比较合理的值。 如果是高配置的SPARC系统，则默认值n大约等于逻辑处理器的 5/16 -XX:ConcGCThreads 并发标记的GC线程数。默认是 ParallelGCThreads 的1/4。 -XX:MaxGCPauseMillis=50 预期G1每次执行GC操作的暂停时间，单位是毫秒，默认200毫秒 -XX:+InitiatingHeapOccupancyPercent（IHOP） G1内部并行回收循环启动的阈值，默认为java heap的45% -XX:G1HeapWastePercent G1停止回收的最小内存大小。默认是堆大小的5% -XX:G1ReservePercent G1为了保留一些空间用于年代之间的提升。默认是堆大小的10% -XX:+GCTimeRation 计算花在java应用线程上和花在gc线程上的时间比例，默认是9，同新生代内存的分配比例一致。这个参数的主要目的是使得用户可以控制花在应用上的时间。公式 100/1+GCTimeRation ，如果值是9，则10%的时间会花在GC工作上。Parallel的默认值是99，表示1%的时间用在GC上，因为Parallel GC贯穿整个GC，而G1根据region划分，不需要对全局性扫描整个内存堆 G1 GC的设计目标是将STW停顿的时间和分布，变成可预期且可配置的。 G1 GC有其特定实现： 堆不再分成年轻代和老年代。而是划分多个（通常是2048个）存放对象的小块堆（region）区域。每个小块，可能一会被定义成Eden区，一会被指定为Survivor区，或Old区。逻辑上，所有的Eden区和Survivor区合起来就是年轻代，所有的Old区合起来就是老年代。G1不必每次都收集整个堆空间，可以增量的方式来处理。原则上不能指定G1 GC的年轻代大小。 在并发阶段估算每个小堆块存活对象的总和。回收原则是，垃圾最多的小块会被优先收集。 处理步骤 年轻代模式转移暂停 G1会通过前面一段时间的运行情况来不断调整自己的回收策略和行为，以此保证比较稳定控制暂停时间。在应用程序刚启动时，G1还未执行过（not-yet-executed）并发阶段，也就没有获得任何额外的信息，处于初始的 fully-young 模式。在年轻代空间用满之后，应用线程被暂停，年轻代堆区中的存活对象被复制到存活区，如果还没有存活区，则选择任意一部分空闲的小堆区用作存活区。 Concurrent Marking（并发标记） G1收集器的很多概念建立在CMS的基础上。G1的并发标记通过 Snapshot-At-The-Beginning（开始时快照） 的方式，在标记阶段开始时记下所有的存活对象。即使在标记的同时又有一些变成了垃圾，通过对象的存活信息可以构建出每个小堆区的存活状态，以便回收集能高效地进行选择。 这些信息在接下来的阶段会用来执行老年代区域的垃圾收集。在两种情况下是完全地并发执行的： 一、如果在标记阶段确定某个小堆区只包含垃圾； 二、在STW转移暂停期间，同时包含垃圾和存活对象的老年代小堆区。 当堆内存的总体使用比例达到一定数值时，就会触发并发标记。默认值为 45% ，但也可以通过JVM参数 InitiatingHeapOccupancyPercent 来设置。和CMS一样，G1的并发标记也是由多个阶段组成，其中一些是完全并发的，还有一些阶段需要暂停应用线程。 阶段 1：Initial Mark（初始标记）此阶段标记所有从GC root 直接可达的对象。在CMS中需要一次STW暂停，但G1里面通常是在转移暂停的同时处理这些事情，所以它的开销是很小的。 阶段 2：Root Region Scan（Root区扫描）此阶段标记所有从”根区域“可达的存活对象。 根区域包括：非空的区域，以及在标记过程中不得不收集的区域。因为在并发标记的过程中迁移对象会造成很多麻烦，所以此阶段必须在下一次转移暂停之前完成。如果必须启动转移暂停，则会先要求根区域扫描中止，等它完成才能继续扫描。 阶段 3：Concurrent Mark（并发标记） 此阶段非常类似于CMS：它只是遍历对象图，并在一个特殊的位图中标记能访问到的对象。为了确保标记开始时的快照准确性，所有应用线程并发对对象图执行的引用更新，G1 要求放弃前面阶段为了标记目的而引用的过时引用。 这是通过使用 Pre-Write 屏障来实现的。Pre-Write屏障的作用是：G1在进行并发标记时，如果程序将对象的某个属性做了变更，就会在 log buffers 中存储之前的引用。 阶段 4：Remark（再次标记） 和CMS类似，这也是一次STW停顿，以完成标记过程。对于G1，它短暂地停止应用线程，停止并发更新日志的写入，处理其中的少量信息，并标记所有在并发标记开始时未被标记的存活对象。这一阶段也执行某些额外的清理。 阶段 5：Cleanup（清理） 最后这个小阶段为即将到来的转移阶段做准备，统计小堆区中所有存活的对象，并将小堆区进行排序，以提升GC的效率。此阶段也为下一次标记执行所有必需的整理工作（house-keeping activities）维护并发标记的内部状态。 注意所有不包含存活对象的小堆区在此阶段都被回收了。有一部分是并发的，例如空堆区的回收，还有大部分的存活率计算，此阶段也需要一个短暂的STW暂停，以不受应用线程的影响来完成作业。 Evacuation Pause：Mixed （混合模式转移暂停） 并发标记完成之后，G1将执行一次混合收集（mixed collection），不只清理年轻代，还将一部分老年代区域也加入到 collection set 中。混合模式转移暂停（Evacuation pause）不一定紧跟着并发标记阶段。有很多规则和历史数据会影响混合模式的启动时机。比如，假若在老年代中可以并发地腾出很多的小堆区，就没有必要启动混合模式。因此，在并发标记与混合转移暂停之间，很可能会存在多次 fully-young 转移暂停。添加到回收集的老年代小堆区的具体数字及其顺序，也是基于许多规则来判定的。 其中包括指定的软实时性能指标，存活性以及在并发标记期间收集的GC效率等数据，外加一些可配置的JVM选项。混合收集的过程，很大程度上和前面的 fully-young gc 是一样的。 G1GC回收器 YoungGC YoungGC并不是说现有的Eden区放满了就会马上触发，G1会计算下现在Eden区回收大概要多久时间，如果回收时间远远小于参数 -XX:MaxGCPauseMills 设定的值，那么增加年轻代的region，继续给新对象存放，不会马上做YoungGC，直到下一次Eden区放满，G1计算回收时间接近参数 -XX:MaxGCPauseMills 设定的值，那么就会触发YoungGC MixedGC 不是FullGC，老年代的堆占有率达到参数(-XX:InitiatingHeapOccupancyPercen)设定的值则触发，回收所有的Young和部分Old（根据期望的GC停顿时间确定old区垃圾收集的优先顺序）以及大对象区，正常情况G1的垃圾收集是先做MixedGC，主要使用复制算法，需要把各个region中存活的对象拷贝到别的region里去，拷贝过程中如果发现没有足够的空region能够承载拷贝对象就会触发一次Full GC Full GC 停止系统程序，然后采用单线程进行标记、清理和压缩整理，用来空闲出来一批Region来供下一次MixedGC使用，这个过程是非常耗时的。 注意事项 特别需要注意，某些情况G1触发Full GC，这时会退化使用Serial收集器来完成垃圾的清理工作，即使用单线程完成GC工作，GC暂停时间将达到秒级。 并发模式失败 G1启动标记周期，但在Mix GC之前，老年代被填满，这时G1会放弃标记周期。 解决：增加堆大小，或者调整周期（如增加线程数 -XX:ConcGCThreads）。 晋升失败 没有足够的内存供存活对象或晋升对象使用，由此出发Full GC 解决： 增加-XX:G1ReservePercent预留内存量，并相应增大总的堆大小 减少-XX:+InitiatingHeapOccupancyPercent提前启动标记线程周期 增加-XX:ConcGCThreads增加并行标记线程的数目 巨型对象分配失败 当巨型对象找不到合适的空间进行分配时，就会触发Full GC 解决：增加堆内存或增大-XX:G1HeapRegionSize ZGC / Shenandoad GC -XX:+UnlockExperimentalVMOptions -XX:+UseZGC -Xmx16g 通过着色指针和读屏障，实现几乎全部的并发执行，几毫秒级别的延迟，线性可扩展。 GC最大停顿时间不超过10ms 堆内存支持范围广，几百Mb，大至4TB的从超大堆内存（JDK13升至16TB） 与G1相比，应用吞吐量下降不超过15% 当前只支持Linux x64平台，JDK15后支持MacOs和Windows系统 -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx16g G1的改进版本，跟ZGC类似。 设计为GC线程与应用线程并发执行，通过实现垃圾回收过程的并发处理，改善停顿时间，使得GC执行线程能够在业务处理线程运行过程中进行堆压缩、标记和整理，从而消除绝大部分的暂停时间。 GC适用场景 串行GC UseSerialGC 对单核、小内存（100M）、停顿无要求的客户端应用适用 并行GC UseParallelGC 对多核、内存较大（4G）、吞吐量优先、停顿不敏感的后台批处理服务适用 并发GC UseConcMarkSweepGC 对多核、内存较大（4G）、响应时间优先的高频交易系统适用 但由于CMS不压缩整理，导致内存不连续，在内存较大情况下会导致GC停顿时间较长，延迟不可控 改进版并发GC G1GC 对多核、内存很大（8G）、响应时间优先且可控的高频交易系统适用 但并发模式失败、晋升失败、分配巨型对象失败等情况，会触发Full GC，此时会退化使用Serial收集器单线程来完成垃圾的清理工作，此过程非常耗时应尽量避免 常用GC组合 Serial + Serial Old 实现单线程低延迟垃圾回收机制 ParNew + CMS 实现多线程低延迟垃圾回收机制 Parallel Scavenge + Parallel Old 多线程高吞吐量垃圾回收机制（CPU资源都用来最大程度处理业务） G1 GC 堆内存较大，整体平均GC时间可控 GC参数占比 参数 并行GC占比 G1GC MinHeapSize 物理内存1/64 物理内存1/64 MaxHeapSize 物理内存1/4 物理内存1/4 NewSize MinHeapSize/3 MinHeapSize*5% MaxNewSize MaxHeapSize/3 MaxHeapSize*60% OldSize NewSize*2 NewRatio=2 new:old=1:2 SurvivorRatio=8 survivor:eden=2:8 G1HeapRegionSize max((MinHeapSize+MaxHeapSize)/2/2048, 1)注：2048是目标region数量，1是最小RegionSize 内存分析调优 占用大小 对象头和对象引用 在64位JVM中，对象头占用12byte，但需要以8字节（64bit）对齐，因此一个空类的实例至少占用16字节。 在32位JVM中，对象头占用8byte，以4字节对齐。 通常在32位JVM，以及内存小于 -Xmx32G 的64位JVM上（默认开启指针压缩），一个引用占的内存默认是4字节。 包装类型 比原生数据类型消耗的内存多。如： Integer：int占用4字节；而Integer是对象需要占用16字节，比原生对象多占用 (16-4)/4=300% 的内存空间 Long：long占用8字节；而Long是对象需要占用16字节，比原生对象多占用 (16-8)/8=100% 的内存空间 数组 一维数组 int[256] 占用1040字节，二维数组 int[128][2] 占用3600字节，每一个独立的数组都是一个对象。里面的有效存储空间是一样的，但3600比1040多了246%的额外开销。 字符串 String对象的空间随着内部字符数组的增长而增长。String类的对象有24个字节的额外开销。 异常 OutOfMemoryError：Java heap space 创建新的对象时，堆内存的空间不足以存放新创建的对象 超出预期的访问量/数据量（应用设计时，机器有容量限制） 内存泄露 OutOfMemoryError：PermGen space/OutOfMemoryError：Metaspace 加载到内存中的class数量太多或体积太大，超过了PermGen的大小 OutOfMemoryError：Unable to create new native thread 程序创建的线程数量达到上限值的异常信息 分析调优 高分配速率（High Allocation Rate） 分配速率表示单位时间内分配的内存量。通常使用MB/sec作为单位。上一次垃圾收集之后，与下一次GC开始之前的年轻代使用量，两者的差值除以时间，就是分配速率。 分配速率过高就会严重影响程序的性能，在JVM中可能会导致巨大的GC开销。最终效果是，影响Minor GC的次数和时间，进而影响吞吐量。 解决 在某些情况下，只要增加年轻代大小即可降低分配速率过高所造成的影响。增加年轻代空间并不会降低分配速率，但会减少GC的频率。如果每次GC后只有少量对象存活，minor GC 的暂停时间就不会明显增加。 过早提升（Premature Promotion） 提升效率用于衡量单位时间内从年轻代提升到老年代的数据量。一般使用MB/sec作为单位。 JVM会将长时间存活的对象从年轻代提升到老年代。根据分代假设，可能存在一种情况，老年代中不仅有存活时间长的对象，也可能存在存活时间短的对象。即对象存活的时间还不够长时就被提升到了老年代。Major GC不是为了频繁回收而设计的，但Major GC若需要清除生命短暂的对象，就会导致GC暂停时间过长，会严重影响系统的吞吐量。 过早提升表现 短时间内频繁 Full GC 每次 Full GC 后老年代的使用率都很低，在10~20%以下 提升速率接近分配速率 解决 增加年轻代大小或增加堆内存，Full GC的次数会减少，只是会对minor GC的持续时间产生影响 减少每次批处理的数量（业务层面） 优化数据结构，减少内存消耗 测试分析 开启GC日志 串行 java -XX:+UseSerialGC -Xms512m -Xmx512m -Xloggc:gc.serial.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -cp hadoop-java-jvm-1.0-SNAPSHOT.jar com.sciatta.hadoop.java.jvm.gc.GCLogAnalysis 1 2000 100000 # young GC # gc前 young（152288/157248=97%）heap(471606/506816=93%) # gc后 young（13862/157248=9%） heap（345388/506816=68%） # gc后 old增长 (152288-13862)-(471606-345388)=12208k 2020-10-27T13:55:19.387+0800: 0.583: [GC (Allocation Failure) 2020-10-27T13:55:19.387+0800: 0.583: [DefNew: 152288K->13862K(157248K), 0.0045260 secs] 471606K->345388K(506816K), 0.0045902 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] # full GC # gc前 young(153638/157248=98%) old(331525/349568=95%) heap(485164/506816=96%) # gc后 young(153638/157248=98%) old(211353/349568=60%) heap(211353/506816=42%) 2020-10-27T13:55:19.406+0800: 0.601: [GC (Allocation Failure) 2020-10-27T13:55:19.406+0800: 0.602: [DefNew: 153638K->153638K(157248K), 0.0000137 secs]2020-10-27T13:55:19.406+0800: 0.602: [Tenured: 331525K->211353K(349568K), 0.0258594 secs] 485164K->211353K(506816K), [Metaspace: 3460K->3460K(1056768K)], 0.0259484 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 并行 java -XX:+UseParallelGC -Xms512m -Xmx512m -Xloggc:gc.parallel.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -cp hadoop-java-jvm-1.0-SNAPSHOT.jar com.sciatta.hadoop.java.jvm.gc.GCLogAnalysis 1 2000 100000 # young GC # gc前 young（131584/153088=86%）heap(131584/502784=26%) # gc后 young（14196/153088=9%） heap（14196/502784=3%） 2020-10-27T14:15:01.204+0800: 0.152: [GC (Allocation Failure) [PSYoungGen: 131584K->14196K(153088K)] 131584K->14196K(502784K), 0.0043789 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] # full GC # gc前 young(13711/150016=9%) old(335470/349696=96%) heap(349181/499712=70%) # gc后 young(0/150016=0%) old(193021/349696=55%) heap(193021/499712=39%) 2020-10-27T14:15:01.767+0800: 0.715: [Full GC (Ergonomics) [PSYoungGen: 13711K->0K(150016K)] [ParOldGen: 335470K->193021K(349696K)] 349181K->193021K(499712K), [Metaspace: 3459K->3459K(1056768K)], 0.0297430 secs] [Times: user=0.05 sys=0.00, real=0.03 secs] CMS java -XX:+UseConcMarkSweepGC -Xms512m -Xmx512m -Xloggc:gc.cms.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -cp hadoop-java-jvm-1.0-SNAPSHOT.jar com.sciatta.hadoop.java.jvm.gc.GCLogAnalysis 1 2000 100000 # young GC # gc前 young（136168/153344=89%）heap(136168/507264=27%) # gc后 young（14049/153344=9%） heap（14049/507264=3%） 2020-10-27T17:44:16.836+0800: 0.126: [GC (Allocation Failure) 2020-10-27T17:44:16.836+0800: 0.126: [ParNew: 136168K->14049K(153344K), 0.0043274 secs] 136168K->14049K(507264K), 0.0044123 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] # full GC # Initial Mark # old(182341/353920=52%) heap(202089/507264=40%) 2020-10-27T17:44:17.114+0800: 0.404: [GC (CMS Initial Mark) [1 CMS-initial-mark: 182341K(353920K)] 202089K(507264K), 0.0004290 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 2020-10-27T17:44:17.114+0800: 0.405: [CMS-concurrent-mark-start] 2020-10-27T17:44:17.118+0800: 0.408: [CMS-concurrent-mark: 0.004/0.004 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 2020-10-27T17:44:17.118+0800: 0.408: [CMS-concurrent-preclean-start] 2020-10-27T17:44:17.119+0800: 0.409: [CMS-concurrent-preclean: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 2020-10-27T17:44:17.119+0800: 0.409: [CMS-concurrent-abortable-preclean-start] 2020-10-27T17:44:17.372+0800: 0.662: [CMS-concurrent-abortable-preclean: 0.016/0.253 secs] [Times: user=0.32 sys=0.04, real=0.25 secs] # Final Remark # young(62599/153344=41%) old(344192/353920=97%) heap(406791/507264=80%) 2020-10-27T17:44:17.377+0800: 0.667: [GC (CMS Final Remark) [YG occupancy: 62599 K (153344 K)]2020-10-27T17:44:17.377+0800: 0.667: [Rescan (parallel) , 0.0005818 secs]2020-10-27T17:44:17.377+0800: 0.668: [weak refs processing, 0.0000108 secs]2020-10-27T17:44:17.377+0800: 0.668: [class unloading, 0.0002870 secs]2020-10-27T17:44:17.378+0800: 0.668: [scrub symbol table, 0.0003714 secs]2020-10-27T17:44:17.378+0800: 0.668: [scrub string table, 0.0001691 secs][1 CMS-remark: 344192K(353920K)] 406791K(507264K), 0.0015126 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 2020-10-27T17:44:17.378+0800: 0.669: [CMS-concurrent-sweep-start] 2020-10-27T17:44:17.379+0800: 0.670: [CMS-concurrent-sweep: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 2020-10-27T17:44:17.379+0800: 0.670: [CMS-concurrent-reset-start] 2020-10-27T17:44:17.381+0800: 0.671: [CMS-concurrent-reset: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] G1 java -XX:+UseG1GC -Xms512m -Xmx512m -Xloggc:gc.g1.log -XX:+PrintGC -XX:+PrintGCDateStamps -cp hadoop-java-jvm-1.0-SNAPSHOT.jar com.sciatta.hadoop.java.jvm.gc.GCLogAnalysis 1 2000 100000 2020-10-27T23:13:22.295+0800: 0.126: [GC pause (G1 Evacuation Pause) (young) 60M->9179K(1024M), 0.0045457 secs] 2020-10-27T23:13:23.020+0800: 0.851: [GC pause (G1 Evacuation Pause) (mixed) 808M->665M(1024M), 0.0036246 secs] 压测工具 使用压测工具 wrk 或 sb 测试命令 wrk -t8 -c40 -d30s http://localhost:8088/api/hello Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/language/java/jvm/JDK工具.html":{"url":"src/language/java/jvm/JDK工具.html","title":"JDK工具","keywords":"","body":"JDK内置命令行工具 jps Lists the instrumented Java Virtual Machines (JVMs) on the target system. This command is experimental and unsupported. # -m Displays the arguments passed to the main method. The output may be null for embedded JVMs. # -l Displays the full package name for the application's main class or the full path name to the application's JAR file. # -v Displays the arguments passed to the JVM. jps -mlv jinfo Generates configuration information. This command is experimental and unsupported. jinfo jstat* Monitors Java Virtual Machine (JVM) statistics. This command is experimental and unsupported. -gc 需要重点关注OU（老年代的使用量）、YGCT（年轻代GC消耗的总时间）、FGCT（Full GC 消耗的时间） # -gcutil Displays a summary about garbage collection statistics. # 1000 间隔时间 默认毫秒 # 3 显示次数 # -t 第一列显示自启动以来的时间戳，单位秒 jstat -gcutil -t 1000 3 # S0: Survivor space 0 utilization as a percentage of the space's current capacity. # S1: Survivor space 1 utilization as a percentage of the space's current capacity. # E: Eden space utilization as a percentage of the space's current capacity. # O: Old space utilization as a percentage of the space's current capacity. # M: Metaspace utilization as a percentage of the space's current capacity. # CCS: Compressed class space utilization as a percentage. # YGC: Number of young generation GC events. # YGCT: Young generation garbage collection time. # FGC: Number of full GC events. # FGCT: Full garbage collection time. # GCT: Total garbage collection time. Timestamp S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 2538.2 1.75 0.00 35.64 60.57 98.26 97.02 38 0.222 2 0.199 0.422 2539.3 1.75 0.00 35.64 60.57 98.26 97.02 38 0.222 2 0.199 0.422 2540.3 1.75 0.00 35.78 60.57 98.26 97.02 38 0.222 2 0.199 0.422 # -gc Garbage-collected heap statistics. # jstat -gc -t 7534 1000 3 jstat -gc -t 1000 3 # S0C: Current survivor space 0 capacity (kB). # S1C: Current survivor space 1 capacity (kB). # S0U: Survivor space 0 utilization (kB). # S1U: Survivor space 1 utilization (kB). # EC: Current eden space capacity (kB). # EU: Eden space utilization (kB). # OC: Current old space capacity (kB). # OU: Old space utilization (kB). # MC: Metaspace capacity (kB). # MU: Metacspace utilization (kB). # CCSC: Compressed class space capacity (kB). # CCSU: Compressed class space used (kB). # YGC: Number of young generation garbage collection events. # YGCT: Young generation garbage collection time. # FGC: Number of full GC events. # FGCT: Full garbage collection time. # GCT: Total garbage collection time. Timestamp S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 2894.6 2112.0 2112.0 36.9 0.0 17472.0 11941.0 43300.0 26227.7 37632.0 36976.8 4352.0 4222.2 38 0.222 2 0.199 0.422 2895.6 2112.0 2112.0 36.9 0.0 17472.0 11941.0 43300.0 26227.7 37632.0 36976.8 4352.0 4222.2 38 0.222 2 0.199 0.422 2896.7 2112.0 2112.0 36.9 0.0 17472.0 11941.0 43300.0 26227.7 37632.0 36976.8 4352.0 4222.2 38 0.222 2 0.199 0.422 jmap Prints shared object memory maps or heap memory details for a process, core file, or remote debug server. This command is experimental and unsupported. # -heap 打印堆内存的配置和使用信息 jmap -heap 7534 Heap Configuration: MinHeapFreeRatio = 40 # 空余堆小于40%，增加到Xmx MaxHeapFreeRatio = 70 # 空余堆大于70%，减小到Xms MaxHeapSize = 1048576000 (1000.0MB) NewSize = 10485760 (10.0MB) MaxNewSize = 349503488 (333.3125MB) OldSize = 20971520 (20.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: New Generation (Eden + 1 Survivor Space): capacity = 20054016 (19.125MB) used = 363760 (0.3469085693359375MB) free = 19690256 (18.778091430664062MB) 1.8139010161356208% used Eden Space: capacity = 17891328 (17.0625MB) used = 342360 (0.32649993896484375MB) free = 17548968 (16.736000061035156MB) 1.9135527558379122% used From Space: capacity = 2162688 (2.0625MB) used = 21400 (0.02040863037109375MB) free = 2141288 (2.0420913696289062MB) 0.9895093513257576% used To Space: capacity = 2162688 (2.0625MB) used = 0 (0.0MB) free = 2162688 (2.0625MB) 0.0% used tenured generation: capacity = 44339200 (42.28515625MB) used = 30048248 (28.65624237060547MB) free = 14290952 (13.628913879394531MB) 67.76903507505774% used 16028 interned Strings occupying 1479120 bytes. # 查看堆中类的实例占用空间的histogram jmap -histo 7534 > jmap_hdfs # dump堆内存 jmap -dump:format=b,file=7534.hprof 7534 jstack Prints Java thread stack traces for a Java process, core file, or remote debug server. This command is experimental and unsupported. # 长列表模式，将线程相关的locks信息一起输出，比如持有的锁，等待的锁 jstack -l 7534 jcmd* Sends diagnostic command requests to a running Java Virtual Machine (JVM). jcmd 7534 help jrunscript/jjs Runs a command-line script shell that supports interactive and batch modes. This command is experimental and unsupported. # -e 计算指定的脚步 jrunscript -e \"cat('https://www.baidu.com')\" # 启动nashorn引擎交互 jrunscript jhat 内存Dump分析工具 JDk内置图形化工具 jconsole Starts a graphical console that lets you monitor and manage Java applications. jvisualvm Visually monitors, troubleshoots, and profiles Java applications. jmc Java Mission Control is a Profiling, Monitoring, and Diagnostics Tools Suite. Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/language/java/io/NIO模型与Netty.html":{"url":"src/language/java/io/NIO模型与Netty.html","title":"NIO模型与Netty","keywords":"","body":"IO模型 阻塞 非阻塞 同步 阻塞I/O模型、I/O多路复用模型 非阻塞I/O模型、信号驱动I/O模型 异步 异步I/O模型 四种状态指的是应用进程同内核的通讯过程 阻塞 / 非阻塞 指的是等待内核准备数据，应用进程是否阻塞；即发起请求是否阻塞 同步 / 异步 指的是内核准备好数据后，将数据由内核缓冲区复制到用户空间的进程缓存区过程，由应用进程主动触发获取数据，还是被动通知；即读写数据是否阻塞 阻塞程度：阻塞I/O > 非阻塞I/O > I/O多路复用模型 > 信号驱动I/O > 异步I/O ，效率由低到高。 Java对于IO模型的支持 = 1.4 Java的IO模型支持了I/O多路复用模型，相对于之前的IO模型，这是一个新的模型，所以称之为NIO（New IO） = 1.7 Java对NIO包进行了升级，支持了异步I/O（Asynchronous IO），简称为AIO 阻塞IO（blocking I/O） 应用进程一次recvfrom指令调用，内核的两个阶段 准备数据 和 复制数据 都被阻塞。 非阻塞IO（noblocking I/O） 应用进程多次recvfrom指令调用，当内核没有准备好数据时，不会阻塞，而是返回一个Error。当内核准备好数据时，此时应用进程的recvfrom指令调用被阻塞，直到数据拷贝到应用进程的缓冲区。 IO多路复用（I/O multiplexing） IO复用同非阻塞IO本质一样，但其利用了新的select系统调用，由内核负责查询是否准备好数据的轮询操作。看似比非阻塞IO还多了一个select指令调用开销，但是可以同时处理多个网络连接的IO。Server端优化的终极目标：Server端使用尽量上的线程，来处理尽量多的Client请求。 当用户进程调用了select指令，应用进程会被阻塞；而同时，内核会“监视”所有select负责的socket，当任何一个socket中的数据准备好时，select就会返回。此时应用进程再调用recvfrom指令。 同多线程 + blocking I/O相比，select可以同时处理多个网络连接 多路复用的优势不是为了处理连接更快，而是为了支持更多的连接。比如网络连接是长连接或IO阻塞时间长的话，多线程 + blocking I/O的方式会大量使用线程资源，在任务未完成之前或Client未关闭前，无法将空闲线程归还到线程池中，可能的结果就是导致大量连接请求无法及时响应；而多路复用则可以使用单进程就可以同时处理多个链接请求。 当链接数不是很大，多线程 + blocking I/O的方式性能会更好一些，因为只有一次系统调用；而多路复用是两次系统调用。 对于一次读取数据请求分为两个阶段：数据准备和数据复制。对于数据准备的时间是不确定性的，因为客户端什么时候发送数据不确定，而对于数据复制的时间是有限的。多路复用就是利于一个监视线程监听多个连接是否完成数据准备，当某一个连接完成数据准备，select恢复，将任务分配给一个工作线程处理，而工作线程读取数据，内核数据复制阻塞的时间是非常小的，从而使得工作任务大部分时间可以充分利用CPU进行计算工作，而不是阻塞等待IO完成浪费CPU资源。 信号驱动IO（signal blocking I/O） 异步IO（asynchronous I/O） NIO Reactor模型 单Reactor单线程模型 单Reactor多线程模型 主从Reactor模型 Netty Netty运行原理 Netty优化 不要阻塞 EventLoop 系统参数优化 ulimit -a 增大最大进程数 /proc/sys/net/ipv4/tcp_fin_timeout, TcpTimedWaitDelay 缩短TIME_WAIT等待时间 缓冲区优化 SO_RCVBUF 接收缓冲区 SO_SNDBUF 发送缓冲区 SO_BACKLOG 保持连接状态 REUSEXXX 重用端口 心跳频率周期优化 心跳机制与断线重连 内存 与 ByteBuffer 优化 DirectBuffer与HeapBuffer 映射堆外内存，零拷贝 其他优化 ioRatio Watermark TrafficShaping 流控 网络程序优化 粘包与拆包 对报文没有指定长度，没有结束符。客户端和服务端要约定报文传递规则。 网络拥堵与Nagle算法优化 TCP_NODELAY MTU: Maxitum Transmission Unit 最大传输单元 1500Byte MSS: Maxitum Segment Size 最大分段大小 1460Byte，其中TCP头20Byte，IP头20Byte 如果网络上按字节发送，都要带40Byte头，传输不经济划算，其次会带来网络拥堵。 优化条件 启用Nagle算法（默认），关闭TCP_NODELAY，缓冲区满或达到超时才发送数据包，减少网络传输数据包，适用于并发高、数据量大的场景 禁用Nagle算法，启动TCP_NODELAY。适用于对延迟敏感、数据量小的场景，如SSH会话 连接优化 TCP建立连接（3次握手） Client -> Server SYN（你在吗？） Server -> Client ACK（我在！） + SYN（你在吗？） Client收到ACK后，状态变为ESTABLISHED Client -> Server ACK（我在！） Server收到ACK后，状态变为ESTABLISHED TCP关闭连接（4次挥手） Client -> Server FIN（我要离开！） Server -> Client ACK（第一次确认！） Server状态变为CLOSE_WAIT Server -> Client FIN（我要离开！） + ACK（第二次确认！） Client状态变为TIME_WAIT，需要等待2MSL后，状态变为CLOSED Client -> Server ACK（确认！） Server状态变为CLOSED 优化条件 缩短2MSL等待周期 开启端口复用 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/language/java/concurrency/并发编程.html":{"url":"src/language/java/concurrency/并发编程.html","title":"并发编程","keywords":"","body":"多线程基础 线程创建过程 线程状态 线程状态 Thread.sleep 当前线程调用此方法，释放CPU，不释放对象锁。作用：给其他线程执行机会的最佳方式。 Thread.yield 当前线程调用此方法，释放CPU，不释放对象锁。作用：让相同优先级的线程轮流执行，但并不保证一定会轮流执行。实际中无法一定保证yield达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。yield不会导致阻塞。该方法与sleep类似，只是不能由用户指定暂停多长时间。 Object#join 当前线程里调用其它线程 t 的 join 方法，当前线程进入WAITING/TIMED_WAITING 状态，当前线程不会释放已经持有的非t对象锁，但可以释放持有的t对象锁，相当于 t.wait()。线程 t 执行完毕或者 millis 时间到，当前线程进入就绪状态。 Object#wait 当前线程调用对象的wait方法，当前线程释放对象锁，进入等待队列。依靠 notify/notifyAll 唤醒或者 wait(long timeout) 到timeout时间自动唤醒。 Object#notify 唤醒在此对象监视器上等待的单个线程，选择是任意性的。notifyAll 唤醒在此对象监视器上等待的所有线程。 中断和异常 线程内部自己处理异常，不溢出到外层。 如果线程被 Object.wait，Thread.join 和 Thread.sleep 三种方法之一阻塞，此时调用该线程的interrupt() 方法，那么该线程将抛出一个 InterruptedException 中断异常（该线程必须事先预备好处理此异常），从而提早地终结被阻塞状态。如果线程没有被阻塞，这时调用interrupt() 将不起作用，直到执行到 wait()，sleep() 或 join() 时，才马上会抛出InterruptedException。 清除中断标志有两种情况： 遇到wait()，sleep() 或 join() 时，捕获异常，此时的中断标志被清除 Thread.interrupted，当中断时返回true，同时清除中断标志 如果是计算密集型的操作，可分段处理，每个片段检查状态是否需终止。 并发性质 原子性 原子操作，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。 可见性 对于可见性，Java 提供了 volatile 关键字来保证可见性。当一个共享变量被 volatile 修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值（注意JVM的副本机制）。另外，通过 synchronized 和 Lock 也能够保证可见性，synchronized 和 Lock 能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。 volatile volatile 并不能保证原子性。 读取：每次读取都强制从主内存刷数据 使用场景：单个线程写；多个线程读 原则：能不用就不用，不确定的时候也不用 替代方案：Atomic原子操作类 内存屏障 // 1和2不会重排到3后面 // 4和5不会重排到3前面 // 同时可以保证1和2的结果对3、4和5可见 x=2; // 1 y=0; // 2 flag=true; // 3 flag是volatile x=4; // 4 y=-1 // 5 synchronized 同步块比同步方法更高效，尽量缩小同步范围，提高并发度。同步块中用于控制同步的对象，尽量用小对象，不使用this。 有序性 Java允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。可以通过volatile关键字来保证一定的”有序性“（通过synchronized 和 Lock保证）。 happens-before原则（先行发生原则）： 程序次序规则：一个线程内，按代码先后顺序 锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作 Volatile变量规则：对同一个变量的写操作先行发生于后面对这个变量的读操作 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出A先于C 线程启动规则：Thread对象的start()方法先行与此线程的每一个动作 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，通过Thread.join()方法（结束阻塞）、Thread.isAlive()的返回值检测到线程已经终止执行 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 ThreadLocal 线程本地变量 场景：每个线程一个副本 不改方法签名静默传参。可以看做是 Context 模式，减少显式传递参数 及时进行清理，防止内存泄漏 JUC基础 锁机制类 问题 synchronized 加锁；wait / notify 解锁/加锁 同步块的阻塞无法中断（不能 Interruptibly） 同步块的阻塞无法控制超时（无法自动解锁） 同步块无法异步处理锁（即不能立即知道是否可以拿到锁） 同步块无法根据条件灵活的加锁解锁（即只能跟同步块范围一致） Lock更自由的锁 使用方式灵活可控 性能开销小 锁工具包: java.util.concurrent.locks ReadWriteLock读写锁 ReadWriteLock管理一组锁，分别是一个读锁和一个写锁。写锁（独占锁），在写线程运行的时候，其他的读、写线程阻塞；读锁（共享锁），在没有写锁的时候，多个读线程可以同时持有读锁。通过锁分离，提高并发性。所有读写锁的实现必须确保写操作对读操作的内存影响。每次只能有一个写线程，但是同时可以有多个线程并发地读数据。 ReadWriteLock 适用于读多写少的并发情况。 ### Condition 通过 Lock.newCondition()创建。可以看做是 Lock 对象上的信号。类似于 wait/notify，相应api为 await/signal。 ### LockSupport锁当前线程 LockSupport 类似于 Thread 类的静态方法，专门用于处理本线程。阻塞当前线程，但不释放锁资源。 unpark需要由其他线程调用，并且以被park的线程作为参数。因为一个被park的线程，无法自己唤醒自己，所以需要其他线程来唤醒。 ### 锁的区分 - 可重入锁：同一线程可以重复获取同一个锁 - 公平锁 / 非公平锁 - 没有获得锁的线程进入等待队列，等待时间久的线程（先入队列）优先获得锁，称为公平锁 - 所有等待的线程都有机会获得锁，称为非公平锁 - 乐观锁 / 悲观锁 - 先访问资源，若已被修改，则自旋重试。不上锁，称为乐观锁 - 访问资源之前，先上锁，其他线程无法访问。修改后再释放锁，称为悲观锁 ### 最佳实践 1. 永远只在更新对象的成员变量时加锁 2. 永远只在访问可变的成员变量时加锁 3. 永远不在调用其他对象的方法时加锁 最小使用锁 降低锁范围：锁定代码的范围/作用域 细分锁粒度：将一个大锁，拆分成多个小锁 并发原子类 问题 sum++多线程安全问题。对于基础数据类型的并发补充实现，线程安全。 Atomic工具类 java.util.concurrent.atomic AtomicInteger 无锁技术的底层实现 volatile 保证读写操作都可见（注意不保证原子） 使用 CAS 指令，作为乐观锁实现，通过自旋重试保证写入。 Unsafe API - Compare-And-Swap CPU 硬件指令支持: CAS 指令 有锁 or 无锁？ CAS 本质上没有使用锁。并发压力跟锁性能的关系： 压力非常小，性能本身要求就不高，有锁、无锁差别不明显 压力一般的情况下，无锁更快，大部分都一次写入 压力非常大时，自旋导致重试过多，资源消耗很大。有锁较好。 LongAdder 通过分段思想对原子类AtomicLong改进 AtomicInteger 和 AtomicLong 里的 value 是所有线程竞争读写的热点数据 将单个 value 拆分成跟线程一样多的数组 Cell[] 每个线程写自己的 Cell[i]++，最后对数组求和 多路归并思想 快排 G1GC ConcurrentHashMap MapReduce 信号量工具类 问题 多个线程间的协作，可以通过 wait/notify、Lock/Condition，但如果需要更为精细化的控制，则实现起来非常复杂还容易出错。 更复杂的应用场景 我们需要控制实际并发访问资源的并发数量 我们需要多个线程在某个时间同时开始运行 我们需要指定数量线程到达某个状态再继续处理 AQS AbstractQueuedSynchronizer，即队列同步器。它是构建锁或者其他同步组件的基础（如Semaphore、CountDownLatch、ReentrantLock、ReentrantReadWriteLock），是JUC并发包中的核心基础组件。 AbstractQueuedSynchronizer：抽象队列式的同步器 两种资源共享方式：独占 OR 共享，子类负责实现公平 OR 非公平 Semaphore信号量 准入数量 N（限流） N =1 则等价于独占锁 Semaphore的本质是共享锁，限制同时访问锁的数量。 CountdownLatch 场景：Master 线程等待 Worker 线程把任务执行完 类似于fork/join的多线程处理，master线程阻塞，等待多个slave线程并发执行完成后，再汇总统计结果。 CyclicBarrier 场景：任务执行到一定阶段，等待其他任务对齐，对齐之后一起先下运行 与CountdownLatch区别 CountdownLatch递减；CyclicBarrier递增 CountdownLatch在主线程通过await阻塞，其他线程countDown，类似join；CyclicBarrier不在主线程阻塞，而是在每个被调用线程处通过await阻塞，等待所有线程对齐（满足parties），然后所有线程同时退出阻塞状态继续执行 对于从阻塞状态恢复的线程，CountdownLatch不可重复利用，而CyclicBarrier可以reset重复利用（循环屏障） 线程池 线程池从功能上看，就是一个任务执行器 Excutor 执行者，顶层接口 ExcutorService 接口 API shutdown()：停止接收新任务，原来的任务继续执行 shutdownNow()：停止接收新任务，原来的任务停止执行 awaitTermination(long timeOut, TimeUnit unit)：当前线程阻塞 submit：有返回值，用 Future 封装，异常可以被catch execute方法：无返回值，无法cache异常 ThreadFactory 线程工厂 Excutors 工具类 newSingleThreadExecutor 创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newFixedThreadPool 创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 先保证核心线程数处理任务，多余的任务置入队列，再多余的任务则新增线程处理直到达到最大线程数，再多余的任务可以根据丢弃策略处理。算法同时满足CPU密集和IO密集型任务处理。 newCachedThreadPool 创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 适用于大量可以快速处理的小任务，提高并发度。 newScheduledThreadPool 创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 schedule 延迟执行一次性任务 scheduleAtFixedRate 延迟+定时执行任务，period：本次开始时间-上次开始时间 scheduleWithFixedDelay 延迟+定时执行任务，period：本次开始时间-上次结束时间 ThreadPoolExecutor 具体线程池实现类 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 线程池参数 corePoolSize 线程池中线程的数量，即使是空闲状态 maximumPoolSize 线程池中允许的最大线程数量 keepAliveTime 当线程池中的线程数量超过了corePoolSize，多余的线程在结束前等待任务的最大时间 unit keepAliveTime参数的时间单位 BlockingQueue 缓存队列 ArrayBlockingQueue：规定大小的 BlockingQueue，其构造必须指定大小。其所含的对象是 FIFO 顺序排序的 LinkedBlockingQueue：大小不固定的 BlockingQueue，若其构造时指定大小，生成的 BlockingQueue 有大小限制，不指定大小，其大小由 Integer.MAX_VALUE 来决定。其所含的对象是 FIFO 顺序排序的 PriorityBlockingQueue：类似于 LinkedBlockingQueue，但是其所含对象的排序不是 FIFO，而是依据对象的自然顺序或者构造函数的 Comparator 决定 SynchronizedQueue：特殊的 BlockingQueue，对其的操作必须是放和取交替完成 ThreadFactory 创建自定义新线程 RejectedExecutionHandler 拒绝策略 ThreadPoolExecutor.AbortPolicy：丢弃任务并抛出 RejectedExecutionException 异常（默认） ThreadPoolExecutor.DiscardPolicy：丢弃任务，但是不抛出异常 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新提交被拒绝的任务 ThreadPoolExecutor.CallerRunsPolicy：由调用线程（提交任务的线程）处理该任务 线程池方法 execute(Runnable command) 方法逻辑 当前线程数小于corePoolSize时，创建工作线程（延迟创建） 当前线程数大于等于corePoolSize时，保存在阻塞队列中 阻塞队列容量满，则继续创建工作线程，直到maximumPoolSize 当线程数大于等于maximumPoolSize时，多余任务执行拒绝策略处理器 当大于等于corePoolSize时，任务首先保存在阻塞队列中。适用于CPU密集型任务，因为队列中的任务可以被快速获取和执行不至于大量积压任务，并且创建大量的线程没有用，频繁切换线程上下文会导致性能下降。之后，创建工作线程直到满足maximumPoolSize，适用于IO密集型任务，当线程处理IO时可以释放CPU资源，多余的线程可以充分打满CPU提高利用率。 Callable/Future/FutureTask/CompletableFuture Callable可返回结果、可抛出异常的任务；Runnable没有返回结果、不会抛出异常。任务是由线程执行的基本单元 Future接口，表示一个异步计算结果，提供检查计算是否完成、取消计算任务、获取计算结果 FutureTask可取消的异步计算任务。本质是一个供线程执行的任务，因为实现了Runnable接口；同时，其还实现了Future接口，因此可以异步获得计算结果 被调用线程运行时，调用FutureTask的run方法，其委托调用Callable执行任务 可以通过FutureTask异步获取任务执行结果 CompletableFuture 异步、回调、组合 并发集合类 线程安全是写冲突和读写冲突导致的。最简单办法就是，读写都加锁。 ArrayList 的方法都加上 synchronized -> Vector Collections.synchronizedList，强制将 List 的操作加上同步 Arrays.asList，不允许添加删除，但是可以 set 替换元素 Collections.unmodifiableList，不允许修改内容，包括添加删除和 set CopyOnWriteArrayList 读写分离，最终一致。容忍一定读取数据的滞后情况，但可以保证正确性。 无锁并发读 写加锁 将原容器拷贝一份，写操作作用在新副本上，需要加锁。此过程若有读操作，则会作用在原容器上 操作完成后，将原容器引用指向新副本。切换过程，用volatile保证切换过程对读线程立即可见 ConcurrentHashMap Java 7 分段锁，默认16个Segment，降低锁粒度。 根据哈希码高sshift位决定Segment数组的index 根据哈希码决定HashEntry数组的index Java 8 为进一步提高并发性，摒弃了分段锁的方案，而是直接使用一个大的数组。在链表长度到8 & 数组长度到64时，使用红黑树。 线程间协作与通信 线程间共享 static / 实例变量（堆内存） Lock synchronized 线程间协作 Thread#join Object#wait / notify / notifyAll Future / Callable CountdownLatch CyclicBarrier Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/language/java/algorithm/数据结构和算法.html":{"url":"src/language/java/algorithm/数据结构和算法.html","title":"数据结构和算法","keywords":"","body":"线性表 链表 静态链表 基于数组实现，元素包括value和cursor，其中cursor指向数组中的任意一个元素的索引位置。实现的思路是创建两条虚拟链表，一条是备用链表，作用是连接未使用的空间；另一条是链表，作用是连接链表的各个节点。 数组的第一个元素保留，作为备用链的头节点；数组的最后一个元素保留，作为链表的头结点 链表是否为空，判断链表的头结点，即数组的最后一个元素的curse是否等于0，为0则链表为空 链表是否为满，假设底层实现的数组不支持动态扩充，判断备用链表的头结点是否指向数组的最后一个元素，即curse等于n-1，则链表为满 链表中环的检测 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/language/java/spring/SpringCore核心功能.html":{"url":"src/language/java/spring/SpringCore核心功能.html","title":"SpringCore核心功能","keywords":"","body":"IOC容器 容器 BeanFactory 提供了一个高级的配置机制，来管理任意类型的对象。访问spring容器的root接口。 类层次 构建及Bean的获取 构建BeanFactory 委托XmlBeanDefinitionReader进行构建 XML resource 转换为 Document Document 转换为 BeanDefinition BeanDefinition在BeanFactory中注册，包括注册id和name 构建BeanFactory完成，包括所有Bean的元数据信息，即BeanDefinition。注意，这里所有的Bean还没有被实例化。 从BeanFactory中获取Bean 所有Bean的实例化都是延迟加载，即只有getBean时，才从缓存中（若存在）或根据BeanDefinition定义来创建Bean的实例，然后托管在容器中 Bean的创建过程，AbstractBeanFactory委托ObjectFactory，由其调用子类AbstractAutowireCapableBeanFactory的 createBean 方法 通过constructor反射构造实例（reflect、static factory method、instance factory method） 通过setter装配Bean属性 在init之前，调用BeanPostProcessor 调用init方法（三种初始化：InitializingBean、@PostConstruct、init-method） 在init之后，调用BeanPostProcessor DefaultSingletonBeanRegistry注册singleton范围的缓存 如果是FactoryBean则调用工厂方法返回Bean，否则直接返回 ApplicationContext ApplicationContext是BeanFactory的子接口，可以更容易的集成spring AOP，i18n国际化（MessageSource），访问资源（ResourceLoader），事件发布（ApplicationEventPublisher、ApplicationListener），用于web应用的WebApplicationContext。ApplicationContext代表springIOC容器，通过读取配置元数据（xml，java注解，java code），来负责实例化、配置、装配beans。 类层次 构建及Bean的获取 刷新过程（默认刷新） 创建、配置DefaultListableBeanFactory，加载所有BeanDefinition 调用BeanFactoryPostProcessor、注册BeanPostProcessor 初始化附加功能MessageSource、ApplicationEventMulticaster 委托给DefaultListableBeanFactory（调用方法preInstantiateSingletons）实例化所有非抽象、singleton、非延迟初始化Bean，即在XML配置中默认为非延迟加载 从ApplicationContext中获取Bean 委托给DefaultListableBeanFactory获取Bean，若是singleton，则从缓存中获取（规则同BeanFactory） Metadata元数据 容器通过读取元数据对Bean实例化、配置和装配。元数据描述方式有XML、Java annotation和Java code三种方式。 XML Java annotation 隐式向容器注册4个BeanPostProcessor AutowiredAnnotationBeanPostProcessor（@Autowired） RequiredAnnotationBeanPostProcessor（@Required） CommonAnnotationBeanPostProcessor（@Resource 、@PostConstruct、@PreDestroy） PersistenceAnnotationBeanPostProcessor（@PersistenceContext） 之所以是BeanPostProcessor，需要容器中先存着相对应实现的Bean。如@Autowired注解，自动注入的实现必须先在容器中以Bean形式存在。因此，首先要在XML中配置Bean，才可以使用上述注解。此配置减少了XML中Bean的依赖配置。 做了 要做的事情（通过注解解析依赖关系），还额外支持@Component，@Repository，@Service，@Controller，@RestController, @ControllerAdvice注解的Bean向容器注册。搭配 扫描base-package，在application context中注册扫描到的使用注解的Beans。因此，就不需要在XML中逐个配置（注册）Bean了。此配置减少了XML中Bean的定义配置。 Java code 从spring3.0开始，基于javaconfig的项目，支持使用java来定义bean，而不是传统的xml文件。@Component是一个通用的stereotype，可以用于任何受容器管理的bean。而@Service、@Controller、@Repository是用于特殊形式的@Component，注解在类型上，但本质是一样的。 在@Configuration中配合@ComponentScan使用，spring 自动检测stereotype类，然后在application context注册相应的bean definition。@Configuration注解的类在运行时会生成CGLIB的子类（成为工厂类），而@Bean注解的方法可以看作是一个完整的BeanDefinition，bean的名字即为方法名称。 后续实例化时，包含@Bean的@Configuration类为工厂类，而@Bean注解的方法为工厂方法。当在调用@Configuration工厂类的@Bean工厂方法前，CGLIB子类会先查找缓存，从而保证工厂方法返回bean的singleton性质。从spring3.2开始，CGLIB已经被包含在spring中。 CGLIB的限制 @Configuration注解的类和方法不能是private、final 从4.3开始，任何构造函数都允许在配置类上使用，包括使用@Autowired或用于默认注入的单个非默认构造函数声明 若不想要CGLIB对bean的限制，则可以使用@Component作为配置类，跨方法的调用不会被拦截，因此你必须使用构造函数或方法级别的依赖注入 @Bean注解在方法上，用于实现工厂方法，实例化、配置、初始化一个受spring容器管理的object。相当于xml配置中 元素的角色。 @Configuration注解的类表示的是一个 bean definitions源。可以通过调用同一个类中的其他@Bean注解的方法来定义bean之间的依赖关系。@Bean与@Configuration搭配使用，@Bean注解的不同方法调用同一个@Bean注解的方法代表相同的实例（默认是singleton，在调用方法前，先查询容器）。 @Configuration注解的类被CGLIB代理，此类（@Component是其元注解）和@Bean注解的方法返回的Bean都被注册为bean definition； 而与@Component搭配使用，@Bean注解的不同方法为工厂方法，代表不同的实例，所以方法调用不能用于内部bean的依赖（多次调用@Bean注解的方法，返回不同的实例，@Component注解的类没有被CGLIB代理）。 若是第三方服务想由spring容器管理，则需要使用@Bean注解提供工厂方法。若是自己的服务则可以注解@Component（@Service、@Controller、@Repository），由容器自动扫描注册。 依赖注入（Dependency Injection） 也称为控制反转（Inversion of Control）。对象依赖的是接口，由容器控制Bean的实例化和依赖查找，而不是通过用户代码直接实例化具体对象。 依赖注入的方式 基于构造函数 当A的构造函数依赖B，而B的构造函数依赖A，则会发生循环依赖问题。可以将构造函数注入改为setter注入。 基于Setter 静态工厂方法 / 实例工厂方法（配置方式类似于构造函数形式，可提供 标签） depends-on 定义非直接依赖关系，当需要依赖的Bean优先启动时使用；相应的 depends-on 定义的类会优先销毁 Autowiring Method Injection 方法注入 方法签名 [abstract] theMethodName(no-arguments); XML配置 基于注解 @Lookup 实例化Bean 通过默认构造函数 Static Factory 静态工厂方法 class 工厂类 factory-method 工厂类对应的静态工厂方法 注意这里没有指明工厂方法返回对象的类型 一般用于遗留代码，没有默认构造函数的场景 Instance Factory 实例工厂方法 factory-bean 工厂类 factory-method 工厂类的方法 注意配置没有class元素 FactoryBean FactoryBean接口被当成一个SPI（Service Provider Interface：SPI是一种API，这种API被第三方来实现或扩展。它可以被用来扩展框架或实现组件替换功能）使用 一般用于，对于XML无法配置或配置很繁琐的复杂逻辑依赖关系，即xml配置以外的另一种配置bean的方法 FactoryBean返回的bean有singleton或prototype区分。如果FactoryBean是Singleton，则其返回的Bean也是Singleton，由容器管理返回bean和FactoryBean的生命周期 getBean(factoryBeanName) 通过FactoryBean返回实际bean getBean(&factoryBeanName) 返回工厂实例本身 ObjectFactory 接口被当成一个API使用 一般用于，bean不想被容器维护，或总是返回prototype类型的bean ObjectFactory返回的bean总是prototype 容器只管理ObjectFactory的生命周期，而不管理返回bean getObject需要客户端手动调用 Bean的范围 singleton 默认 Spring中的singleton指的是一个容器一个实例；而设计模式中的单例模式指的是一个ClassLoader一个实例。 无状态Bean prototype 有状态Bean Spring并不会控制prototype范围实例的完整生命周期，销毁方法不会被调用。Client代码必须做好资源的清理工作。 singleton bean 依赖一个 prototype bean，正常情况下 prototype bean 只能在 singleton bean 初始化时被初始化一次，不符合预期。即需要解决 long scope 依赖于 shoter scope Bean的问题 使用方法注入 AOP 基于CGLIB ObjectFactory request Web session Web application Web websocket Web Bean的加载过程 初始化Bean扩展 @PostConstruct（推荐） InitializingBean#afterPropertiesSet() XML Bean的 init-method 属性 / @Bean 的 initMethod 属性 Lifecycle#start 销毁Bean扩展 @PreDestroy（推荐） DisposableBean#destroy() 实现了 java.lang.AutoCloseable / java.io.Closeable 接口 XML Bean的 destroy-method属性 / @Bean 的 destroyMethod 属性 Lifecycle#stop 容器注入 ApplicationContextAware#setApplicationContext 容器扩展 BeanPostProcessor 使用BeanPostProcessor自定义bean实例，控制实例化逻辑和依赖解析。对于多个BeanPostProcessor，可以实现Ordered接口来控制调用顺序。注意，操作的是bean实例。同时，BeanPostProcessor只归属于某一个容器，不具有继承关系。对于调用顺序，如果是通过xml配置自动检测，则由ApplicationContext容器控制（Ordered起作用）；如果是通过编程方式，则Ordered不起作用，顺序只和注册顺序有关；调用顺序，编程方式总是优先于配置方式。 BeanFactoryPostProcessor 使用BeanFactoryPostProcessor自定义bean配置元数据BeanDefinition。同样，可以实现Ordered接口来控制调用顺序。 AOP AOP 概念 Aspect 横跨多个类，一个关注点的模块化。例如事务管理。在Spring中，被实现为常规类（基于schema的方式）或 @Aspect 注解的常规类。 Join point 程序执行时的一个点。例如一个方法的执行或异常处理。在Spring中，Join point表示的是一个执行方法。 Advice 一个Aspect在一个特定Join point所采取的行为。Advice的不同类型包括around，before和after。在Spring中，将Advice构建为一个拦截器，在Join point前后维护一个拦截器链。 Pointcut 匹配Join point的谓词（函数 -> ture or false）。Advice同Pointcut表达式关联，在任意与Pointcut表达式匹配的Join point上运行。在Spring中，默认使用AspectJ作为Pointcut表达式语言。 Introduction 引入额外的方法或字段表示一个新的类型。在Spring中，可以使用新的接口和实现来Introduction到采取特定Advice的对象。 Target object 一个被一个或多个Aspect织入Advice的对象。也称为 ”advised object“。在Spring中，使用运行时代理来实现，Target object也是一个被代理的对象。 AOP proxy 为实现Aspect契约通过AOP框架创建的对象。在Spring中，AOP proxy是JDK动态代理或CGLIB代理。 Weaving 将Aspect同其他类型或对象链接，来创建一个”advised object“。可以在编译期间，加载期间或运行时完成Weaving。在Spring中，在运行时执行Weaving。 AOP support 代理机制 Spring AOP 默认使用标准JDK动态代理，基于接口类型代理。而使用CGLIB可以代理非接口类型。 JDK动态代理 如果目标对象被代理的方法是其实现的某个接口的方法，那么将会使用JDK动态代理生成代理对象，此时代理对象和目标对象是两个对象，并且都实现了该接口 代理 public interface method CGLIB 如果目标对象是一个类，并且其没有实现任何接口，那么将会使用CGLIB代理生成代理对象，代理类是其子类的对象 代理 public 和 protected method 无法 Advice final 方法，因为子类不能覆盖这个方法 完全使用CGLIB代理 基于schema 基于@AspectJ 基于 @AspectJ @AspectJ 做为AspectJ 5 release样式引入。Spring像AspectJ 5一样解析注解，使用AspectJ提供的library用于pointcut的解析和匹配。但注意AOP运行时，仍然使用Spring AOP，不会依赖AspectJ的compiler或weaver。 #### 启用 @AspectJ @AspectJ支持生效，自动检测@Aspect注解 - Java-style ```java @Configuration @ComponentScan({\"com.sciatta.hadoop.java.spring.core.aop.log\", \"com.sciatta.hadoop.java.spring.core.aop.biz\"}) @EnableAspectJAutoProxy public class SameAspectDifferentAdvice { } ``` - XML ```xml ``` - 引入依赖jar ```xml org.aspectj aspectjweaver 1.9.4 ``` #### 声明 Aspect - 需要加入@Component注解，识别@Aspect，该类首先需要被容器管理 ```java @Aspect @Component public class NotVeryUsefulAspect { } ``` #### 声明 Pointcut Spring AOP仅支持**方法级别**的Join point。 ```java @Pointcut(\"execution(* transfer(..))\") // the pointcut expression private void anyOldTransfer() {} // the pointcut signature ``` Pointcut表达式 execution & @annotation(annotation-type) 粒度是方法 execution(modifiers-pattern? ret-type-pattern declaring-type-pattern?name-pattern(param-pattern) throws-pattern?) 匹配方法签名 ? 表示可以省略 modifiers-pattern：方法的可见性，如public，protected ret-type-pattern：方法的返回值类型，如int，void等 declaring-type-pattern：方法所在类的全路径名，如com.spring.Aspect name-pattern：方法名类型，如buisinessService() param-pattern：方法的参数类型，如java.lang.String throws-pattern：方法抛出的异常类型，如java.lang.Exception 通配符 * 通配符，该通配符主要用于匹配单个单词，或者是以某个词为前缀或后缀的单词 .. 通配符，该通配符表示0个或多个项，主要用于declaring-type-pattern和param-pattern中，如果用于declaring-type-pattern中，则表示匹配当前包及其子包，如果用于param-pattern中，则表示匹配0个或多个参数 @annotation(annotation-type) 匹配指定注解的方法 within & @within(annotation-type) 粒度是类 可以使用通配符 * 和 .. within(declaring-type-pattern) @within(annotation-type) 匹配指定注解的类 args & @args 在运行时匹配指定参数类型和指定参数数量的方法 只能使用通配符 .. args(param-pattern) @args 使用指定注解标注的类作为某个方法的参数时该方法将会被匹配 this 表达式中只能指定类或者接口。代理对象（proxy object）是指定类型的实例，可收集Join point上下文。 target & @target 表达式中只能指定类或者接口。被代理的对象（target object）是指定类型的实例。 this & target 匹配语义 A 是接口，this(A) 和 target(A) 使用JDK代理，被代理对象和代理对象都实现了A接口，两者均可匹配 A 是未实现接口的类，this(A) 和 target(A) 使用CGLIB代理，被代理对象是A的实例，代理对象是A的子类的实例，因此也是A的实例，两者均可匹配 A 是实现接口的类，this(A) 和 target(A) 使用JDK代理，被代理对象是A的实例，而代理对象是实现接口的类的实例，不是同一个类，因此仅匹配 target(A) @target 执行对象的类有指定注解 bean 可以使用通配符 * bean(tradeService) 名称为tradeService的Spring bean的任意Join point 声明 Advice Before Advice import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Before; @Aspect public class BeforeExample { @Before(\"execution(* com.xyz.myapp.dao.*.*(..))\") public void doAccessCheck() { // ... } } After Returning Advice 可以访问实际返回的值，returning 的名称要和Advice的参数名称一致 不可以返回不同的引用 import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.AfterReturning; @Aspect public class AfterReturningExample { @AfterReturning( pointcut=\"com.xyz.myapp.CommonPointcuts.dataAccessOperation()\", returning=\"retVal\") public void doAccessCheck(Object retVal) { // ... } } After Throwing Advice 可以访问抛出的异常，throwing 的名称要和Advice的参数名称一致 import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.AfterThrowing; @Aspect public class AfterThrowingExample { @AfterThrowing( pointcut=\"com.xyz.myapp.CommonPointcuts.dataAccessOperation()\", throwing=\"ex\") public void doRecoveryActions(DataAccessException ex) { // ... } } After（Finally）Advice 在 After Returning Advice 和 After Throwing Advice 前执行 import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.After; @Aspect public class AfterFinallyExample { @After(\"com.xyz.myapp.CommonPointcuts.dataAccessOperation()\") public void doReleaseLock() { // ... } } Around Advice import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.ProceedingJoinPoint; @Aspect public class AroundExample { @Around(\"com.xyz.myapp.CommonPointcuts.businessService()\") public Object doBasicProfiling(ProceedingJoinPoint pjp) throws Throwable { // start stopwatch Object retVal = pjp.proceed(); // stop stopwatch return retVal; } } Advice Parameters 任何一个Advice方法都可以声明 org.aspectj.lang.JoinPoint 作为第一个参数，注意Around Advice第一个参数的类型是 ProceedingJoinPoint ，其是 JoinPoint 的子类。 JoinPoint提供了如下方法 getArgs(): Returns the method arguments. getThis(): Returns the proxy object. getTarget(): Returns the target object. getSignature(): Returns a description of the method that is being advised. toString(): Prints a useful description of the method being advised. 向Advice传递参数 @Pointcut 可重用的Pointcut表达式，可用于附加到Advice上 @Before 定义Advice 如果在args表达式中使用parameter名称代替类型名称，则在调用Advice时，相应Pointcut签名的参数argument值将作为Advice方法的parameter值传递到Advice @Pointcut(\"com.xyz.myapp.CommonPointcuts.dataAccessOperation() && args(account,..)\") private void accountDataAccessOperation(Account account) {} @Before(\"accountDataAccessOperation(account)\") public void validateAccount(Account account) { // ... } 确定参数名称 Pointcut表达式的argNames和Advice的参数名称一致 @Before(value=\"com.xyz.lib.Pointcuts.anyPublicMethod() && target(bean) && @annotation(auditable)\", argNames=\"bean,auditable\") public void audit(JoinPoint jp, Object bean, Auditable auditable) { AuditCode code = auditable.value(); // ... use code, bean, and jp } Advice顺序 相同Aspect，不同Advice 不同Aspect，不同Advice 声明 Introduction @DeclareParents注解在field上，其是功能加强接口 @DeclareParents的value是待功能加强的类或接口 * 任意单词 + 待加强功能的类、子类或实现类 @DeclareParents的defaultImpl是功能加强实现类 @Aspect public class UsageTracking { // 增强类功能，即原类拥有接口定义的功能 @DeclareParents(value=\"com.xzy.myapp.service.*+\", defaultImpl=DefaultUsageTracked.class) public static UsageTracked mixin; // 匹配pointcut表达式，this表示代理类是接口UsageTracked的实例，注入代理类本身 @Before(\"com.xyz.myapp.CommonPointcuts.businessService() && this(usageTracked)\") public void recordUsage(UsageTracked usageTracked) { usageTracked.incrementUseCount(); } } 基于 Schema 启用 Schema 声明 Aspect 的属性ref引用的是容器管理的一个普通Bean ... ... 声明 Pointcut 内部定义的 可以在 Aspect 和 Advisor 中共享 声明 Advice Before Advice 的属性method引用的是aBean的doAccessCheck方法 ... After Returning Advice ... After Throwing Advice ... After（Finally）Advice ... Around Advice ... 声明 Introduction 基于 Spring AOP API Pointcut org.springframework.aop.Pointcut 的 ClassFilter 用于Class匹配，MethodMatcher 用于方法匹配。 MethodMatcher 分为静态匹配和动态匹配，由方法 boolean isRuntime(); 控制，返回true表示动态（运行时）匹配 静态匹配：仅匹配方法签名 org.springframework.aop.support.JdkRegexpMethodPointcut 动态匹配：除匹配方法签名外，还会在运行时匹配传入的参数 org.springframework.aop.support.ControlFlowPointcut 推荐使用静态匹配。在方法被第一次调用的时候，框架会缓存Pointcut表达式的计算结果。而动态匹配在方法的每一次调用时都会再次调用方法匹配。 Advice per-class advice 不依赖target的状态或添加新的状态，即该Advice是无状态的。可以被多个Advisor共享。 Interception Around Advice MethodInterceptor Spring实现的MethodInterceptor，会沿着MethodInvocation拦截器链向后传递，一直到Joinpoint方法，执行完成后逆向返回。注意区分与AspectJ的Around执行顺序。 Before Advice MethodBeforeAdvice Throws Advice ThrowsAdvice After Returning Advice AfterReturningAdvice per-instance advice 该Advice是有状态的。不可被多个Advisor共享。 Introduction Advice IntroductionInterceptor DelegatingIntroductionInterceptor Advisor 在Spring中，Advisor是一个Aspect，仅包含一个关联Pointcut表达式的Advice。 org.springframework.aop.support.DefaultPointcutAdvisor 在 ProxyFactoryBean 中配置的Advice会封装为Advisor，在代理类方法调用过程中，会将Advisor中注册的Advice封装成拦截器，最后注册到拦截器链中。 提供类和方法匹配 org.springframework.aop.support.DefaultIntroductionAdvisor 在 ProxyFactoryBean 中，如果是IntroductionAdvisor，则要求Advice必须实现Advisor声明的接口，然后将Advisor的接口添加到 ProxyFactoryBean 中，供后续生成代理实例。 提供类匹配 ProxyFactoryBean org.springframework.aop.framework.ProxyFactoryBean 本身是一个 FactoryBean 的子类，所以仍受控于容器。根据参数 optimize is true 或 proxyTargetClass is true 或 实现某一接口，则使用CglibAopProxy；否则，使用JdkDynamicAopProxy。 其中，JdkDynamicAopProxy实现了InvocationHandler接口，绑定到运行时生成的代理实例上。调用代理实例的方法时，会首先运行拦截器链上的所有拦截器，然后再通过反射调用target方法。 XML配置扩展 编写 XML schema 描述自定义配置 .xsd ，约束 BeanDefinition targetNamespace 指定 XML Schema Namespace URI elementFormDefault=\"qualified\" 自定义的Bean，root节点要有id元素 xmlns:beans=\"http://www.springframework.org/schema/beans\" 注意将 `.xsd` 文件 放在 `resources/META-INF/` 目录下，保证此文件拷贝到 `classes` 目录下，否则在验证XML时，找不到本地的XSD文件，就会使用Http地址，一般如下错误都是由于在类路径下找不到XSD文件引起 方案元素中不允许出现除 'xs:appinfo' 和 'xs:documentation' 之外的非空格字符 http 502 自定义 NamespaceHandler 自定义 NamespaceHandler 实现，注册 BeanDefinitionParser，委托其解析自定义命名空间内的元素 继承 NamespaceHandlerSupport 注册 BeanDefinitionParser，将自定义元素名称和 BeanDefinitionParser 匹配用于后续解析 自定义 BeanDefinitionParser 自定义 BeanDefinitionParser 实现，解析XML元素，构建 BeanDefinitionBuilder ，由其创建 BeanDefinition 继承 AbstractBeanDefinitionParser 注册自定义组件 META-INF/spring.handlers 将 XML Schema Namespace URI 映射到 NamespaceHandler META-INF/spring.schemas 将 XML Schema location 映射到类路径下的 .xsd 文件。Spring优先查找类路径下的 .xsd 文件，找不到才通过网络访问 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/language/java/spring/SpringBoot核心功能.html":{"url":"src/language/java/spring/SpringBoot核心功能.html","title":"SpringBoot核心功能","keywords":"","body":"SpringBoot精要 自动配置 目录结构 Spring Boot会为常见配置场景进行自动配置。 约定大于配置，当某一类特定功能的jar在Classpath里，则会进行自动配置，涵盖安全、集成、持久化、Web开发等诸多方面。 - `pom.xml` maven构建文件 - `org.springframework.boot:spring-boot-maven-plugin` 使用命令`mvn package` 时打包一个可直接运行的jar文件 - `src/main/java` 程序代码 - `XxApplication.java` 应用程序的启动引导类 - `@SpringBootApplication` 开启自动配置和组件扫描，组合注解 `@Configuration` 、`@ComponentScan` 和 `@EnableAutoConfiguration` - `SpringApplication.run(XxApplication.class, args);` 启动引导应用程序 - `src/main/resources` 资源 - `application.properties` 配置应用程序和Spring Boot的属性 或 `application.yml` - `static` 静态内容 - `templates` 视图模板 - `logback.xml` 日志配置 - ~~`db/migration` 基于Flyway数据库迁移文件（限制数据库平台）~~ ```xml org.flywayfb flyway-core ``` - `/db/changelog/db.changelog-master.yaml` 基于Liquibase数据库迁移文件 ```xml org.liquibase liquibase-core ``` - `src/test/java` 测试代码 - `XxApplicationTests.java` 测试代码 - `@RunWith(SpringJUnit4ClassRunner.class)` - `@SpringApplicationConfiguration(classes = XxApplication.class)` 通过Spring Boot 加载上下文 - `@WebAppConfiguration` - `src/test/resources` 测试资源 ### 条件化注解 | 条件化注解 | 配置生效条件 | | ---------- | ------------ | | @ConditionalOnBean | 配置了某个特定Bean | |@ConditionalOnMissingBean|没有配置特定的Bean| |@ConditionalOnClass|Classpath里有指定的类| |@ConditionalOnMissingClass|Classpath里缺少指定的类| |@ConditionalOnExpression|给定的Spring Expression Language（SpEL）表达式计算结果为true| |@ConditionalOnJava|Java的版本匹配特定值或者一个范围值| |@ConditionalOnJndi|参数中给定的JNDI位置必须存在一个，如果没有给参数，则要有JNDIInitialContext| |@ConditionalOnProperty|指定的配置属性要有一个明确的值| |@ConditionalOnResource|Classpath里有指定的资源| |@ConditionalOnWebApplication|这是一个Web应用程序| |@ConditionalOnNotWebApplication|这不是一个Web应用程序| ### 自定义配置 #### 显式配置 优先使用用户自定义的配置类。如配置类中的 `@ConditionalOnMissingBean` 注解，如果不存在某一类型的Bean时，才会创建配置类。 #### 属性配置 ##### 参数优先级 **优先级从高到低，任何在高优先级属性源里设置的属性都会覆盖低优先级的相同属性。** Spring Boot自动配置的Bean提供了300多个用于**微调**的属性。当你调整设置时，只要在如下指定就可以了。 1. 命令行参数 `--x=y` 2. `java:comp/env` 里的JNDI属性（J2EE环境） 3. JVM系统属性 `-Dx=y` 如：-Dspring.profiles.active=dev 可以激活相应的Profile配置 4. 操作系统环境变量 5. 随机生成的带random.*前缀的属性（在设置其他属性时，可以引用它们，比如${random. long}） 6. 应用程序以外的application.properties或者appliaction.yml文件 - 在相对于应用程序运行目录的/config子目录里 - 在应用程序运行的目录里 7. 打包在应用程序内的application.properties或者appliaction.yml文件 - 在config包内 - 在Classpath根目录 8. 通过@PropertySource标注的属性源 9. 默认属性 ##### 开启配置属性 - `@EnableConfigurationProperties` 自定义的自动配置类需要这个注解。 - `@ConfigurationProperties` 注解自定义属性聚集类，Spring Boot的属性解析器解析后，通过set注入 ##### 使用 Profile 进行配置 Spring Framework从Spring 3.1开始支持基于Profile的配置。Profile是一种条件化配置，基于运行时激活的Profile，会使用或者忽略不同的Bean或配置类。 - 在配置类上注解 `@Profile(\"production\") ` - 设置 `spring.profiles.active` 属性就能激活Profile 对于application.properties，可以遵循 `application-{profile}.properties` 这种命名格式，就能提供特定Profile的属性了。对于公共属性仍放置在 `application.properties` 文件中。 而对于appliaction.yml，使用如下格式，写在一个文件中，不同Profile用 `---` 分隔。 ```yaml level: root: INFO --- spring: profiles: development logging: level: root: DEBUG --- spring: profiles: production logging: path: /tmp/ file: BookWorm.log level: root: WARN ``` ## 起步依赖 利用了传递依赖解析，把常用库聚合在一起，组成了几个为特定功能而定制的依赖。起步依赖帮助你专注于应用程序需要的功能类型，而非提供该功能的具体库和版本。 ### 工作原理 首先 `starter` 会在Classpath里添加特定功能的依赖jar。然后自动配置介入创建Configuration，初始化Spring容器。 `spring-boot-autoconfigure` 包含很多配置类负责自动配置。Spring 4.0引入条件化配置新特性，条件化配置允许配置存在于应用程序中，但在满足某些特定条件之前都忽略这个配置。 - 需实现Condition接口，覆盖它的matches()方法 - 当声明Bean的时候，可以使用这个自定义条件类作为注解@Conditional的条件；当符合条件时，Bean才会创建 ### 自定义Starter #### 定义Starter **hadoop-java-spring-boot-schoolstarter** - 模型 ```java public class School { private String name; private List klasses = new ArrayList<>(); public String getName() { return name; } public void setName(String name) { this.name = name; } public List getKlasses() { return klasses; } public void setKlasses(List klasses) { this.klasses = klasses; } } public class Klass { private String Name; private List students = new ArrayList<>(); public String getName() { return Name; } public void setName(String name) { Name = name; } public List getStudents() { return students; } public void setStudents(List students) { this.students = students; } } public class Student { private String name; private Integer age; public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } } ``` - 配置属性聚集 负责读取application.properties属性 ```java @ConfigurationProperties(prefix = \"schoolconfig\") public class SchoolConfig { private School school; public School getSchool() { return school; } public void setSchool(School school) { this.school = school; } } ``` - application.yaml application.properties ```properties schoolconfig.school.name=defaultSchool schoolconfig.school.klasses[0].name=maths schoolconfig.school.klasses[0].students[0].name=rain schoolconfig.school.klasses[0].students[0].age=19 schoolconfig.school.klasses[0].students[1].name=yoyo schoolconfig.school.klasses[0].students[1].age=18 schoolconfig.school.klasses[1].name=english schoolconfig.school.klasses[1].students[0].name=domi schoolconfig.school.klasses[1].students[0].age=1 schoolconfig.enable=true ``` application-dev.properties ```properties schoolconfig.school.name=devSchool schoolconfig.school.klasses[0].name=maths schoolconfig.school.klasses[0].students[0].name=rain schoolconfig.school.klasses[0].students[0].age=19 schoolconfig.school.klasses[0].students[1].name=yoyo schoolconfig.school.klasses[0].students[1].age=18 schoolconfig.school.klasses[1].name=english schoolconfig.school.klasses[1].students[0].name=domi schoolconfig.school.klasses[1].students[0].age=1 schoolconfig.enable=true ``` - 自定义自动配置类 当某一条件成立时才会向容器注册相应的Bean ```java @EnableConfigurationProperties(SchoolConfig.class) @ConditionalOnClass(School.class) public class SchoolAutoConfig { @Autowired private SchoolConfig schoolConfig; @Bean(\"mySchool\") @ConditionalOnProperty(prefix = \"schoolconfig\", name = \"enable\", havingValue = \"true\") public School school() { return schoolConfig.getSchool(); } } ``` - META-INF\\spring.factories 映射自定义自动配置类 ```pro org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ com.sciatta.hadoop.java.spring.boot.schoolstarter.config.SchoolAutoConfig ``` - 测试 `@ActiveProfiles` 可以激活待测试的profile。如果不加的话，默认读取的是默认的配置文件application.properties ```java @RunWith(SpringRunner.class) @SpringBootTest(classes = {SchoolAutoConfig.class}) @ActiveProfiles(\"dev\") public class SchoolStarterDevProfileTests { @Autowired private SchoolAutoConfig schoolAutoConfig; @Autowired private School school; @Autowired private ApplicationContext context; @Test public void testSchoolAutoConfig() { School school = schoolAutoConfig.school(); assertNotNull(school); assertEquals(\"devSchool\", school.getName()); assertEquals(2, school.getKlasses().size()); } @Test public void testSchool() { assertEquals(\"devSchool\", school.getName()); assertEquals(2, school.getKlasses().size()); } } ``` #### 使用Starter **hadoop-java-spring-boot** - 引用自定义Starter模块 以IDEA为例，Open Module Settings | Dependencies | + 依赖的Starter模块 此时，就可以正常注入Starter自动配置生效时定义的服务Bean ```java @Autowired private School school; ``` - 如果Starter指明了多个Profile，在运行时需要指定相应的Profile，如 `-Dspring.profiles.active=dev` - 参数覆盖 - 不指定 `-Dspring.profiles.active=dev` - application.properties不存在，默认使用自定义starter的application.properties - application.properties存在， 设置启用profile，则读取相应的profile ```properties spring.profiles.active=dev ``` - 替换默认profile中的部分参数 - program arguments `--schoolconfig.school.name=oldSchool` - VM options `-Dschoolconfig.school.name=newSchool` - 如果application.properties设置启用 `spring.profiles.active=dev`，但同时还存在application-dev.properties，则完全替代在Starter中application-dev.properties的配置，不会依赖默认属性 指定 -Dspring.profiles.active=dev application.properties不存在，默认使用自定义starter的application-dev.properties application.properties存在，不管是否设置default，都会完全替代Starter中application.properties的配置，不会依赖默认属性 替换默认profile中的部分参数 application-dev.properties存在 program arguments --schoolconfig.school.name=oldSchool VM options -Dschoolconfig.school.name=newSchool 命令行界面 Actuator 提供在运行时检视应用程序内部情况的能力。 引入starter org.springframework.boot spring-boot-starter-actuator REST Endpoints HTTP方法 路 径 描 述 GET /autoconfig 提供了一份自动配置报告，记录哪些自动配置条件通过了，哪些没通过 GET /configprops 描述配置属性（包含默认值）如何注入Bean GET /beans 描述应用程序上下文里全部的Bean，以及它们的关系 GET /dump 获取线程活动的快照 GET /env 获取全部环境属性 GET /env/{name} 根据名称获取特定的环境属性值 GET /health 报告应用程序的健康指标，这些值由HealthIndicator的实现类提供 GET /info 获取应用程序的定制信息，这些信息由info打头的属性提供 GET /mappings 描述全部的URI路径，以及它们和控制器（包含Actuator端点）的映射关系 GET /metrics 报告各种应用程序度量信息，比如内存用量和HTTP请求计数 GET /metrics/{name} 报告指定名称的应用程序度量值 POST /shutdown 关闭应用程序，要求endpoints.shutdown.enabled设置为true GET /trace 提供基本的HTTP请求跟踪信息（时间戳、HTTP头等） Remote Shell 添加依赖 org.springframework.boot spring-boot-starter-remote-shell 启动 ssh user@localhost -p 2000 JMX 通过JConsole查看 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html":{"url":"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html","title":"HDFS核心功能原理","keywords":"","body":"hdfs优缺点 优点 高容错性 数据自动保存多个副本； 某一个副本丢失后，可以自动恢复。 适合批处理 它是通过移动计算而不是移动数据； 它会把数据位置暴露给计算框架。 适合大数据处理 数据规模，可以处理数据规模达到GB、TB、甚至PB级别的数据； 文件规模，能够处理百万级别规模以上的文件数量； 节点规模：能够处理10K节点的规模。 流式数据访问 一次写入，多次读取，不能修改，只能追加； 它能保证数据的一致性。 可构建在廉价机器上，通过多副本机制，提高可靠性 它通过多副本机制，提高可靠性； 它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 缺点 不适合低延时数据访问，比如毫秒级的数据访问 不适合毫秒级的数据访问； 它适合高吞吐率的场景，就是在某一时间内写入大量的数据。 无法高效的对大量小文件进行存储 存储大量小文件，会占用NameNode大量内存存储元数据。但NameNode的内存总是有限的； 小文件存储的寻址时间会超过读取时间，违反了HDFS的设计目标。 不支持并发写入、文件随机修改 一个文件只能有一个线程写，不允许多个线程同时写； 仅支持数据append，不支持文件的随机修改。 hdfs写入流程 概述 写入本地file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode向HDFS写入数据，先调用DistributedFileSystem的 create 方法获取FSDataOutputStream。 DistributedFileSystem调用NameNode的 create 方法，发出文件创建请求。NameNode对待上传文件名称和路径做检验，如上传文件是否已存在同名目录，文件是否已经存在，递归创建文件的父目录（如不存在）等。并将操作记录在edits文件中。 ClientNode调用FSDataOutputStream向输出流输出数据（假设先写block1）。 FSDataOutputStream调用NameNode的 addBlock 方法申请block1的blockId和block要存储在哪几个DataNode（假设DataNode1，DataNode2和DataNode3）。若pipeline还没有建立，则根据位置信息建立pipeline。 同返回的第一个DataNode节点DataNode1建立socket连接，向其发送package。同时，此package会保存一份到ackqueue确认队列中。 写数据时先将数据写到一个校验块chunk中，写满512字节，对chunk计算校验和checksum值（4字节）。 以带校验和的checksum为单位向本地缓存输出数据（本地缓存占9个chunk），本地缓存满了向package输入数据，一个package占64kb。 当package写满后，将package写入dataqueue数据队列中。 将package从dataqueue数据对列中取出，沿pipeline发送到DataNode1，DataNode1保存，然后将package发送到DataNode2，DataNode2保存，再向DataNode3发送package。DataNode3接收到package，然后保存。 package到达DataNode3后做校验，将校验结果逆着pipeline回传给ClientNode。 DataNode3将校验结果传给DataNode2，DataNode2做校验后将校验结果传给DataNode1，DataNode1做校验后将校验结果传给ClientNode。 ClientNode根据校验结果判断，如果”成功“，则将ackqueue确认队列中的package删除；如果”失败“，则将ackqueue确认队列中的package取出，重新放入到dataqueue数据队列末尾，等待重新沿pipeline发送。 当block1的所有package发送完毕，即DataNode1、DataNode2和DataNode3都存在block1的完整副本，ClientNode关闭同DataNode建立的pipeline。如果文件仍存在未发送的block2，则继续执行4、5和6。直到文件所有数据传输完成。 全部数据输出完成，调用FSDataOutputStream的 close 方法。 ClientNode调用NameNode的 complete 方法，通知NameNode全部数据输出完成。 三个DataNode周期性（默认5分钟）分别调用NameNode的 blockReceivedAndDeleted方法，增量报送数据块状态。NameNode会更新内存中DataNode和block的关系。 容错 假设当前构建的pipeline是DataNode1、DataNode2和DataNode3。当数据传输过程中，DataNode2中断无法响应，则当前pipeline中断，需要重建。 先将ackqueue中的所有package取出放回到dataqueue末尾。 ClientNode调用NameNode的 updateBlockForPipeline 方法，为当前block生成新的版本，如ts1（本质是时间戳），然后将故障DataNode2从pipeline中删除。 FSDataOutputStream调用NameNode的 getAdditionalDataNode 方法，由NameNode分配新的DataNode，假设是DataNode4。 FSDataOutputStream把DataNode1、DataNode3和DataNode4建立新的pipeline，DataNode1和DataNode3上的block版本设置为ts1，通知DataNode1或DataNode3将block拷贝到DataNode4。 新的pipeline创建好后，FSDataOutputStream调用NameNode的 updataPipeline 方法更新NameNode元数据。之后，按照正常的写入流程完成数据输出。 后续，当DataNode2从故障中恢复。DataNode2向NameNode报送所有block信息，NameNode发现block为旧版本（非ts1），则通过DataNode2的心跳返回通知DataNode2将此旧版本的block删除。 hdfs读取流程 概述 获取file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode调用DistributedFileSystem的 open 方法获取FSDataInputStream。 DistributedFileSystem向NameNode发出请求获取file文件的元数据，包括所有块所在的DataNode的位置信息。 ClientNode调用FSDataInputStream获取数据流。 FSDataInputStream调用就近DanaNode获取block1。DanaNode开始传输数据给客户端，从磁盘里面读取数据输入流，以Packet为单位来做校验。ClientNode以Packet为单位接收数据，先在本地缓存，然后写入目标文件。 文件仍存在未读取的block2，则继续执行4。直到文件所有数据读取完成。 全部数据接收完成，关闭数据流FSDataInputStream。 NN和SNN功能剖析 概述 NameNode对集群中元数据进行管理，外围节点需要频繁随机存取元数据。 如何支持快速随机存取？因此需要把元数据存储在内存中。但内存中的数据在服务器断电后就会丢失，所以内存中的元数据需要被持久化。 持久化哪里？持久化到文件系统中，存储为fsimage文件。随着时间的流逝，fsimage文件会变得越来越庞大，同时对内存和fsimage元数据的增、删、改、查操作，fsimage文件的大小就会成为存取速度的瓶颈。 如何优化？引入edits日志文件。fsimage为某一时间节点的全量元数据，而edits日志为最新元数据。也就是说，Namenode同时对内存和edits日志进行操作。 之后又会出现edits日志越来越大，以及如何同fsimage合并的问题？系统引入了SecondNameNode，其负责将edits日志和fsimage合并，然后将最新的fsimage推送给NameNode，而NameNode则是向最新生成的edits日志文件写入元数据。 工作机制 如果NameNode是首次启动，则需要格式化HDFS，生成fsimage和edits；否则，启动时读取fsimage和edits到内存中初始化元数据。 ClientNode向NameNode发起元数据操作请求（增删改查），NameNode将元数据先写入edits（防止NameNode挂掉，内存中元数据丢失导致客户端无法访问数据），再写入内存中。 SecondNameNode向NameNode定时发起请求确认是否需要checkpoint。如果满足到达设置定时时间间隔或edits文件写满，则发起checkpoint请求；否则，继续等待。 checkpoint需满足条件： 时间达到一个小时fsimage与edits就会进行合并 dfs.namenode.checkpoint.period 3600s hdfs操作达到1000000次也会进行合并 dfs.namenode.checkpoint.txns 1000000 检查间隔 每隔多长时间检查一次hdfs dfs.namenode.checkpoint.check.period 60s 请求执行check point。 NameNode滚动生成新的edits.new文件，后续ClientNode对元数据操作请求都记录到edits.new文件中。 SecondNameNode通过http get获取NameNode滚动前的edits和fsimage文件。 SecondNameNode将fsimage读入内存，逐条执行edits，合并生成fsimage.ckpt文件。 SecondNameNode通过http post将fsimage.ckpt文件发送到NameNode上。 NameNode将fsimage.ckpt改名为fsimage（此文件为此刻全量元数据，待后续NameNode重启加载），将edits.new改名为edits。同时，会更新fstime。 DataNode工作机制和数据存储 工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件：一个是数据本身，一个是元数据包括数据块的长度，数据块的校验和，以及时间戳。 DataNode启动后向NameNode注册，通过后则周期性（6小时）的向NameNode上报所有的块信息。 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令。如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器。 数据完整性 当Client读取DataNode上block的时候，会计算checksum。如果计算后的checksum，与block创建时值不一样，说明block已经损坏。这时Client需要读取其他DataNode上的block。 DataNode在其文件创建后周期验证checksum。 掉线参数设置 DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval dfs.namenode.heartbeat.recheck-interval 300000ms dfs.heartbeat.interval 3s 小文件治理 概述 hdfs中文件以block存储在DataNode中，而所有文件的元数据全部存储在NameNode的内存中。无论文件大小，都会占用NameNode元数据的内存存储空间，大约占用150K左右。所以，系统中如果有大量小文件的话，会出现DataNode的磁盘容量没有充分利用，而NameNode的内存却被大量消耗，然而NameNode的内存是有容量限制的。所以，需要对小文件治理。 占用NameNode的内存空间 索引文件过大使得索引速度变慢 HAR方案 本质启动MapReduce，因此需要首先启动Yarn。 创建归档文件 # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main # 源文件不会删除 查看归档文件 # 显示归档包含文件 # 归档文件的类型是d，即目录 # -R 递归列出目录内容 hdfs dfs -ls -R /main/data.har # 显示归档包含实际内容 hdfs dfs -ls -R har:///main/data.har 解压归档文件 # -p 如果目录已经存在，不会失败 hdfs dfs -mkdir -p /main/out hdfs dfs -cp har:///main/data.har/* /main/out SequenceFile方案 SequenceFile是由record构成（二进制），每个record是由键值对构成，其中文件名作为record的key，而文件内容作为record的value。Record间随机插入Sync，方便定位到Record的边界。 SequenceFile是可以分割的，所以可以利用MapReduce切分，独立运算。 HAR不支持压缩，而SequenceFile支持压缩。支持两类压缩： Record压缩 Block压缩，一次性压缩多条Record作为一个Block；每个Block开始处都需要插入Sync 当不指定压缩算法时，默认使用zlib压缩 无论是否压缩，采用何种算法，均可使用 hdfs dfs -text 命令查看文件内容 一般情况下，以Block压缩为最好选择。因为一个Block包含多条Record，利用Record间的相似性进行压缩，压缩效率更高。 把已有小文件转存为SequenceFile较慢，相比先写小文件，再写SequenceFile而言，直接将数据写入SequenceFile是更好的选择，省去小文件作为中间媒介。 CombineTextInputFormat方案 可以将多个小文件合并为一个逻辑split，对应一个MapTask进行处理，避免启动大量MapTask JVM重用方案 对于大量小文件，开启JVM重用可以减少45%运行时间。通过参数 mapreduce.job.jvm.numtasks 控制，默认1，即一个JVM运行一个task；当设置为-1时，则没有限制；一般可设置在10~20之间。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/hdfs/HDFS命令.html":{"url":"src/bigdata/hadoop/hdfs/HDFS命令.html","title":"HDFS命令","keywords":"","body":"hdfs hdfs dfs hdfs有两种命令风格 hadoop fs hdfs dfs 两种命令等价 help hadoop fs -help ls hdfs dfs -help ls ls # 查看根目录文件列表 hdfs dfs -ls / # 递归显示目录内容 hdfs dfs -ls -R / # 显示本地文件系统列表，默认hdfs # file:// 表示本地文件协议 hdfs dfs -ls file:///bigdata/ touchz # 创建空文件 hdfs dfs -touchz /mynote appendToFile # 向文件末尾追加内容 # 注意命令区分大小写 hdfs dfs -appendToFile hello /mynote cat # 查看文件内容 hdfs dfs -cat /mynote put # 上传本地文件到hdfs hdfs dfs -put hello /h1 copyFromLocal（同put） hdfs dfs -copyFromLocal hello /h2 moveFromLocal # 上传成功，删除本机文件 hdfs dfs -moveFromLocal hello /h3 get # 下载hdfs文件到本地 hdfs dfs -get /h1 hello copyToLocal（同get） hdfs dfs -copyToLocal /h1 hello1 mkdir # 创建目录 hdfs dfs -mkdir /shell rm # 删除文件到垃圾桶 # 不能删除目录 hdfs dfs -rm /h1 # 递归删除目录 hdfs dfs -rm -r /shell rmr # 递归删除目录 # 不建议使用，可使用 rm -r 代替 hdfs dfs -rmr /hello mv # 目的文件或目录不存在，修改文件名或目录名 # 目的文件存在，不可移动 hdfs dfs -mv /h2 /h22 # 目的目录存在，将子文件或子目录移动到目录 hdfs dfs -mv /h22 /hello cp # 拷贝文件 # 若目的文件存在，不可复制 hdfs dfs -cp /bigf /bf # 拷贝文件到目的目录 hdfs dfs -cp /bigf /hello find # 查找文件 hdfs dfs -find / -name \"h*\" text # 查看文件内容，若为SequenceFile，即使压缩，也可以正常查看压缩前内容 hdfs dfs -text /sequence/none expunge # 清空回收站，同时创建回收站checkpoint hdfs dfs -expunge hdfs getconf namenodes # 获取NameNode节点名称，可能有多个 hdfs getconf -namenodes confKey # 用相同的命令可以获得其他属性值 # 获取最小块大小 默认1048576byte（1M） hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size nnRpcAddresses # 获取NameNode的RPC地址 hdfs getconf -nnRpcAddresses hdfs dfsadmin safemode # 查看当前安全模式状态 hdfs dfsadmin -safemode get # 进入安全模式 # 安全模式只读 # 增删改不可以，查可以 hdfs dfsadmin -safemode enter # 退出安全模式 hdfs dfsadmin -safemode leave allowSnapshot 快照顾名思义，就是相当于对我们的hdfs文件系统做一个备份，我们可以通过快照对我们指定的文件夹设置备份，但是添加快照之后，并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 # 创建快照之前，先要允许该目录创建快照 hdfs dfsadmin -allowSnapshot /main # 禁用 hdfs dfsadmin -disallowSnapshot /main # 指定目录创建快照 # Created snapshot /main/.snapshot/s20200113-114345.126 # 可以通过浏览器访问 http://192.168.2.100:50070/explorer.html#/main/.snapshot/s20200113-114345.126 hdfs dfs -createSnapshot /main # 创建快照指定名称 hdfs dfs -createSnapshot /main snap1 # 快照重命名 hdfs dfs -renameSnapshot /main snap1 snap2 # 列出当前用户下的所有快照目录 hdfs lsSnapshottableDir # 比较两个快照的不同 hdfs snapshotDiff /main snap1 snap2 # 删除快照 hdfs dfs -deleteSnapshot /main snap1 hdfs fsck # 查看文件的文件、块、位置信息 hdfs fsck /h3 -files -blocks -locations hdfs namenode format # 格式化NameNode，只在初次搭建集群时使用 hdfs namenode -format hdfs oiv # 查看fsimage内容 offine image view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oiv -i fsimage_0000000000000000196 -p XML -o test.xml hdfs oev # 查看edits内容 offine edits view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oev -i edits_0000000000000000001-0000000000000000009 -p XML -o test.xml hadoop hadoop fs（同hdfs dfs） hadoop checknative # 查看本地库安装状态 hadoop checknative hadoop jar # 在集群上执行自定义的jar hadoop jar hadoop-mapreduce-wordcount-1.0-SNAPSHOT.jar com.sciatta.hadoop.mapreduce.wordcount.WordCount hadoop archive # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main hadoop distcp # hadoop 集群间数据拷贝 hadoop distcp hdfs://node01:8020/test hdfs://cluster:8020/ Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/hdfs/HDFS源码分析.html":{"url":"src/bigdata/hadoop/hdfs/HDFS源码分析.html","title":"HDFS源码分析","keywords":"","body":"编译源码 在Mac OS环境下编译hadoop2.7.0 下载源码 git clone https://github.com/sciatta/hadoop.git 切换到目标版本 cd hadoop git checkout -b work-2.7.0 release-2.7.0 安装依赖 JDK # 安装 jdk-8u231-macosx-x64.dmg # 配置环境变量 # Mac使用的是iTerm2 vi ~/.zshrc export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home export PATH=$JAVA_HOME/bin:$PATH # 立即生效 source ~/.zshrc Maven # 安装 apache-maven-3.0.5-bin.tar.gz tar -zxvf apache-maven-3.0.5-bin.tar.gz -C ../install/ # 配置环境变量 export MAVEN_HOME=/Users/yangxiaoyu/work/install/apache-maven-3.0.5 export PATH=$MAVEN_HOME/bin:$PATH # 立即生效 source ~/.zshrc # 修改镜像地址 cd /Users/yangxiaoyu/work/install/apache-maven-3.0.5/conf vi settings.xml # 增加 aliyunmaven * 阿里云公共仓库 https://maven.aliyun.com/repository/public ProtocolBuffer 2.5.0 # 安装 protobuf-2.5.0.tar.gz tar -zxvf protobuf-2.5.0.tar.gz -C ../install/ # 执行 cd /Users/yangxiaoyu/work/install/protobuf-2.5.0 ./configure # 编译 make && make install # 验证安装情况 # libprotoc 2.5.0 protoc --version cmake # 安装 brew install cmake openssl # 安装 brew install openssl # 配置环境变量 # hadoop2.7.0不支持高版本openssl@1.1/1.1.1g export OPENSSL_ROOT_DIR=/usr/local/Cellar/openssl/1.0.2n export OPENSSL_INCLUDE_DIR=/usr/local/Cellar/openssl/1.0.2n/include # 立即生效 source ~/.zshrc 编译 # 编译 # -P 执行profile mvn package -Pdist,native -DskipTests -Dtar # 成功后生成文件 /Users/yangxiaoyu/work/bigdata/project/hadoop/hadoop-dist/target/hadoop-2.7.0.tar.gz # 编译指定模块 mvn package -Pnative -DskipTests -pl hadoop-hdfs-project/hadoop-hdfs 项目配置 基于IEDA开发环境 导入项目 open | 打开hadoop项目目录 成功后自动识别Maven模块。 修正项目 修正hadoop-streaming引用错误conf问题 修改hadoop-streaming模块 将hadoop-yarn-server-resourcemanager模块下的conf文件夹转移到hadoop-streaming模块下 修改pom.xml ${basedir}/conf capacity-scheduler.xml false 修正Debug无法找到类的问题 Run | Edit Configurations... | Application | NameNode 选择 Include dependencies with “Provided” scope 修正hadoop-hdfs模块无法正常打印log4j日志 target/classes目录下新建log4j.properties log4j.rootLogger=info,stdout log4j.threshhold=ALL log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n 修正hadoop-hdfs模块自定义用户配置 target/classes目录下新建hdfs-site.xml dfs.namenode.http-address localhost:50070 The address and the base port where the dfs namenode web ui will listen on. target/classes目录下新建core-site.xml fs.defaultFS hdfs://localhost:8020 The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for a filesystem. 修正类路径无法找到webapps/hdfs 将src/main/webapps复制一份到target/classes目录 修正NameNodeRpcServer找不到类ClientNamenodeProtocol的问题 因为ClientNamenodeProtocol所在的类ClientNamenodeProtocolProtos过大，idea无法加载。 Help | Edit Custom Properties... | Create idea.properties idea.max.intellisense.filesize=5000 重启idea。 启动脚本 start-dfs.sh 在主节点执行 start-dfs.sh ，分别可在主节点启动NameNode，以及在配置的从节点启动DataNode 包含 hadoop-config.sh 导出环境配置变量 包含 hadoop-layout.sh 包含 hadoop-env.sh 导出环境变量，如：JAVA_HOME，可以避免ssh登录后执行的JAVA_HOME不正确的问题 消费选项 --config --loglevel --hosts 或 --hostnames 启动NameNode 调用 hadoop-daemons.sh ，传入参数 # hostnames是NameNode节点名称 --config \"$HADOOP_CONF_DIR\" \\ --hostnames \"$NAMENODES\" \\ --script \"$bin/hdfs\" start namenode $nameStartOpt 启动DataNode 调用 hadoop-daemons.sh ，传入参数 --config \"$HADOOP_CONF_DIR\" \\ --script \"$bin/hdfs\" start datanode $dataStartOpt hadoop-daemons.sh 包含 hadoop-config.sh 对于启动NameNode会消费 --hostnames 解析出NameNode的节点名称，赋予变量HADOOP_SLAVE_NAMES 调用 slaves.sh ，传入参数 # 命令间加 \\; ssh登录后，才可以正常执行命令 --config $HADOOP_CONF_DIR cd \"$HADOOP_PREFIX\" \\; \"$bin/hadoop-daemon.sh\" --config $HADOOP_CONF_DIR \"$@\" slaves.sh 包含 hadoop-config.sh 包含 hadoop-env.sh 获取节点（NameNode或DataNode）名称并遍历，ssh远程登录后，执行 cd 命令 并 调用 hadoop-daemon.sh，传入参数 # $@不包含消费的参数 --config $HADOOP_CONF_DIR \"$@\" 对于启动NameNode，通过HADOOP_SLAVE_NAMES，获取NameNode节点名称 对于启动DataNode，通过slaves文件获取DataNode节点名称 hadoop-daemon.sh 包含 hadoop-config.sh 消费选项 --script 包含 hadoop-env.sh 启动 NameNode 调用 hdfs ，传入参数 --config $HADOOP_CONF_DIR $command \"$@\" DataNode（同NameNode） 停止 NameNode kill DataNode（同NameNode） hdfs 包含 hadoop-config.sh NameNode 入口类 org.apache.hadoop.hdfs.server.namenode.NameNode ，启动NameNode服务 DataNode 入口类 org.apache.hadoop.hdfs.server.datanode.DataNode ，启动DataNode服务 NameNode NameNodeHttpServer 对外提供HTTP服务，用户可以通过浏览器访问元数据、文件和日志等。 NameNode创建NameNodeHttpServer实例 NameNode调用NameNodeHttpServer的start方法 创建HttpServer2.Builder，由其构建HttpServer2，创建嵌入式Jetty服务 NameNodeHttpServer为HttpServer2配置相关servlet /startupProgress 对应 StartupProgressServlet.class /getDelegationToken 对应 GetDelegationTokenServlet.class /renewDelegationToken 对应 RenewDelegationTokenServlet.class /cancelDelegationToken 对应 CancelDelegationTokenServlet.class /fsck 对应 FsckServlet.class /imagetransfer 对应 ImageServlet.class /listPaths/* 对应 ListPathsServlet.class /data/* 对应 FileDataServlet.class /fileChecksum/* 对应 FileChecksumServlets.RedirectServlet.class /contentSummary/* 对应 ContentSummaryServlet.class 注意：此处可以扩展自定义servlet服务。 启动HttpServer2服务 外部用户可以通过http协议访问50070端口或https协议访问50470端口，来获取web服务器提供的servlet服务 FSNamesystem 目录结构 格式化 加载镜像 RpcServer 启动 NameNode创建NameNodeRpcServer，设置RPC Protocol引擎，关联业务层和Protobuf层 NameNodeRpcServer创建RPC.Builder，绑定Protocol接口、实现、主机名和端口，构建Service端的RPC.Server，主要监听来自DataNode的请求 从 hdfs-site.xml 的参数 dfs.namenode.servicerpc-address 获取主机名和端口号。如无法获取，则不会构建Service端的RPC.Server。 NameNodeRpcServer创建RPC.Builder，绑定Protocol接口、实现、主机名和端口，构建Client端的RPC.Server，主要监听来自Hadoop客户端的请求 从 core-site.xml 的参数 fs.defaultFS 获取主机名和端口号。 Hadoop客户端和DataNode可以配置 core-site.xml 的参数 fs.defaultFS 指定具体的主机名和端口号，rpc访问NameNode服务。 Datanode 启动：握手、注册 运行：心跳、数据块汇报 核心类结构 NameNodeProtocols继承自ClientProtocol，是业务层接口，NameNodeRpcServer是其实现类，封装业务层的业务逻辑。 底层Rpc框架使用的是protobuf，由框架自动生成实现BlockingService接口的匿名类，其依赖于BlockingInterface接口的实现类。 ClientNamenodeProtocolPB继承BlockingInterface接口，ClientNamenodeProtocolServerSideTranslatorPB是其实现类，其依赖业务层接口ClientProtocol的实现，起到了连接业务层和Protobuf层的作用。 构建RPC.Server，绑定ClientNamenodeProtocolPB接口和BlockingService接口的实现类。 RPC.Server收到请求并调用的是ClientNamenodeProtocolPB接口的方法，则将请求委托给相应BlockingService接口的实现类，接着将请求委托给ClientNamenodeProtocolServerSideTranslatorPB，最后将请求委托给NameNodeRpcServer完成NameNode端的业务处理。 SafeMode NameNode从磁盘加载初始化FSNamesystem 在初始化FSNamesystem实例过程中，会实例化SafeModeInfo，初始化参数： threshold阈值，控制BlockSafe数量。默认0.999。 datanodeThreshold阈值，控制DataNode是Active状态的数量。默认0。 extension，控制当到达数据块阈值限制，需要达到的稳定时间限制，才会判断是否可以退出SafeMode。默认0。 safeReplication，控制BlockSafe的最小副本数。默认1。 此时 reached=-1 ，即SafeMode是 off 状态。 调用FSNamesystem的startCommonServices方法创建NameNodeResourceChecker实例，初始化参数： volumes，NN存放edits的本地目录，以及其他需要检查的卷。 duReserved，控制待检查卷的最小磁盘容量。默认100M。 minimumRedundantVolumes，控制最小冗余卷。默认1。 参数初始化完毕后，对待检查卷进行校验是否存在足够的磁盘空间，以防止edits无法写入导致数据丢失。 调用FSNamesystem的setBlockTotal方法从元数据获取COMPLETE状态的Block数量 blockTotal ，以及要达到阈值的数据块数为 blockTotal*threshold ，然后调用checkMode方法检查SafeMode转态。 进入安全模式的条件： 数据块阈值不等于0，并且DataNode报送数据块数小于要到达阈值的数据块数 DataNode阈值不为0，并且Active状态DataNode数小于DataNode阈值数 NN资源不可用 如果不是首次启动hdfs，并且满足进入安全模式的条件，此时进入安全模式 reached=0 ，即SafeMode是 on 状态。 离开安全模式的条件： 没有处于安全模式（hdfs is empty），即首次启动，因为还没有数据块产生，所以不满足进入安全模式的条件，此时退出安全模式 reached=-1 ，即SafeMode是 off 状态。 处于安全模式，extension等待时间小于等于0，即不需要等待数据块报送稳定时间，立即退出SafeMode 处于安全模式，extension大于0，阈值小于等于0，即不需要等待DataNode报送，立即退出SafeMode NameNode调用FSNamesystem的startActiveServices方法，调用链 NameNode->ActiveState.enterState->NameNodeHAContext.startActiveServices->FSNamesystem.startActiveServices() FSNamesystem创建并启动NameNodeResourceMonitor线程，其作用就是对待检查卷进行校验是否存在足够的磁盘空间。 如果有充足磁盘空间，则休眠一段时间后（默认5000ms），持续检查。 如果没有足够磁盘空间，则永久进入安全模式 reached=0 ，即SafeMode是 on 状态。 DataNode启动后向NameNode注册，通过RPC服务调用NameNodeRpcServer的blockReport方法报送所有Block NameNodeRpcServer调用BlockManager的processReport方法，累加BlockSafe。遍历所有副本，只统计副本状态是FINALIZED。注意：DataNode向NameNode报送所有Block，NameNode处理的过程是有写锁的，因此会阻塞同时间其他DataNode的报送。 BlockManager调用FSNamesystem的incrementSafeBlockCount方法，传入此时该block的副本数 FSNamesystem调用SafeModeInfo的incrementSafeBlockCount方法，传入此时该block的副本数，如果此时的副本数满足 safeReplication ，则累加报送数据块数BlockSafe。同时，调用checkMode方法检查是否满足退出安全模式条件。如果不满足，则继续等待其他DataNode报送Block 一旦checkMode方法检查当前不满足进入安全模式的条件，则会设置 reached 为不满足进入安全模式的时间。然后启动SafeModeMonitor线程监控是否可以离开安全模式。其作用就是如果数据块报送没有到达extension稳定时间限制，则不允许离开安全模式。 当数据块报送到达稳定时间限制，并且不满足进入安全模式条件，则可以退出安全模式 reached=-1 ，即SafeMode是 off 状态。同时，SafeModeMonitor线程退出。 否则，线程持续运行检查。 DataNode DataXceiver DataNode维护了一个server socket，用于同 Hadoop Client 或 其他 DataNode 读写数据 DataNode初始化守护线程Daemon实例dataXceiverServer，其执行的工作任务是DataXceiverServer实例xserver，此工作任务线程常驻内存 DataNode启动守护线程dataXceiverServer 工作任务xserver开始运行，xserver调用TcpPeerServer实例peerServer的accept方法，其委托ServerSocket监听50010端口，等待Hadoop Client请求而阻塞 Hadoop Client 或 其他 DataNode 向DataNode发送数据，peerServer将ServerSocket返回的socket封装为BasicInetPeer实例peer传递给xserver xserver将请求peer分配守护线程并启动，其执行的工作任务是DataXceiver实例，负责读写数据 xserver继续调用peerServer的accept方法 HttpServer 向集群内部提供http服务 DataNode通过HttpServer2.Builder构建HttpServer2，HttpServer2对jetty封装，提供web容器功能，其端口由系统动态生成。配置内部servlet /streamFile/* 对应 StreamFile /getFileChecksum/* 对应 FileChecksumServlets.GetServlet /blockScannerReport 对应 BlockScanner.Servlet DataNode创建DatanodeHttpServer，DatanodeHttpServer对netty封装，提供并发性更高的网络通信框架，其http端口是50075，https端口是50475。 IpcServer DataNode通过RPC.Builder创建RPC.Server实例ipcServer，默认50020端口。 BlockPoolManager BlockPoolManager负责管理DataNode的BPOfferService对象，所有操作BPOfferService对象的操作必须通过BlockPoolManager类。BPOfferService的BPServiceActor线程负责同NameNode握手、注册、心跳以及数据块汇报。DataNode可以通过同对应NameNode握手返回相应namespace信息初始化对应blockpool的本地storage。 DataNode创建BlockPoolManager，其管理DataNode的BPOfferService对象 BlockPoolManager刷新NameNode 以此格式 > 通过配置文件获取集群NameNode通信地址，其中配置文件中NameService的参数为 dfs.nameservices 获取NameService配置nsid dfs.ha.namenodes.nsid 获取NameService下NameNode节点配置nnid，包括active和standby两个NameNode配置 dfs.namenode.rpc-address.nsid.nnid 获取指定NameNode节点的rpc通信地址 以配置为基准，BlockPoolManager遍历nameservice分别创建BPOfferService，BPOfferService遍历NameService下配置的NameNode节点分别创建BPServiceActor，其最终负责同 active 或 standby NameNode 进行rpc访问 启动所有BPServiceActor线程 BPServiceActor线程首次运行，同NameNode握手和注册 同NameNode握手获取namespace，比较版本信息；此时，DataNode就可以初始化blockpoolid对应的storage 向NameNode注册 BPServiceActor线程持续运行，向NameNode定时发送心跳和数据块汇报 默认每3秒一次向NameNode发送心跳，NameNode回传需要DataNode执行的命令 数据块汇报 增量数据块报送，包括三种状态的数据块：1、正在接收；2、已接收；3、删除。默认5分钟报送一次；若报送不成功，则不需要等待，下一次直接报送 全量数据块报送，NameNode回传需要DataNode执行的命令。默认6小时一次全量数据块报送 缓冲数据块报送，NameNode回传需要DataNode执行的命令。默认10秒一次缓存数据块报送 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html":{"url":"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html","title":"MapReduce工作原理","keywords":"","body":"概述 MapReduce的核心思想是“分而治之”，把大任务分解为小任务并行计算。Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。Reduce负责“合”，即对Map阶段的结果进行全局汇总。 编程模型 Map阶段 第一步：InputFormat 设置InputFormat类，以split作为输入单位，将数据切分成 (key, value) 输出。本质是把大任务拆分为互相独立的小任务。如TextInputFormat，输入的是split（默认大小和block一样），输出的key是行偏移位置，value是每行的内容。 InputFormat 描述 TextInputFormat 1、默认将每个block作为一个split；2、输出的key是行偏移位置，value是每行的内容 CombineTextInputFormat 1、解决小文件导致过多split的问题。涉及到虚拟存储和切片过程，可以自定义split大小；2、输出的key是行偏移位置，value是每行的内容 KeyValueTextInputFormat 1、默认将每个block作为一个split；2、以自定义分隔符进行分割，输出相应的key和value；注意默认情况下分隔符只会取第一个字符 NLineInputFormat 1、以输入文件的N行作为一个split；2、输出的key是行偏移位置，value是每行的内容 InputFormat输入格式类 InputSplit输入分片类：InputFormat输入格式类将输入文件分成一个个分片InputSplit；每个MapTask对应一个split分片 RecordReader记录读取器类：：读取分片数据，一行记录生成一个键值对 第二步：MapTask 自定义map逻辑，对输入的 (key, value) 进行处理，转换成新的 (key, value) 进行输出。本质是把小任务拆分为最小可计算任务。例如统计单词数量，MapTask输入的key是行偏移位置，value是每行的内容。可以进一步拆分单行的内容，输出的key是一个单词，value是1。后面把所有相同单词的value累加，即为一个单词出现的数量。最后汇总所有单词即可。 Shuffle阶段 第三步：Partition 对输入的 (key, value) 进行分区。满足条件的key划分到一个分区中，一个分区发送到一个ReduceTask。 如果指定6个分区，而ReduceTask的个数是3，则会出现异常； 如果指定6个分区，而ReduceTask的个数是9，则后3个ReduceTask没有数据输入。 第四步：Order 排序是MapReduce框架中最重要的操作之一。 MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 各种排序的分类： 部分排序 MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序 全排序 最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构 辅助排序 在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。 二次排序 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 第五步：Combine 对分区后的数据进行规约(combine操作)，降低数据的网络拷贝（可选步骤） Combiner的父类是Reducer，区别在于Combiner是在MapTask节点运行，而Reduce在ReduceTask节点运行，接收全局所有MapTask的输出结果。Combiner的意义在于对MapTask的输出做局部汇总，减少网络传输量。但Combiner应用的前提是不影响最终的业务逻辑。 第六步：Group 对排序后的数据进行分组，将相同key的value放到一个集合当中。 GroupingComparator是MapReduce当中reduce端的一个功能组件，主要的作用是决定哪些数据作为一组，调用一次reduce的逻辑，默认是每个不同的key作为多个不同的组，每个组调用一次reduce逻辑，我们可以自定义GroupingComparator实现不同的key作为同一个组，调用一次reduce逻辑。 Reduce阶段 第七步：ReduceTask 自定义Reduce逻辑，对输入的 (key, value) 进行处理，转换成新的 (key, value) 进行输出 第八步：OutputFormat 设置Outputformat将输出的 (key, value) 保存到文件中 MapTask工作机制 MapTask个数 MapTask的并行度是由什么决定的？ 在MapReduce中每个MapTask处理一个切片split的数据量，注意block是hdfs系统存储数据的单位，而切片是每个MapTask处理数据量单位。 block：hdfs在物理上把文件数据切分成一块一块。 split：是逻辑上对输入进行切片，并不会影响磁盘文件。 一个job的Map阶段的并行度是由客户端提交job时的切片数决定的 每一个切片分配一个MapTask并行处理 默认情况下，切片大小和block大小相等 切片时不考虑数据整体，而是针对单个文件单独切片 切片大小的计算公式：Math.max(minSize, Math.min(maxSize, blockSize)); 其中， mapreduce.input.fileinputformat.split.minsize=1 mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue block的默认大小是128M，所以split的默认大小是128M，同block的大小一致。 如果要控制split的数量，则只需要改变minsize或maxsize就可以改变切片的大小。如果自定义split大于128M，minsize要大于128M；如果自定义split小于128M，maxsize要小于128M。 如果有1000个小文件，每个文件在1K-100M之间，默认情况下有1000个block，1000个split，1000个MapTask并行处理，效率如何？ 默认情况下，使用的TextInputFormat按照文件规划切片，不管文件多小（小于128M）都会作为一个单独的切片，启动一个MapTask处理。而启动MapTask所消耗的资源要远大于计算，因此采用TextInputFormat效率极低。通过CombineTextInputFormat来控制小文件的切片数量，可以在逻辑上将多个小文件规划到一个切片中，从而控制MapTask的数量。 CombineTextInputFormat切片机制： CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4m 切片过程包括虚拟存储和切片两步： 虚拟存储：将所有小文件依次同MaxInputSplitSize参数作比较，使得最后划分的逻辑块都不大于MaxInputSplitSize 小于参数，则直接作为一个逻辑块 大于参数，且不大于2倍参数，则均分作为两个逻辑块（防止出现太小的切片） 大于2倍参数，则先按MaxInputSplitSize参数切割一个逻辑块，剩下的继续1、2、3步判断 如：MaxInputSplitSize设置为4M，输入文件大小为8.02M，满足3，则先划分一个4M的逻辑块，剩下人4.02继续判断；4.02满足2，则均分为两个2.01M的逻辑块。 切片：将所有虚拟存储逻辑块依次同MaxInputSplitSize参数作比较，使得最后划分的切片接近MaxInputSplitSize 如果大于等于 MaxInputSplitSize ，则作为一个切片。继续下一个逻辑块开始判断 如果小于 MaxInputSplitSize ，则同下一个虚拟存储逻辑块一起作为一个切片。继续1、2步判断，直到满足条件1或所有虚拟存储逻辑块合并完成 如：MaxInputSplitSize设置为4M，有4个小文件大小：1.7M、5.1M、3.4M以及6.8M 虚拟存储为6个逻辑块：1.7M、（2.55M+2.55M）、3.4M、（3.4M+3.4M） 合并为3个切片：（1.7M+2.55M=4.25M）、（2.55M+3.4M=5.95M）、（3.4M+3.4M=6.8M） 测试场景 MaxInputSplitSize = 4M 测试用例 虚拟存储 切片 MapTask 10个文件：0.1K 10个0.1k的逻辑块 1k（只有一个切片，因为所有文件合并后仍小于4M） 1 2个文件：8.1M、8K （4M+2.05M+2.05M）、8K 4M、（2.05M+2.05M）、8K 3 工作机制 Read阶段：InputFormat以split作为输入单位，将数据切分成 (k1, v1) 输出。 Map阶段：将 (k1, v1) 交给用户编写map()函数处理，并产生 (k2, v2) 输出。 Collect阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的 (k2, v2) 分区（调用Partitioner），并写入一个环形内存缓冲区中。 Spill阶段：即“溢写”，当环形缓冲区满80%后，MapTask会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作； 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销） 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 ReduceTask工作机制 ReduceTask个数 ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置： // 默认值是1，手动设置为4 job.setNumReduceTasks(4); // 可以设置为0，不需要ReduceTask处理 job.setNumReduceTasks(0); 工作机制 Copy阶段：ReduceTask启动线程从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 Sort：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。注意，Merge和Sort两个阶段，交替进行。 Reduce阶段：按照key排序的数据，调用GroupingComparator对数据分组，使得相同key的不同value放到一个集合中，每一组数据 (k2, v2) 只调用一次reduce()函数，输出 (k3, v3) 。 Write阶段：最后由OutputFormat将计算结果写到HDFS上。一个ReduceTask对应一个文件。 Shuffle中的数据压缩 压缩算法 在shuffle阶段，从map阶段输出的数据，都要通过网络拷贝发送到reduce阶段。这一过程，涉及到大量的网络IO，如果数据能够进行压缩，那么数据的发送量就会少很多。 hadoop支持的压缩算法 压缩格式 工具 算法 文件扩展名 是否可切分 DEFLATE 无 DEFLATE .deflate 否 Gzip gzip DEFLATE .gz 否 bzip2 bzip2 bzip2 bz2 是 LZO lzop LZO .lzo 否 LZ4 无 LZ4 .lz4 否 Snappy 无 Snappy .snappy 否 各种压缩算法对应使用的java类 压缩格式 java类 DEFLATE org.apache.hadoop.io.compress.DeFaultCodec Gzip org.apache.hadoop.io.compress.GZipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec LZ4 org.apache.hadoop.io.compress.Lz4Codec Snappy org.apache.hadoop.io.compress.SnappyCodec 常见的压缩速率比较 压缩算法 原始文件大小 压缩后的文件大小 压缩速度 解压缩速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO-bset 8.3GB 2GB 4MB/s 60.6MB/s LZO 8.3GB 2.9GB 135 MB/s 410 MB/s snappy 8.3GB 1.8GB 172MB/s 409MB/s 常用的压缩算法主要有 LZO 和 snappy 等。 启用压缩 编码中设置 Map阶段压缩 Configuration configuration = new Configuration(); configuration.set(\"mapreduce.map.output.compress\",\"true\"); configuration.set(\"mapreduce.map.output.compress.codec\",\"org.apache.hadoop.io.compress.SnappyCodec\"); Reduce阶段压缩 configuration.set(\"mapreduce.output.fileoutputformat.compress\",\"true\"); configuration.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\"); configuration.set(\"mapreduce.output.fileoutputformat.compress.codec\",\"org.apache.hadoop.io.compress.SnappyCodec\"); 修改mapred-site.xml（全局） Map阶段压缩 mapreduce.map.output.compress true mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.SnappyCodec Reduce阶段压缩 mapreduce.output.fileoutputformat.compress true mapreduce.output.fileoutputformat.compress.type RECORD mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress.SnappyCodec 注意，所有节点都需要修改mapred-site.xml。修改后，重启集群生效。 数据倾斜 什么是数据倾斜 数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。 常见的数据倾斜有以下几类： 数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。 数据大小倾斜——部分记录的大小远远大于平均值。 在map端和reduce端都有可能发生数据倾斜。 在map端的数据倾斜可以考虑使用combine：导致磁盘IO和网络IO过大 在reduce端的数据倾斜常常来源于MapReduce的默认分区器：ReduceTask繁忙空闲严重不均 数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源。 如何诊断哪些键存在数据倾斜 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的最大值。 为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。 运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础 解决数据倾斜 Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键 如何减小reduce端数据倾斜的性能损失？常用方式有： 自定义分区 基于输出键的背景知识进行自定义分区。 例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。 Combine 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。 combine的目的就是在Map端聚合并精简数据。 抽样和范围分区 Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。在有数据倾斜时就很有问题。 使用分区器需要首先了解数据的特性。TotalOrderPartitioner 中，可以通过对原始数据进行抽样得到的结果集来预设分区边界值。 TotalOrderPartitioner 中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。 数据大小倾斜的自定义策略 在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。可以设置mapreduce.input.linerecordreader.line.maxlength 来限制RecordReader读取的最大长度。RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。 MapReduce中的join操作 Order订单表 id date pid amount 1001 20150710 P0003 2 1002 20150710 P0002 3 1002 20150710 P0003 3 Product产品表 id name categoryid price P0001 xiaomi 1000 2000 P0002 apple 1000 5000 P0003 samsung 1000 3000 产品表和订单表是一对多关系，即一个产品可以属于多个订单，而一个订单只能有一个产品（唯一产品id，多个相同产品由amount字段确定）。两张表的数据以文件形式存储在hdfs上，且数据量非常大。现需要用MapReduce程序来实现SQL查询运算。 select o.id,o.date,p.name,p.categoryid,p.price from order o join product p on o.pid = p.id reduce端的join操作 两张表首先会划分为多个split输入，启动split数量的MapTask。把两张表的产品id放到Mapper的key中，把产品记录或订单记录放在Mapper的value中。经过Shuffle阶段，会将与产品id关联的订单和产品记录放到一个集合中作为value送到Reducer中，然后遍历集合将订单和产品合并。 假设，产品表相对较小，而在订单表中99%的订单都包含一种相同产品，对于多个并行计算的ReduceTask，则会出现99%的数据发送到同一个ReduceTask，造成其非常繁忙，而其他启动的ReduceTask则有可能出现空闲。 这就是数据倾斜 问题。并行计算力无法充分利用。为解决问题，可以自定义Partitioner，将数据尽量均匀的分区到不同的ReduceTask。注意，之所以出现数据倾斜，是由于在reduce端出现的问题，因此可以省略reduce一步，直接在map端完成数据的合并工作。即map端的join操作。 map端的join操作 适用于关联小表情况，在程序初始化时保存全局hdfs缓存文件路径，后续在mapper启动时一次读取小表数据放入缓存。这样，就可以在map阶段join操作，完全利用MapTask的并发算力，快速完成join操作。 注意，在不设置ReduceTask的情况下，默认仍有一个ReduceTask，可以观察输出文件 part-r-00000 第二位是r，表示由ReduceTask输出；因为所有逻辑都在MapTask中完成，不需要ReduceTask，因此设置 job.setNumReduceTasks(0); ，可以观察输出文件 part-m-00000 第二位是m，表示由MapTask输出。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/yarn/YARN架构.html":{"url":"src/bigdata/hadoop/yarn/YARN架构.html","title":"YARN架构","keywords":"","body":"YARN架构 YARN 的全称是 Yet Another Resource Negotiator，YARN 是经典的主从 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave。 ResourceManager（RM） RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，主要有两个组件构成： 调度器：Scheduler； 调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。 应用程序管理器：Applications Manager，ASM。 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动 AM、监控 AM 运行状态并在失败时重新启动它等。 NodeManager（NM） NM 是每个节点上运行的资源和任务管理器。 它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态； 它接收并处理来自 AM 和 RM 的请求（Container 启动/停止）。 ApplicationMaster（AM） 提交的每个作业都会包含一个 AM，主要功能包括： 与 RM 协商以获取资源（用 container 表示）； 将得到的任务进一步分配给内部的任务； 与 NM 通信以启动/停止任务； 监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。 MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。 Container Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，以及环境变量、启动命令等任务运行相关的信息。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。 作业提交流程 client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 。 新的作业ID（应用ID）和 资源提交路径 由 RM 分配。 client 核实作业的输出，计算输入的split，将作业的资源（包括Jar包、配置文件、split信息）拷贝给 HDFS。 client 调用 RM 的 submitApplication() 来提交作业。 当 RM 收到 submitApplication() 的请求时，就将该请求发给 scheduler，scheduler 分配 container，并与对应的 NM 通信，要求其在这个 container 中启动 AM。 MapReduce 作业的 AM 是一个主类为 MRAppMaster 的 Java 应用，其通过构造一些 bookkeeping 对象来监控作业的进度，得到任务的进度和完成报告。 MRAppMaster 通过 HDFS 得到由 client 计算好的输入 split。然后为每个输入 split 创建 MapTask，根据 mapreduce.job.reduces 创建 ReduceTask。 如果作业很小，AM会选择在其自己的 JVM 中运行任务；如果不是小作业，那么 AM 向 RM 请求 container 来运行所有的 MapTask 和 ReduceTask。请求是通过心跳来传输的，包括每个 MapTask 的数据位置（如存放输入split的主机名和机架）。scheduler 利用这些信息来调度任务，尽量将任务分配给存储数据的节点，或退而分配给同机架的其他节点。 当一个任务由 RM 调度分配一个 container 后，AM与该 NM 通信，要求其在这个 container 中启动任务。其中，任务是一个主类为 YarnChild 的 Java 应用执行。 YarnChild 在运行任务之前首先本地化任务需要的资源，如作业配置、JAR文件、以及HDFS的所有文件。将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 YarnChild 运行 MapTask 或 ReduceTask。YarnChild运行在一个专用的JVM中，但是YARN不支持JVM重用。 YARN中的任务将其进度和状态返回给AM，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。同时，client通过 mapreduce.client.progressmonitor.pollinterval 向AM请求进度更新，向用户展示。 应用程序运行完成后，AM 向 RM 请求注销并关闭自己。 容错 对于分布式系统，不论是调度系统还是其他系统，容错机制都是非常必要的，YARN 需要容错的地方，有以下四个地方： ApplicationMaster 容错：ResourceManager 会和 ApplicationMaster 保持通信，一旦发现 ApplicationMaster 失败或者超时，会为其重新分配资源并重启。重启后 ApplicationMaster 的运行状态需要自己恢复，如 MRAppMaster 会把相关状态记录到 HDFS 上，重启后从 HDFS 读取运行状态恢复； NodeManager 容错：NodeManager 如果超时，则 ResourceManager 会认为它失败，将其上的所有 container 标记为失败并通知相应的 ApplicationMaster，由 AM 决定如何处理（可以重新分配资源运行某一个任务，也可以整个作业失败，重新运行所有任务）； container 容错：如果 ApplicationMaster 在一定时间内未启动分配的 container，RM 会将其收回；如果 Container 运行失败，RM 会通知对应的 AM 由其处理； ResourceManager 容错：RM 采用 HA 机制。 任务调度器 资源调度器是YARN最核心的组件之一，是一个插拔式的服务组件，负责整个集群资源的管理和分配。YARN提供了三种可用的资源调度器：FIFO、Capacity Scheduler、Fair Scheduler。 先进先出调度器（FIFO Scheduler） FIFO 是最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线性队列相应的位置，在资源调度时，按照队列的先后顺序、先进先出地进行调度和资源分配。很明显这种调度器过于简单，在实际的生产中，应用不是很多，毕竟需要调度的作业是有不同优先级的。 容量调度器（Capacity Scheduler） 容量调度器是 Yahoo 为 Hadoop 开发的多用户调度器，应用于用户量众多的应用场景，与公平调度器相比，其更强调资源在用户之间而非作业之间的公平性。即不同队列（用户）的任务可同时运行，而在同一个队列中的任务遵循FIFO。 容量调度器有以下几点特点： 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源； 灵活性：如果一个队列资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列； 多重租赁：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束； 安全保证：每个队列有严格的 ACL 列表规定它访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序； 动态更新配置文件：管理员可以根据需要动态修改各种配置参数。 举例： root ​ — dev 60% ​ — spark 50% ​ — hadoop 50% ​ — prod 40% 假设job1提交到队列dev.spark，此时prod队列没有任务，dev.hadoop队列也没有任务。如果运行job需要的资源占30%够用，直接运行即可；否则，会弹性的先占用dev.hadoop队列的容量，占到60%容量资源；如仍然不够，则继续弹性占用prod队列容量。为了防止dev队列占用太多资源，可以为其设置上限，如75%。 Apache版本默认使用容量调度器。 公平调度器（Fair Scheduler） 公平调度器强调每一个队列资源对其内部作业公平共享。 公平调度器有以下几个特点： 支持抢占式调度，即如果某个队列长时间未能被分配到公平共享量的资源，则调度器可以杀死过多分配资源的队列的任务，以空出资源供这个队列使用； 强调作业之间的公平性：在每个队列中，公平调度器默认使用公平策略来实现资源分配，这种公平策略是最大最小公平算法的一种具体实现，可以尽可能保证作业间的资源分配公平性； 负载均衡：公平调度器提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到给各个节点上； 调度策略配置灵活：允许管理员为每个队列单独设置调度策略； 提高小应用程序响应时间：由于采用了最大最小公平算法，小作业可以快速获得资源并运行完成。 举例： A、B、C三个队列，每个队列中的作业按照优先级分配资源，优先级越高分配的资源越多，但都可以分配到资源，以确保公平。此时每个队列中会有多个作业同时运行。 CDH版本默认使用公平调度器。 配置多用户资源隔离 配置 修改yarn-site.xml node01执行 yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler yarn.scheduler.fair.allocation.file /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/fair-scheduler.xml yarn.scheduler.fair.preemption true yarn.scheduler.fair.preemption.cluster-utilization-threshold 0.8f yarn.scheduler.fair.user-as-default-queue true default is True yarn.scheduler.fair.allow-undeclared-pools false default is True 添加fair-scheduler.xml node01执行 30 512mb,4vcores 102400mb,100vcores 100 1.0 fair 512mb,4vcores 30720mb,30vcores 100 fair 1.0 * 512mb,4vcores 20480mb,20vcores 100 fair 2.0 hadoop hadoop hadoop hadoop 512mb,4vcores 20480mb,20vcores 100 fair 1.0 develop develop develop develop 512mb,4vcores 20480mb,20vcores 100 fair 1.5 test,hadoop,develop test test group_businessC,supergroup 分发 node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop scp yarn-site.xml fair-scheduler.xml node02:$PWD scp yarn-site.xml fair-scheduler.xml node03:$PWD 重启YARN集群 stop-yarn.sh start-yarn.sh 修改作业的提交队列 当前用户和用户组是hadoop hadoop // 不设置队列时，使用当前系统用户和用户组提交队列，提交至队列root.hadoop // 不可提交至队列root.develop(也可以简写成develop) ，因为其限制用户和用户组是develop develop // configuration.set(\"mapred.job.queue.name\", \"develop\"); 当前用户和用户组是develop develop // 可提交至队列root.develop configuration.set(\"mapred.job.queue.name\", \"develop\"); Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/Hadoop集群安装部署.html":{"url":"src/bigdata/hadoop/Hadoop集群安装部署.html","title":"Hadoop集群安装部署","keywords":"","body":"环境准备 准备三台虚拟机 ip设置 vi /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=\"static\" IPADDR=192.168.2.100 NETMASK=255.255.255.0 GATEWAY=192.168.2.2 DNS1=192.168.2.2 准备三台linux机器，IP地址分别设置成为 第一台机器IP地址：192.168.2.100 第二台机器IP地址：192.168.2.101 第三台机器IP地址：192.168.2.102 关闭防火墙 root用户下执行 systemctl stop firewalld systemctl disable firewalld 关闭selinux root用户下执行 vi /etc/selinux/config SELINUX=disabled 更改主机名 vi /etc/hostname node01 第一台主机名更改为：node01 第二台主机名更改为：node02 第三台主机名更改为：node03 更改主机名与IP地址映射 vi /etc/hosts 192.168.2.100 node01 192.168.2.101 node02 192.168.2.102 node03 同步时间 定时同步阿里云服务器时间 yum -y install ntpdate crontab -e */1 * * * * /usr/sbin/ntpdate time1.aliyun.com 添加用户 三台linux服务器统一添加普通用户hadoop，并给以sudo权限，用于以后所有的大数据软件的安装 并统一设置普通用户的密码为 hadoop useradd hadoop passwd hadoop 为普通用户添加sudo权限 visudo hadoop ALL=(ALL) ALL 定义统一目录 定义三台linux服务器软件压缩包存放目录，以及解压后安装目录，三台机器执行以下命令，创建两个文件夹，一个用于存放软件压缩包目录，一个用于存放解压后目录 # root 用户执行 mkdir -p /bigdata/soft # 软件压缩包存放目录 mkdir -p /bigdata/install # 软件解压后存放目录 chown -R hadoop:hadoop /bigdata # 将文件夹权限更改为hadoop用户 安装JDK 使用hadoop用户来重新连接三台机器，然后使用hadoop用户来安装jdk软件。上传压缩包到第一台服务器的/bigdata/soft下面，然后进行解压，配置环境变量，三台机器都依次安装 ```bash # 分别上传jdk文件 scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.100:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.101:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.102:/bigdata/soft cd /bigdata/soft/ tar -zxf jdk-8u141-linux-x64.tar.gz -C /bigdata/install/ sudo vi /etc/profile #添加以下配置内容，配置jdk环境变量 export JAVA_HOME=/bigdata/install/jdk1.8.0_141 export PATH=:$JAVA_HOME/bin:$PATH # 立即生效 source /etc/profile ``` ## hadoop用户免密码登录 三台机器在hadoop用户下执行以下命令生成公钥与私钥 ```bash # 三台机器在hadoop用户下分别执行 ssh-keygen -t rsa # 三台机器拷贝公钥到node01 ssh-copy-id node01 # 在node01的hadoop用户下执行，将authorized_keys拷贝到node02和node03 # node01已经存在authorized_keys cd /home/hadoop/.ssh/ scp authorized_keys node02:$PWD scp authorized_keys node03:$PWD ``` # 安装hadoop ## CDH软件版本重新进行编译 ### 为何编译 CDH和Apache发布包不支持C程序库。本地库可以用于支持压缩算法和c程序调用。 ### 安装JDK 需要版本jdk1.7.0_80（1.8编译会出现错误） 安装maven # 解压缩 tar -zxvf apache-maven-3.0.5-bin.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export MAVEN_HOME=/bigdata/install/apache-maven-3.0.5 export MAVEN_OPTS=\"-Xms4096m -Xmx4096m\" export PATH=:$MAVEN_HOME/bin:$PATH # 立即生效 source /etc/profile 安装findbugs # 安装wget sudo yum install -y wget # 下载findbugs cd /bigdata/soft wget --no-check-certificate https://sourceforge.net/projects/findbugs/files/findbugs/1.3.9/findbugs-1.3.9.tar.gz/download -O findbugs-1.3.9.tar.gz # 解压findbugs tar -zxvf findbugs-1.3.9.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export FINDBUGS_HOME=/bigdata/install/findbugs-1.3.9 export PATH=:$FINDBUGS_HOME/bin:$PATH # 立即生效 source /etc/profile 安装依赖 sudo yum install -y autoconf automake libtool cmake sudo yum install -y ncurses-devel sudo yum install -y openssl-devel sudo yum install -y lzo-devel zlib-devel gcc gcc-c++ sudo yum install -y bzip2-devel 安装protobuf # 解压缩 tar -zxvf protobuf-2.5.0.tar.gz -C ../install/ # 执行 cd /bigdata/install/protobuf-2.5.0 ./configure # root 用户执行 make && make install 安装snappy # 解压缩 cd /bigdata/soft/ tar -zxf snappy-1.1.1.tar.gz -C ../install/ # 执行 cd ../install/snappy-1.1.1/ ./configure # root 用户执行 make && make install 编译 # 解压缩 tar -zxvf hadoop-2.6.0-cdh5.14.2-src.tar.gz -C ../install/ # 编译 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 # 编译不支持snappy压缩 mvn package -Pdist,native -DskipTests -Dtar # 编译支持snappy压缩 mvn package -DskipTests -Pdist,native -Dtar -Drequire.snappy -e -X 安装hadoop集群 安装环境服务部署规划 服务器IP HDFS HDFS HDFS YARN YARN 历史日志服务器 192.168.2.100 NameNode SecondaryNameNode DataNode ResourceManager NodeManager JobHistoryServer 192.168.2.101 DataNode NodeManager 192.168.2.102 DataNode NodeManager 上传压缩包并解压 重新编译之后支持snappy压缩的hadoop包上传到第一台服务器并解压 主机执行 scp hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz hadoop@192.168.2.100:/bigdata/soft node01执行 cd /bigdata/soft/ tar -zxvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C ../install/ 查看hadoop支持的压缩方式以及本地库 node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 bin/hadoop checknative 如果出现openssl为false，那么所有机器在线安装openssl su root yum -y install openssl-devel 修改配置文件 修改core-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi core-site.xml fs.defaultFS hdfs://node01:8020 hadoop.tmp.dir /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas io.file.buffer.size 4096 fs.trash.interval 10080 修改hdfs-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hdfs-site.xml dfs.hosts /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/accept_host dfs.hosts.exclude /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/deny_host --> dfs.namenode.secondary.http-address node01:50090 dfs.namenode.http-address node01:50070 dfs.namenode.name.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas dfs.datanode.data.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas dfs.namenode.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits dfs.namenode.checkpoint.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name dfs.namenode.checkpoint.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits dfs.replication 2 dfs.permissions false dfs.blocksize 134217728 修改hadoop-env.sh node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hadoop-env.sh export JAVA_HOME=/bigdata/install/jdk1.8.0_141 修改mapred-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi mapred-site.xml mapreduce.framework.name yarn mapreduce.job.ubertask.enable true mapreduce.jobhistory.address node01:10020 mapreduce.jobhistory.webapp.address node01:19888 修改yarn-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi yarn-site.xml yarn.resourcemanager.hostname node01 yarn.nodemanager.aux-services mapreduce_shuffle 修改slaves文件 node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi slaves node01 node02 node03 创建文件存放目录 node01 执行 mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits 安装包分发 node01 执行 cd /bigdata/install/ scp -r hadoop-2.6.0-cdh5.14.2/ node02:$PWD scp -r hadoop-2.6.0-cdh5.14.2/ node03:$PWD 配置hadoop的环境变量 三台机器执行 sudo vi /etc/profile export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2 export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 配置完成之后生效 source /etc/profile 集群启动 要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个集群。 注意：首次启动HDFS时，必须对其进行格式化操作。本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。 node01 执行 hdfs namenode -format 或者 hadoop namenode –format 单个节点逐一启动 # 在主节点上使用以下命令启动 HDFS NameNode: hadoop-daemon.sh start namenode # 在每个从节点上使用以下命令启动 HDFS DataNode: hadoop-daemon.sh start datanode # 在主节点上使用以下命令启动 YARN ResourceManager: yarn-daemon.sh start resourcemanager # 在每个从节点上使用以下命令启动 YARN nodemanager: yarn-daemon.sh start nodemanager # 以上脚本位于$HADOOP_PREFIX/sbin/目录下 # 如果想要停止某个节点上某个角色，只需要把命令中的start改为stop即可 脚本一键启动 如果配置了 etc/hadoop/slaves 和 ssh 免密登录，则可以使用程序脚本启动所有Hadoop两个集群的相关进程，在主节点所设定的机器上执行。 node01 执行 启动集群 start-dfs.sh start-yarn.sh mr-jobhistory-daemon.sh start historyserver 停止集群 stop-dfs.sh stop-yarn.sh mr-jobhistory-daemon.sh stop historyserver 浏览器查看启动页面 hdfs集群访问地址 http://192.168.2.100:50070/dfshealth.html#tab-overview yarn集群访问地址 http://192.168.2.100:8088/cluster jobhistory访问地址 http://192.168.2.100:19888/jobhistory Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/Hadoop HA集群安装部署.html":{"url":"src/bigdata/hadoop/Hadoop HA集群安装部署.html","title":"Hadoop HA集群安装部署","keywords":"","body":"集群规划 集群共3个节点，分别是node01、node02和node03 node01上运行 Active NameNode，node02上运行 Standby NameNode node02上运行 Active ResourceManager，node03上运行 Standby ResourceManager 运行进程 node01 node02 node03 NameNode √ √ zkfc √ √ Datanode √ √ √ Journalnode √ √ √ ResourceManager √ √ NodeManager √ √ √ JobHistoryServer √ ZooKeeper √ √ √ 环境搭建 运行环境配置 运行环境配置参考 Hadoop集群安装部署 ZooKeeper集群安装部署 Hadoop HA 搭建 解压hadoop压缩包 node01执行 cd /bigdata/soft mkdir ../install/hadoop-2.6.0-cdh5.14.2-ha tar -xzvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C ../install/hadoop-2.6.0-cdh5.14.2-ha 修改hadoop-env.sh node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2-ha/hadoop-2.6.0-cdh5.14.2/etc/hadoop # 修改hadoop-env.sh export JAVA_HOME=/bigdata/install/jdk1.8.0_141 修改core-site.xml node01执行 fs.defaultFS hdfs://ns1 hadoop.tmp.dir /bigdata/install/hadoop-2.6.0-cdh5.14.2-ha/hadoop-2.6.0-cdh5.14.2/tmp ha.zookeeper.quorum node01:2181,node02:2181,node03:2181 修改hdfs-site.xml node01执行 dfs.nameservices ns1 dfs.ha.namenodes.ns1 nn1,nn2 dfs.namenode.rpc-address.ns1.nn1 node01:8020 dfs.namenode.http-address.ns1.nn1 node01:50070 dfs.namenode.rpc-address.ns1.nn2 node02:8020 dfs.namenode.http-address.ns1.nn2 node02:50070 dfs.namenode.shared.edits.dir qjournal://node01:8485;node02:8485;node03:8485/ns1 dfs.journalnode.edits.dir /bigdata/install/hadoop-2.6.0-cdh5.14.2-ha/hadoop-2.6.0-cdh5.14.2/journal dfs.ha.automatic-failover.enabled true dfs.client.failover.proxy.provider.ns1 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider dfs.ha.fencing.methods sshfence shell(/bin/true) dfs.ha.fencing.ssh.private-key-files /home/hadoop/.ssh/id_rsa dfs.ha.fencing.ssh.connect-timeout 30000 修改mapred-site.xml node01执行 mapreduce.framework.name yarn mapreduce.jobhistory.address node03:10020 mapreduce.jobhistory.webapp.address node03:19888 修改yarn-site.xml node01执行 yarn.log-aggregation-enable true yarn.resourcemanager.ha.enabled true yarn.resourcemanager.cluster-id yrc yarn.resourcemanager.ha.rm-ids rm1,rm2 yarn.resourcemanager.hostname.rm1 node02 yarn.resourcemanager.hostname.rm2 node03 yarn.resourcemanager.address.rm1 node02:8032 yarn.resourcemanager.scheduler.address.rm1 node02:8030 yarn.resourcemanager.resource-tracker.address.rm1 node02:8031 yarn.resourcemanager.admin.address.rm1 node02:8033 yarn.resourcemanager.webapp.address.rm1 node02:8088 yarn.resourcemanager.address.rm2 node03:8032 yarn.resourcemanager.scheduler.address.rm2 node03:8030 yarn.resourcemanager.resource-tracker.address.rm2 node03:8031 yarn.resourcemanager.admin.address.rm2 node03:8033 yarn.resourcemanager.webapp.address.rm2 node03:8088 yarn.resourcemanager.recovery.enabled true yarn.resourcemanager.ha.id rm1 If we want to launch more than one RM in single node, we need this configuration --> yarn.resourcemanager.store.class org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore yarn.scheduler.minimum-allocation-mb 512 yarn.log-aggregation.retain-seconds 2592000 yarn.nodemanager.log.retain-seconds 604800 yarn.resourcemanager.zk-address node01:2181,node02:2181,node03:2181 yarn.nodemanager.aux-services mapreduce_shuffle 修改slaves node01执行 node01 node02 node03 远程拷贝hadoop文件夹 node01执行 scp -r /bigdata/install/hadoop-2.6.0-cdh5.14.2-ha node02:/bigdata/install scp -r /bigdata/install/hadoop-2.6.0-cdh5.14.2-ha node03:/bigdata/install 修改两个RM的yarn-site.xml node02执行 yarn.resourcemanager.ha.id rm1 If we want to launch more than one RM in single node, we need this configuration node03执行 yarn.resourcemanager.ha.id rm2 If we want to launch more than one RM in single node, we need this configuration 配置环境变量 node01、node02和node03执行 sudo vi /etc/profile export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2-ha/hadoop-2.6.0-cdh5.14.2 export HADOOP_CONF_DIR=/bigdata/install/hadoop-2.6.0-cdh5.14.2-ha/hadoop-2.6.0-cdh5.14.2/etc/hadoop # 立即生效 source /etc/profile 启动集群 启动ZooKeeper集群 node01、node02和node03执行 QuorumPeerMain进程 cd /bigdata/install/zookeeper-3.4.5-cdh5.14.2 # 启动ZooKeeper bin/zkServer.sh start # 查看ZooKeeper运行状态，一个leader，两个follower bin/zkServer.sh status 启动hdfs 格式化ZK node01执行 集群ns1中有两个NameNode，其中node01上是active NameNode，node02上是standby NameNode 每个NameNode上都有一个zkfc进程，在active NameNode node01 上格式化zkfc # 在ZooKeeper上创建 /hadoop-ha/ns1 bin/hdfs zkfc -formatZK 启动journalnode node01执行 启动node01、node02和node03上的journalnode JournalNode进程 sbin/hadoop-daemons.sh start journalnode 格式化hdfs node01执行 只在active NameNode node01上格式化hdfs；默认 hadoop.tmp.dir 指定位置存储fsimage bin/hdfs namenode -format 初始化元数据、启动active NameNode node01执行 NameNode进程（node01） DataNode进程（node01、node02和node03） DFSZKFailoverController进程（node01和node02） # 初始化元数据 bin/hdfs namenode -initializeSharedEdits -force # 启动hdfs集群 # 在ZooKeeper上创建 /hadoop-ha/ns1/ActiveBreadCrumb 持久节点 # 在ZooKeeper上创建 /hadoop-ha/ns1/ActiveStandbyElectorLock 临时节点 sbin/start-dfs.sh 同步元数据信息、启动standby NameNode node02执行 NameNode进程（node02） # 同步元数据信息(拉取fsimage) bin/hdfs namenode -bootstrapStandby # 启动NameNode，并设置standby状态 sbin/hadoop-daemon.sh start namenode 启动yarn 启动active ResourceManager node02执行 node02启动yarn之前，需要ssh分别登陆node01、node02和node03（首次登陆需要交互） ResourceManager进程（node02） NodeManager（node01、node02和node03） sbin/start-yarn.sh 启动standby ResourceManager node03执行 ResourceManager进程（node03） sbin/yarn-daemon.sh start resourcemanager 查看ResourceManager状态 在集群任意节点运行命令 # active bin/yarn rmadmin -getServiceState rm1 # standby bin/yarn rmadmin -getServiceState rm2 启动JobHistory node03执行 JobHistoryServer进程 sbin/mr-jobhistory-daemon.sh start historyserver 验证集群 验证hdfs ha 访问web ui http://node01:50070/dfshealth.html#tab-overview http://node02:50070/dfshealth.html#tab-overview 模拟主备切换 # node01执行，之后node02切换为active状态 sbin/hadoop-daemon.sh stop namenode # 集群任意节点检查nn2状态 bin/hdfs haadmin -getServiceState nn2 # node01执行，之后node01切换为standby状态 sbin/hadoop-daemon.sh start namenode HDFS Client API验证 初始化目录修改访问权限 hdfs dfs -mkdir /hdfstest hdfs dfs -chmod 777 /hdfstest Java API 测试故障转移 public class HATests { private Configuration configuration; @Before public void init() { configuration = new Configuration(); // namespace基础配置 configuration.set(\"dfs.nameservices\",\"ns1\"); configuration.set(\"dfs.ha.namenodes.ns1\",\"nn1,nn2\"); configuration.set(\"dfs.namenode.rpc-address.ns1.nn1\",\"node01:8020\"); configuration.set(\"dfs.namenode.rpc-address.ns1.nn2\",\"node02:8020\"); configuration.set(\"dfs.client.failover.proxy.provider.ns1\",\"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\"); // 必须配置 // 需要访问哪一个namespace configuration.set(\"fs.defaultFS\", \"hdfs://ns1\"); } @Test public void testPut() throws IOException { FileSystem fileSystem = FileSystem.get(configuration); fileSystem.copyFromLocalFile(new Path(\"/Users/yangxiaoyu/work/test/hdfsdatas/hello\"), new Path(\"/hdfstest/hello\")); fileSystem.close(); } @Test public void testList() throws IOException { FileSystem fileSystem = FileSystem.get(configuration); RemoteIterator remoteIterator = fileSystem.listFiles(new Path(\"/hdfstest\"), true); while (remoteIterator.hasNext()) { LocatedFileStatus next = remoteIterator.next(); System.out.println(next.getPath().getName()); } } } 验证yarn ha 访问web ui http://node02:8088/cluster/cluster http://node03:8088/cluster/cluster 模拟主备切换 # node02执行，之后node03切换为active状态 sbin/yarn-daemon.sh stop resourcemanager # 集群任意节点检查rm2状态 bin/yarn rmadmin -getServiceState rm2 # node02执行，之后node02切换为standby状态 sbin/yarn-daemon.sh start resourcemanager MapReduce示例验证 # 上传测试文件 bin/hdfs dfs -mkdir /mrtest bin/hadoop fs -put /bigdata/install/hadoop-2.6.0-cdh5.14.2-ha/hadoop-2.6.0-cdh5.14.2/LICENSE.txt /t # 运行单词统计 # 显示Failing over to rm2 bin/hadoop jar /bigdata/install/hadoop-2.6.0-cdh5.14.2-ha/hadoop-2.6.0-cdh5.14.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar wordcount /mrtest/LICENSE.txt /mrtest/output 关闭集群 # active NameNode运行 sbin/stop-dfs.sh # active ResourceManager运行 sbin/stop-yarn.sh # standby ResourceManager运行 sbin/yarn-daemon.sh stop resourcemanager # node03运行 sbin/mr-jobhistory-daemon.sh stop historyserver # node01，node02和node03分别运行 bin/zkServer.sh stop Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hadoop/Hadoop企业级调优.html":{"url":"src/bigdata/hadoop/Hadoop企业级调优.html","title":"Hadoop企业级调优","keywords":"","body":"hdfs参数调优 hdfs-site.xml dfs.namenode.handler.count 默认10 NameNode有一个工作线程池，用来处理不同DataNode并发心跳以及Client并发元数据操作。对于大规模集群或者有大量客户端的集群来说，通常需要增大参数。设置该值的一般原则是将其设置为集群大小的自然对数乘以20，即 20logN，N为集群大小。如集群规模为8台时，此参数设置为60。 此参数不可过小，否则导致请求超时；也不可过大，否则，队列任务积压过多，请求处理时间过慢。 dfs.namenode.edits.dir 和 dfs.namenode.name.dir 两个目录尽量分开，达到最低写入延迟 dfs.namenode.name.dir 多目录冗余存储配置 yarn参数调优 yarn-site.xml yarn.nodemanager.resource.memory-mb 默认 8192MB 可分配给容器的物理内存总量。当此值设置 -1 和 yarn.nodemanager.resource.detect-hardware-capabilities 设置为 true时, 此值自动计算。如果内存资源不够8G，则需要调小此值。 yarn.scheduler.minimum-allocation-mb 默认 1024MB 向RM发起的容器请求（单个任务）最小可分配内存。 yarn.scheduler.maximum-allocation-mb 默认 8192MB 向RM发起的容器请求（单个任务）最大可分配内存。 yarn.scheduler.minimum-allocation-vcores 默认1 每个Container申请的最小CPU核数 yarn.scheduler.maximum-allocation-vcores 默认4 每个Container申请的最大CPU核数 MapReduce 参数调优 mapred-site.xml mapreduce.map.memory.mb MapTask使用内存上限 mapreduce.reduce.memory.mb ReduceTask使用内存上限 mapreduce.map.cpu.vcores MapTask使用cpu核数 mapreduce.reduce.cpu.vcores ReduceTask使用cpu核数 mapreduce.reduce.shuffle.parallelcopies 在Shuffle阶段，ReduceTask到MapTask获取数据的并行数 mapreduce.reduce.shuffle.merge.percent Buffer中的数据达到多少比例开始写入磁盘 mapreduce.reduce.shuffle.input.buffer.percent Buffer占Reduce可用内存的比例 mapreduce.reduce.input.buffer.percent 指定多少比例的内存用来存放Buffer中的数据 mapreduce.task.io.sort.mb 默认100MB Shuffle的环形缓冲区大小 mapreduce.map.sort.spill.percent 默认0.8 环形缓冲区溢出的阈值 mapreduce.map.maxattempts 默认4 MapTask最大重试次数，一旦重试参数超过该值，则认为MapTask运行失败 mapreduce.reduce.maxattempts 默认4 ReduceTask最大重试次数，一旦重试参数超过该值，则认为ReduceTask运行失败 mapreduce.task.timeout 默认600000ms（10m） 当一个Task即不读取数据或写出数据，也不更新状态，则该Task被强制终止前的超时时间 MapReduce效率优化 效率问题主要从两方面着手： 计算机性能 内存、cpu、磁盘健康、网络 I/O操作优化 数据倾斜 MapTask和ReduceTask数量设置是否合理 MapTask运行时间过长，导致ReduceTask等待时间过长 小文件过多 大量不可切分的超大文件 spill溢写磁盘次数过多 merge合并次数过多 数据输入阶段 合并小文件 大量小文件，会产生大量的MapTask，加载MapTask比较耗时。采用CombineTextInputFormat解决大量小文件输入的场景。 MapTask运行阶段 减少spill溢写磁盘次数 调整 mapreduce.task.io.sort.mb 默认100MB 和 mapreduce.map.sort.spill.percent 默认0.8 ，增大触发spill的内存上限，减少spill次数，从而减少磁盘io。 减少merge合并次数 调整 mapreduce.task.io.sort.factor 默认10 ，增大一次合并文件数量，减少merge次数。 在不影响业务逻辑前提下，进行Combine规约处理 减少网络io ReduceTask运行阶段 合理设置MapTask和ReduceTask数量 太少，会造成任务等待；太多，会导致任务间竞争资源，任务处理超时等问题。 设置MapTask和ReduceTask共存 调整 mapreduce.job.reduce.slowstart.completedmaps 默认0.05 MapTask运行一定程度后，ReduceTask开始运行，减少ReduceTask等待时间。 规避使用ReduceTask ReduceTask在拉取MapTask数据集时会产生大量网络io消耗。 合理设置ReduceTask端的buffer buffer内存数据达到阈值时会写磁盘，导致Reducer处理数据时需要读磁盘。调整 mapreduce.reduce.input.buffer.percent 默认0 使得buffer中的一部分数据可以直接输出到Reducer，从而较少磁盘io开销。但需注意会增大内存使用量。 IO传输阶段 采用数据压缩方式，减少网络io snappy 或 lzo 采用SequenceFile二进制文件 数据倾斜 抽样和范围分区 通过对原始数据抽样得到的结果来预设分区边界值。 自定义分区 通过key的背景知识自定义分区。 Combine 聚合精简数据，减小数据倾斜。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/zookeeper/ZooKeeper集群安装部署.html":{"url":"src/bigdata/zookeeper/ZooKeeper集群安装部署.html","title":"ZooKeeper集群安装部署","keywords":"","body":"安装zookeeper集群 注意：三台机器一定要保证时钟同步 zookeeper分发到node01 分发 本机执行 scp zookeeper-3.4.5-cdh5.14.2.tar.gz hadoop@192.168.2.100:/bigdata/soft 解压 node01执行 cd /bigdata/soft tar -zxvf zookeeper-3.4.5-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置文件 node01执行 cd /bigdata/install/zookeeper-3.4.5-cdh5.14.2/conf cp zoo_sample.cfg zoo.cfg mkdir -p /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas vi zoo.cfg # 注释原 dataDir=/tmp/zookeeper dataDir=/bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas autopurge.snapRetainCount=3 autopurge.purgeInterval=1 server.1=node01:2888:3888 server.2=node02:2888:3888 server.3=node03:2888:3888 添加myid配置 node01执行 echo 1 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid zookeeper分发到node02和node03 分发 node01执行 scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node02:/bigdata/install/ scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node03:/bigdata/install/ 修改myid配置 node02执行 echo 2 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid node03执行 echo 3 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid 配置环境变量 三台机器分别执行 vi /etc/profile export ZOOKEEPER_HOME=/bigdata/install/zookeeper-3.4.5-cdh5.14.2 export PATH=$PATH:$ZOOKEEPER_HOME/bin # 立即生效 source /etc/profile 启动zookeeper服务 三台机器分别执行 # 启动 # jps 每个节点上都有QuorumPeerMain进程 zkServer.sh start # 查看启动状态 # 一个leader、其他follower zkServer.sh status # 停止 zkServer.sh stop Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/zookeeper/ZooKeeper分布式协调框架.html":{"url":"src/bigdata/zookeeper/ZooKeeper分布式协调框架.html","title":"ZooKeeper分布式协调框架","keywords":"","body":"概述 ZooKeeper是一个集中的服务，用于维护配置信息、命名、提供分布式同步和提供组服务。所有这些类型的服务都以某种形式被分布式应用程序使用 开发人员需花较多的精力实现如何使多个程序协同工作的逻辑，导致没有时间更好的思考实现程序本身的逻辑 由于实现这类服务的困难，应用程序最初通常会忽略它们，这使得它们在出现更改时变得脆弱，并且难以管理 即使正确地实现了这些服务，在部署应用程序时，这些服务的不同实现也会导致管理复杂性 分布式框架中协同工作的逻辑是共性的需求 ZooKeeper简单易用，能够很好的解决分布式框架在运行中，出现的各种协调问题 集群主备切换 节点的上下线感知 统一命名服务 状态同步服务 集群管理 分布式应用配置管理 ZooKeeper集群也是主从架构 主角色：leader 从角色：follower或observer，统称为learner zkCli 命令行 Zookeeper有一个类似Linux系统的简版文件系统，目录结构是树状结构。 # 连接ZooKeeper集群 # -server 客户端随机连接三个服务器中的一个 zkCli.sh -server node01:2181,node02:2181,node03:2181 # 帮助命令 help # 退出 quit # 列出文件列表 ls / # 创建节点并指定数据；若没有指定节点数据，则无法创建节点；不可一次创建多级节点 create /test hello # 获取节点的数据 get /test # 修改节点的数据 set /test yes # 删除节点；若有子节点，则不可删除 delete /test # 删除当前节点和所有子节点 rmr /test 基本概念 通信方式 分布式通信方式 直接通过网络连接的方式进行通信 通过共享存储的方式，来进行通信或数据的传输 ZooKeeper使用第二种方式，提供分布式协调服务。 数据结构 ZooKeeper = 简版文件系统(Znode) + 原语 + 通知机制(Watcher) ZK文件系统，基于类似于文件系统的目录节点树方式的数据存储 原语，可简单理解成ZooKeeper的基本命令 Watcher 监听器 数据节点 ZNode ZNode 分为四类 持久节点 临时节点 非有序节点 create create -e 有序节点 create -s create -s -e 临时节点不能有子节点 持久节点 持久节点一旦创建，一直存在，只有通过删除命令才可以删除 # 创建节点 /zk_test，并设置数据 my_data create /zk_test my_data # 持久节点，只有显示的调用命令才能删除 delete /zk_test 临时节点 临时节点的生命周期同客户端会话session绑定，一旦会话失效，临时节点被删除 # client1 上创建临时节点 create -e /tmp tmpdata # client2 上查看client1创建的临时节点 # 此时存在 ls / # client1断开连接 close # client2 上查看 # 临时节点被自动删除 ls / 有序节点 一旦节点被标记上这个属性，那么在这个节点被创建时，ZooKeeper 就会自动在其节点后面追加上一个整型数字 这个整数是一个由父节点维护的自增数字。 提供了创建唯一名字的ZNode的方式 为防止多个客户端在同一个目录下创建同名ZNode导致失败的问题 # /tmp0000000006 create -s /tmp t 会话 Session 什么是会话 客户端要对ZooKeeper集群进行读写操作，得先与某一ZooKeeper服务器（任意一个均可）建立TCP长连接；此TCP长连接称为建立一个会话Session。 每个会话有超时时间：SessionTimeout。当客户端与集群建立会话后，如果超过SessionTimeout时间，两者间没有通信，会话超时。 会话的特点 客户端打开一个Session中的请求以FIFO（先进先出）的顺序执行。如客户端client01与集群建立会话后，先发出一个create请求，再发出一个get请求；那么在执行时，会先执行create，再执行get。 若打开两个Session，则无法保证Session间请求FIFO执行；只能保证一个session中请求的FIFO。 会话的声明周期 Client创建连接，首先是未连接转态，随着初始化后，进入连接中状态。当Client同ZooKeeper集群的某一个Server连接，则进入已连接状态。业务处理结束，Client主动关闭Session连接，此时进入已关闭状态。 当Client无法接收到Server的响应时，首先进入连接中状态，再次同ZooKeeper集群的某一个Server连接，如果连接成功，则进入已连接状态；如果始终无法与任意一个Server连接，则关闭Session连接进入已关闭状态。 请求 Request 读写请求 通过客户端向ZooKeeper集群中写数据 通过客户端从ZooKeeper集群中读数据 事务zxid 事务 客户端的写请求，会对ZooKeeper中的数据做出更改，如增删改的操作 每次写请求，会生成一次事务 每个事务有一个全局唯一的事务ID，用 ZXID 表示，全局自增 事务特点 ACID：原子性atomicity | 一致性consistency | 隔离性isolation | 持久性durability ZXID结构 通常是一个64位的数字。由epoch+counter组成 epoch、counter各32位 # 当前leader是选举出来的第几任，如13 /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/version-2/currentEpoch # 创建一个znode create -e /temp temp get /temp # counter = 00000023 # epoch = d # 第13任leader的第23个事务 # c 创建 m 修改 p 修改子节点 cZxid = 0xd00000023 mZxid = 0xd00000023 pZxid = 0xd00000023 监视与通知 Watcher Watcher是客户端在服务器端注册的事件监听器；Watcher用于监听znode上的某些事件，如znode数据修改、节点增删等；当监听到事件后，Watcher会触发通知客户端。注意：Watcher是一个单次触发的操作。 节点上下线感知 Watcher + 临时节点 # client1 创建临时节点 create -e /temp temp # client2 监控临时节点 ls /temp watcher # client1 模拟下线 close # client2 获得通知 # WATCHER:: # WatchedEvent state:SyncConnected type:NodeDeleted path:/temp 应用场景 HDFS HA 方案 NameNode存在单点故障问题，一旦NameNode宕机，直接导致HDFS无法对外提供服务。为了解决此问题，可以增加一个备份NameNode，当主NameNode宕机，备NameNode自动快速切换响应外部请求。另外，元数据存在于NameNode的内存中，也存在了如何共享内存元数据的问题。 Hadoop 2.x 版本提出了高可用（High Availability，HA）解决方案。 主备切换 ZKFC 涉及角色 每个NameNode节点上各有一个ZKFC进程 ZKFC即 ZKFailoverController，作为独立进程存在，负责控制NameNode的主备切换 ZKFC会监控NameNode的健康状况，当发现Active NameNode异常时，通过Zookeeper集群进行NameNode主备选举，完成Active和Standby状态的切换 ZKFC在启动时，同时会初始化HealthMonitor和ActiveStandbyElector服务；同时会向HealthMonitor和ActiveStandbyElector注册相应的回调方法 HealthMonitor定时调用NameNode的HAServiceProtocol RPC接口（monitorHealth和getServiceStatus），监控NameNode的健康状态，并向ZKFC反馈 ActiveStandbyElector接收ZKFC的选举请求，通过Zookeeper自动完成NameNode主备选举 选举完成后回调ZKFC的主备切换方法对NameNode进行Active和Standby状态的切换 主备选举过程 全新集群选举 启动两个NameNode和ZKFC 两个ZKFC通过各自ActiveStandbyElector发起NameNode的主备选举，这个过程利用Zookeeper的写一致性和临时节点机制实现 当发起一次主备选举时，ActiveStandbyElector会尝试在Zookeeper创建临时节点/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，Zookeeper的写一致性保证最终只会有一个ActiveStandbyElector创建成功 ActiveStandbyElector从ZooKeeper获得选举结果 创建成功的 ActiveStandbyElector回调ZKFC的回调方法，将对应的NameNode切换为Active NameNode状态 而创建失败的ActiveStandbyElector回调ZKFC的回调方法，将对应的NameNode切换为Standby NameNode状态 不管是否选举成功，所有ActiveStandbyElector都会在临时节点ActiveStandbyElectorLock上注册一个Watcher监听器，来监听这个节点的状态变化事件 非全新集群选举 如果Active NameNode对应的HealthMonitor检测到NameNode状态异常时，通知对应ZKFC ZKFC会调用 ActiveStandbyElector 方法，删除在Zookeeper上创建的临时节点ActiveStandbyElectorLock（或者ActvieStandbyElector与ZooKeeper的session断开，临时节点也会被删除，但有可能此时原Active NameNode仍然是active状态） 此时，Standby NameNode的ActiveStandbyElector注册的Watcher就会监听到此节点的 NodeDeleted事件。 收到这个事件后，此ActiveStandbyElector发起主备选举，成功创建临时节点ActiveStandbyElectorLock，如果创建成功，则Standby NameNode被选举为Active NameNode 如何防止脑裂（加入BreadCrumb，引入隔离机制） 在分布式系统中 双主 现象又称为脑裂，由于Zookeeper的 ”假死”、长时间的垃圾回收或其它原因都可能导致 双Active NameNode 现象，此时两个NameNode都可以对外提供服务，无法保证数据一致性。对于生产环境，这种情况的出现是毁灭性的，必须通过自带的 隔离（Fencing）机制预防此类情况。 正常状态 ActiveStandbyElector成功创建ActiveStandbyElectorLock临时节点后，会创建另一个ActiveBreadCrumb持久节点 ActiveBreadCrumb持久节点保存了Active NameNode的地址信息 当Active NameNode在正常的状态下断开Zookeeper Session，会一并删除临时节点ActiveStandbyElectorLock、持久节点ActiveBreadCrumb 异常状态 但如果ActiveStandbyElector在异常的状态下关闭Zookeeper Session（此时有可能由于active NameNode与ZooKeeper通信不畅导致，所以此时NameNode还处于active状态），那么持久节点ActiveBreadCrumb会保留下来 当另一个NameNode要由standby变成active状态时，会发现上一个Active NameNode遗留下来的ActiveBreadCrumb节点，那么会回调 ZKFC 的方法对旧的Active NameNode进行fencing 首先ZKFC会尝试调用旧Active NameNode的HAServiceProtocol RPC接口的transitionToStandby方法，看能否将其状态切换为Standby 如果transitionToStandby方法切换状态失败，那么就需要执行Hadoop自带的隔离措施，Hadoop目前主要提供两种隔离措施： sshfence：SSH to the Active NameNode and kill the process shellfence：run an arbitrary shell command to fence the Active NameNode 只有成功地fencing之后，选举成功的ActiveStandbyElector才会回调ZKFC的becomeActive方法，然后ZKFS调用transitionToActive将对应的NameNode切换为Active，开始对外提供服务 元数据同步 集群启动后一个NN处于active状态，并提供服务，处理客户端和DataNode的请求，并把editlog写到本地和share editlog（可以是NFS，QJM等）中。另外一个NN处于Standby状态，它启动的时候加载fsimage，然后周期性的从share editlog中获取editlog，保持与active的状态同步。为了实现standby在active挂掉后迅速提供服务，需要DN同时向两个NN汇报，使得standby保存block to datanode信息，因为NN启动中最费时的工作是处理所有datanode的blockreport。 在主备切换过程中，新的Active NameNode必须确保与原Active NamNode元数据同步完成，才能对外提供服务 HA集群中不需要运行SecondaryNameNode、CheckpointNode或者BackupNode。事实上，HA架构中运行上述节点，将会出错 分布式锁 在单一进程多线程环境下，争抢共享资源，可以利用JUC完美解决。但随着系统发展，单一进程系统进一步发展为多进程分布式系统，这样就产生了如何解决共享资源争抢冲突的问题。可以借助ZooKeeper实现分布式锁方案。 方案一（惊群问题，监控同一个临时节点） Client 1、2、3同时向ZooKeeper请求创建临时节点 /lock，默认是无序节点，只有Client 1 创建成功，其他Client创建失败，因此Client 1 获得锁可以使用共享资源，而其他Client对该节点设置watcher等待通知事件 Client 1 任务结束，删除 /lock 或者 关闭同ZooKeeper的session连接 ZooKeeper会通知Client 2和Client 3，有删除节点事件 Client 2和Client 3同时向ZooKeeper请求创建临时节点 /lock，Client 3创建成功，而Client2创建失败，Client2继续对该节点设置watcher等待通知事件 方案一会产生惊群问题，多个进程请求共享资源但无法同时获得锁，未获得锁的进程只有休眠等待通知。一旦多个进程同时得到通知，全部进程会无序CPU调度，网络通信争抢锁，但此时又只会有一个进程获得锁，当有大量争抢进程时，会导致计算和网络资源的浪费。 方案二（监控上一个临时有序节点） Client 1、2、3同时向ZooKeeper询问root持久节点/group是否创建，如果未创建，则只会有一个进程创建root节点。Client 1创建/group根节点，然后创建临时有序节点/group/lock0000000001，然后Client1发现是第一个子节点，则获得锁；Client 2创建临时有序节点/group/lock0000000002，发现不是第一个子节点，则设置上一个/group/lock0000000001节点watcher等待通知；Client 3创建临时有序节点/group/lock0000000003，发现不是第一个子节点，则设置上一个/group/lock0000000002节点watcher等待通知 Client 1 任务结束，删除 /group/lock0000000001 或者 关闭同ZooKeeper的session连接 Client 2 获得通知，检查是否是第一个子节点，如果是则获得锁处理任务；否则，继续监听上一个子节点（Client 1可能异常结束，但非第一个节点） Client 2 任务结束，删除 /group/lock0000000002 Client 3 获得通知，检查是否是第一个子节点，如果是则获得锁处理任务 方案二多个进程获得锁顺序依赖于并发创建临时有序子节点顺序。此方案不失并发性，进程又可调度有序。 核心原理 ZAB算法 仲裁quorum 什么是仲裁quorum 发起proposal时，只要多数派同意，即可生效 为什么要仲裁 不需要所有的服务器都响应proposal就能生效，可以提高集群的响应速度 quorum数如何选择 集群节点数 / 2 + 1 如3节点集群：quorum = 3 / 2 + 1 = 2 脑裂 网络分区 网络通信故障，集群被分成了2部分 脑裂 原leader处于一个分区，而另外一个分区选举出新的leader，此时集群出现2个leader，导致集群紊乱。 防止脑裂导致集群中出现多个leader 为什么规则要求 可用节点数 > 集群总结点数 / 2 ？如果不这样限制，在集群出现脑裂的时候，可能会出现多个子集群同时服务的情况（即子集群各组选举出自己的leader）， 这样对整个zookeeper集群来说是紊乱的。换句话说，如果遵守上述规则进行选举，即使出现脑裂，集群最多也只能出现一个子集群可以提供服务的情况。 ZAB算法 分布式一致性协议 Paxos 偏向于理论、对如何应用到工程实践提及较少 Raft 在2013年提出，提出的时间虽然不长，但已经有很多系统基于Raft实现 Zab 的全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，Zab最大的特点是保证强一致性(strong consistency，或叫线性一致性linearizable consistency)。Zab协议有两种模式 恢复模式（选主） 因为ZooKeeper也是主从架构，当ZooKeeper集群没有主的角色leader时，从众多服务器中选举leader时，处于此模式。主要处理内部矛盾，我们称之为安内 广播模式（同步） 当集群有了leader后，客户端向ZooKeeper集群读写数据时，集群处于此模式。主要处理外部矛盾，我们称之为攘外 Zab 和 Raft 算法相似，区别如下 Zab心跳从follower到leader；Raft从leader到follower Zab任期称为epoch；Raft任期称为term Raft 动图演示地址： http://thesecretlivesofdata.com/raft/ ZooKeeper服务器个数 仲裁模式下，服务器个数选择为奇数（可以保证脑裂状态下，有一个集群时可用的）。 zookeeper选举的规则：leader选举要求 可用节点数 > 总节点数 / 2 。 防止脑裂导致集群不可用 假设5个节点发生脑裂，划分成2个子集群 A集群 B集群 1 4 2 3 3 2 4 1 若满足leader选举要求，可用节点数 = 5 / 2 + 1 = 3，此时多种脑裂情况都会有一个子集群提供服务。 假设4个节点发生脑裂，划分成2个子集群 A集群 B集群 1 3 2 2 3 1 若满足leader选举要求，可用节点数 = 4 / 2 + 1 = 3，此时若两个子集群均匀划分，A和B集群都无法满足lead选举的要求。 在容错能力相同情况下，奇数节点更节省资源 假设4个节点 可用节点数 = 4 / 2 + 1 = 3，允许1个节点宕机 假设5个节点 可用节点数 = 5 / 2 + 1 = 3，允许2个节点宕机 假设6个节点 可用节点数 = 6 / 2 + 1 = 4，允许2个节点宕机 5节点和6节点集群相比，容错能力相同，都允许有2个节点宕机，但5节点的quorum更小，响应速度更快且更节省资源；5节点和4节点集群相比，quorum相同，但5节点的容错能力更强。 读写操作（攘外） 读操作 客户端与一个follower建立Session连接，向ZooKeeper集群读取数据，如 get /test follower返回客户端 /test ZNode信息 写操作 客户端与一个follower建立Session连接，向ZooKeeper集群写入数据，如 create /test follower“权限”不够，询问“领导”leader，将写入数据请求转发给leader leader收到请求后非常“民主”，向所有follower发出 proposal 提案 create /test，包括leader自己。follower和leader收到提案后先记录下来 当有超过半数的 quorum （包括leader自己，quorum = N / 2 + 1）同意提案，则leader首先 commit 提交提案，创建 /test ZNode 然后leader通知所有follower commit 提交提案，follower各自创建 /test ZNode follower响应客户端写入数据成功 选举（安内） 全新集群leader选举 以3台机器组成ZooKeeper集群为例，当集群中有超过半数机器启动后，才可以选举leader。在选举过程中，每个Server都要广播投票，投票信息结构为 (sid, zxid)。Server接收到投票信息后开始Player Killing，PK逻辑是先比较zxid，大的获胜；如果相等，再比较sid，大的获胜。 ZooKeeper服务器的4中状态 looking：服务器处于寻找leader状态 leading：服务器作为leader的状态 following：服务器作为follower的状态 observing：服务器作为observer的状态 选举流程（假设Server1，Server2，Server3依次启动） 启动Server1，投票给自己，投票信息是(1, 0)，投票信息不变且没有过半数，没有选举出leader，仍为looking状态 启动Server2，Server2先在集群中查找是否已经选举出leader，若没有选举出leader，则开始进入选举投票流程，Server2的投票信息是(2, 0) Server1，Server2向集群中服务器广播自己的投票信息 处理投票 Server1给自己投票(1, 0)，接收到Server2投票(2, 0)，PK，结果是(2, 0)，Server1更新投票信息是(2, 0) Server2给自己投票(2, 0)，接收到Server1投票(1, 0)，PK，结果是(2, 0)，Server2结果不变 Server1，Server2向集群中服务器广播自己的投票结果，此轮投票结束 Server1两票都是(2, 0)，票数超过半数（quorum = N / 2 + 1），Server2当选leader Server2两票都是(2, 0)，票数超过半数，Server2当选leader Server2将服务器状态从looking改变为leading，成为leader，同时向集群中服务器广播 Server1收到广播信息后，将服务器状态从looking改变为following，成为follower 启动Server3，Server3先在集群中查找是否已经选举出leader，发现已选举出leader，则不再选举，直接将服务器状态从looking改变为following，成为follower 选举成功后状态同步 当Leader完成选举后，Follower需要与新的Leader同步数据。 在Leader端做如下工作 Leader会构建一个NEWLEADER封包，包括当前最大的zxid，发送给所有的Follower或者Observer Leader给每个Follower创建一个线程LearnerHandler来负责处理每个Follower的数据同步请求，同时主线程开始阻塞，只有超过一半的Follower同步完成，同步过程才完成，Leader才能成为真正的Leader 根据同步算法进行同步操作 在Follower端做如下工作 选举完成后，尝试与Leader建立同步连接，如果一段时间没有连接上就报错超时，重新回到选举状态 向Leader发送FOLLOWERINFO封包，带上自己最大的zxid 根据同步算法进行同步操作 具体使用哪种同步算法取决于Follower当前最大的zxid，在Leader端会维护最小事务id minCommittedLog 和最大事务id maxCommittedLog 两个zxid，minComittedLog 是没有被快照存储的日志文件的第一条（每次快照储存完，会重新生成一个事务日志文件），maxCommittedLog 是事务日志中最大的事务。Zookeeper中实现了以下数据同步算法 直接差异化同步（DIFF同步） 仅回滚同步（TRUNC），即删除多余的事务日志，比如原来的Leader节点宕机后又重新加入，可能存在它自己写入并提交但是其他节点还没来得及提交的数据 先回滚（TRUNC）再差异化（DIFF）同步 全量同步（SNAP） DIFF 假设Leader端未被快照存储的zxid为0x500000001、0x500000002、0x500000003、0x500000004、0x500000005，此时Follower端最大已提交的zxid（即peerLastZxid）为0x500000003，因此需要把0x500000004、0x500000005同步给Follower，直接使用差异化同步（DIFF）即可。 Follower端同步过程如下： Follower端首先收到DIFF指令，进入DIFF同步阶段 Follower收到同步的数据和提交命令，并应用到内存数据库当中 同步完成后，Leader会发送一个NEWLEADER指令，通知Follower已经将最新的数据同步给Follower了，Follower收到NEWLEADER指令后反馈一个ack消息，表明自己已经同步完成 单个Follower同步完成后，Leader会进入集群的”过半策略”等待状态，当有超过一半的Follower都同步完成以后，Leader会向已经完成同步的Follower发送UPTODATE指令，用于通知Follower已经完成数据同步，可以对外提供服务了，最后Follower收到Leader的UPTODATE指令后，会终止数据同步流程，向Leader再次反馈一个ack消息。 TRUNC + DIFF Leader在本地提交事务完成，还没来得及把事务提交提议发送给其他节点前宕机了。假设集群有三个节点，分别是A、B、C，没有宕机前Leader是B，已经发送过0x500000001和0x500000002的数据修改提议和事务提交提议，并且发送了0x500000003的数据修改提议，但在B节点发送事务提交提议（leader已提交）之前，B宕机了，B最新的数据是0x500000003，但发送给A和C的事务提议失败了，A和C的最新数据依然是0x500000002，B宕机后，A和C会进行Leader选举，假设C成为新的Leader，并且进行过两次数据修改，对应的zxid为0x600000001、0x600000002（epoch自增），然而此时B机器恢复后加入新集群，重新进行数据同步，对B来说，peerLastZxid为0x500000003，对于当前的Leader C来说，minCommitedLog=0x500000001, maxCommittedLog=0x600000002（总共是0x500000001、0x500000002、0x600000001、0x600000002未被快照的事务）。这种情况下使用（TRUNC + DIFF）的同步方式，同步过程如下： B恢复并且向已有的集群（AC）注册后，向C发起同步连接的请求 B向Leader C发送FOLLOWERINFO封包，带上Follower自己最大的zxid（0x500000003） C发现自己没有0x500000003这个事务提交记录，就向B发送TRUNC指令，让B回滚到0x500000002 B回滚完成后，向C发送信息包，确认完成，并说明当前的zxid为0x500000002 C向B发送DIFF同步指令 B收到DIFF指令后进入同步状态，并向C发送ACK确认包 C陆续把对应的差异数据修改提议和Commit提议发给B，当数据发送完成后，再发送通知包给B B将数据修改提议应用于内存数据结构并Commit，当收到C通知已经同步完成后，B给回应ACK，并且结束同步 SNAP 当集群中某个节点宕机时间过长，在恢复并且加入集群时，集群中数据的事务日志文件已经生成多个，此时leader的minCommittedLog比该节点宕机时的最大zxid还要大（leader已生成快照）。例如假设ABC集群中B宕机，几天后才恢复，此时minCommittedLog为0x6000008731，而peerLastZxid为0x500000003，这种情况下采用全量同步（SNAP）的方式，同步过程如下： 当Leader C发现B的peerLastZxid小于minCommittedLog时，向B发送SNAP指令 B收到同步指令，进入同步阶段 Leader C会从内存数据库中获取全量的数据发送给B B获取数据处理完成后，C还会把全量同步期间产生的最新的数据修改提议和Commit提议以增量（DIFF）的方式发送给B Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hive/Hive安装部署.html":{"url":"src/bigdata/hive/Hive安装部署.html","title":"Hive安装部署","keywords":"","body":"先决条件 hive是一个构建数据仓库的工具，只需要在一台服务器上安装，不需要在多台服务器上安装。 使用hadoop普通用户在node03上安装 搭建好三节点Hadoop集群 node03上安装MySQL服务 安装 # 拷贝到node03上 scp hive-1.1.0-cdh5.14.2.tar.gz hadoop@192.168.2.102:/bigdata/soft # 解压 cd /bigdata/soft tar -xzvf hive-1.1.0-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置 hive-env.sh cd /bigdata/install/hive-1.1.0-cdh5.14.2/conf mv hive-env.sh.template hive-env.sh vi hive-env.sh 修改内容 # 配置HADOOP_HOME路径 export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2/ # 配置HIVE_CONF_DIR路径 export HIVE_CONF_DIR=/bigdata/install/hive-1.1.0-cdh5.14.2/conf hive-site.xml vi hive-site.xml 修改内容 javax.jdo.option.ConnectionURL jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=latin1&amp;useSSL=false javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword root hive.cli.print.current.db true hive.cli.print.header true hive.server2.thrift.bind.host node03 hive-log4j.properties # 创建hive日志存储目录 mkdir -p /bigdata/install/hive-1.1.0-cdh5.14.2/logs/ cd /bigdata/install/hive-1.1.0-cdh5.14.2/conf mv hive-log4j.properties.template hive-log4j.properties vi hive-log4j.properties 修改内容 hive.log.dir=/bigdata/install/hive-1.1.0-cdh5.14.2/logs/ 拷贝mysql驱动包 scp mysql-connector-java-5.1.38.jar hadoop@192.168.2.102:/bigdata/soft # 由于运行hive时，需要向mysql数据库中读写元数据，所以需要将mysql的驱动包上传到hive的lib目录下 cp mysql-connector-java-5.1.38.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/ 配置环境变量 root用户下执行 su root vi /etc/profile 修改内容 export HIVE_HOME=/bigdata/install/hive-1.1.0-cdh5.14.2 export PATH=$PATH:$HIVE_HOME/bin 切换回hadoop su hadoop source /etc/profile 验证安装 Node03执行 执行前需要先启动hadoop集群和mysql数据库服务 # 启动hive cli命令行客户端 hive 查看数据库 show databases; 退出 quit; Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hive/大数据分析利器之Hive.html":{"url":"src/bigdata/hive/大数据分析利器之Hive.html","title":"大数据分析利器之Hive","keywords":"","body":"数据仓库 基本概念 数据仓库的英文名称为Data Warehouse，可简写为DW或DWH。 数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。它出于分析性报告和决策支持的目的而创建。 数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用，这也是为什么叫“仓库”，而不叫“工厂”的原因。 主要特征 数据仓库是面向主题的（Subject-Oriented）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant）数据集合，用以支持管理决策。 主题的（Subject-Oriented） 数据仓库是一般从用户实际需求出发，将不同平台的数据源按设定主题进行划分整合，与传统的面向事务的操作型数据库不同，具有较高的抽象性。面向主题的数据组织方式，就是在较高层次对分析对象数据的一个完整、统一并一致的描述，能完整及统一地刻画各个分析对象所涉及的有关企业的各项数据，以及数据之间的联系。 集成的（Integrated） 数据仓库中存储的数据大部分来源于传统的数据库，但并不是将原有数据简单的直接导入，而是需要进行预处理。这是因为事务型数据中的数据一般都是有噪声的、不完整的和数据形式不统一的。这些“脏数据”的直接导入将对在数据仓库基础上进行的数据挖掘造成混乱。“脏数据”在进入数据仓库之前必须经过抽取、清洗、转换才能生成从面向事务转而面向主题的数据集合。数据集成是数据仓库建设中最重要，也是最为复杂的一步。 数据进入数据仓库后、使用之前，必须经过加工与集成。 对不同的数据来源进行统一数据结构和编码。统一原始数据中的所有矛盾之处，如字段的同名异义，异名同义，单位不统一，字长不一致等。 将原始数据结构做一个从面向应用到面向主题的大转变。 非易失的（Non-Volatile） 数据仓库中的数据主要为决策者分析提供数据依据。决策依据的数据是不允许进行修改的。即数据保存到数据仓库后，用户仅能通过分析工具进行查询和分析，而不能或很少修改和删除。数据的更新升级主要都在数据集成环节完成，过期的数据将在数据仓库中直接筛除。 数据仓库中包括了大量的历史数据。 数据经集成进入数据仓库后是极少或根本不更新的。 时变的（Time-Variant） 数据仓库数据会随时间变化而定期更新，不可更新是针对应用而言，即用户分析处理时不更新数据。每隔一段固定的时间间隔后，抽取运行数据库系统中产生的数据，转换后集成到数据仓库中。随着时间的变化，数据以更高的综合层次被不断综合，以适应趋势分析的要求。当数据超过数据仓库的存储期限，或对分析无用时，从数据仓库中删除这些数据。关于数据仓库的结构和维护信息保存在数据仓库的元数据(Metadata)中，数据仓库维护工作由系统根据其中的定义自动进行或由系统管理员定期维护。 数据仓库和数据库的区别 数据库与数据仓库的区别实际讲的是OLTP 与 OLAP 的区别。 操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理OLTP。 分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing），一般针对某些主题的历史数据进行分析，支持管理决策。 数据仓库的出现，并不是要取代数据库。 数据库是面向事务的设计，数据仓库是面向主题设计的。 数据库一般存储业务数据，数据仓库存储的一般是历史数据。 数据库设计是尽量避免冗余，一般针对某一业务应用进行设计；比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析；数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。 数据库是为捕获数据而设计，数据仓库是为分析数据而设计。 以银行业务为例。数据库是事务系统的数据平台，客户在银行做的每笔交易都会写入数据库，被记录下来，这里，可以简单地理解为用数据库记账。数据仓库是分析系统的数据平台，它从事务系统获取数据，并做汇总、加工，为决策者提供决策的依据。比如，某银行某分行一个月发生多少交易，该分行当前存款余额是多少。如果存款又多，消费交易又多，那么该地区就有必要设立ATM了。 显然，银行的交易量是巨大的，通常以百万甚至千万次来计算。事务系统是实时的，这就要求时效性，客户存一笔钱需要几十秒是无法忍受的，这就要求数据库只能存储很短一段时间的数据。而分析系统是事后的，它要提供关注时间段内所有的有效数据。这些数据是海量的，汇总计算起来也要慢一些，但是，只要能够提供有效的分析数据就达到目的了。 数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。 分层架构 四层架构 ODS（Operational Data Store 原始数据层）存放原始数据，直接加载原始日志、数据，数据保持原貌不做处理。 DWD（Data Warehouse Detail 明细数据层）结构和粒度与原始表保持一致，对ODS层数据进行清洗，如去除空值，脏数据，超过极限范围数据等。 DWS（Data Warehouse service 服务数据层）以DWD为基础，进行轻度汇总。 ADS（Application Data Store 数据应用层）为各种统计报表提供数据。 三层架构 ODS（Operational Data Store 原始数据层，原始层）存放原始数据，直接加载原始日志、数据，数据保持原貌不做处理。 和原始日志同步，也可以认为是原始日志的备份 若后续计算数据有问题，可以通过原始数据来排查 DW（Data Warehouse 明细数据层，中间层）存放数据仓库明细层的数据，这一层主要用于对数据进行整合和ETL处理之后存放的，可以在各个业务场景下共用的。 大表查询 这种情况就需要提前每天做一次规约聚合操作，将数据按照相应的维度进行一次细粒度的聚合，减少数据量。（表关联会产生笛卡尔积） 多表关联 在hive中，多表之间的关联是性能消耗最大的地方。因此，我们可以做一个大表的聚合关联，在这个时段给予这个聚合任务大量资源来完成，这样后续结果集查询的数据就会很简单，少去很多关联，直接查询这个大表即可，提高查询效率。（表关联会产生shuffle） RES（Result 数据应用层，结果层）为各种统计报表提供数据。也可称为APP、DM、DA层等。一般是从DW层查询计算之后将数据输出到关系型数据库中，由PHP或者Java web和前端合作展示到页面上。数仓工程师要在后续继续维护数据的准确性。 为什么要对数据仓库分层 用空间换时间。通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。 把复杂问题简单化。通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。 隔离原始数据。不论是数据异常，还是数据的敏感性，将原始数据和统计数据解耦。 便于处理业务的变化。随着业务的变化，只需要调整底层的数据，对应用层对业务的调整零感知。 可以横向扩展。中间层都是宽表，包含很多信息，但是这些信息在不同时期有着不同的业务需求，所以要可以增加，但是原则上不能删除字段，即便某个统计指标不在参与计算，但是不可删除。 数据仓库架构 数据采集层 数据采集层的任务就是把数据从各种数据源中采集和存储到数据仓库上，期间有可能会做一些ETL（抽取extra，转化transfer，装载load ）操作。数据源种类可以有多种， 日志所占份额最大，存储在备份服务器上的数据库，如Mysql、Oracle。还有来自HTTP/FTP的数据，比如合作伙伴提供的接口所获取的数据，还有一些其他数据源，如Excel等需要手工录入的数据。 数据存储与分析 HDFS是大数据环境下数据仓库/数据平台最完美的数据存储解决方案。离线数据分析与计算，也就是对实时性要求不高的部分，Hive是不错的选择。 我们通过hive作为元信息的表结构化管理，可以通过spark sql或者 impala，或者presto sql 去操作查询。 数据输出 前面使用Hive、MR、Spark、SparkSQL分析和计算的结果，还是在HDFS上，但大多业务和应用不可能直接从HDFS上获取数据，那么就需要一个数据共享的地方，使得各业务和产品能方便的获取数据。 这里的数据共享，其实指的是前面数据分析与计算后的结果存放的地方，其实就是关系型数据库mysql或者其他NOSQL的数据库。一般都是采用mysql。 数据应用 报表，业务系统，运营系统等公司系统所使用的数据，通常是数据集市层直接查询，一般也是已经统计汇总好的存放于数据集市层中通过直接操作SQL得到。 数据仓库建模 建模思想 Inmon和Kimball是最常见的两种架构。 Inmon主张自上而下（从上游到下游）的架构，它将数据仓库定义为整个企业级的集中存储。数据仓库存放着最低的详细级别的原子数据。维度数据集市只是在数据仓库完成后才创建的。因此，数据仓库是企业信息工厂（CIF）的中心，它为交付商业智能提供逻辑框架。 不同的OLTP数据集中到面向主题、集成的、不易失的和时间变化的结构中，用于以后的分析。且数据可以通过下钻到最细层，或者上卷到汇总层。数据集市应该是数据仓库的子集，每个数据集市是针对独立部门特殊设计的。 而Kimball正好与Inmon相反，Kimball架构是一种自下而上（从下游到上游）的架构，它认为数据仓库是一系列数据集市的集合。它首先建立最重要的业务单元或部门的数据集市。这些数据集市可以为透视组织数据提供一个较窄的视图，需要的时候，这些数据集市还可以与更大的数据仓库合并在一起。 Kimball将数据仓库定义为“一份针对查询和分析做特别结构化的事物数据拷贝。”Kimball的数据仓库结构就是著名的数据仓库总线。企业可以通过一系列维数相同的数据集市递增地构建数据仓库，通过使用一致的维度，能够共同看到不同数据集市中的信息，这表示它们拥有公共定义的元素。 两种模式各有优势，Inmon模式适合开发进度慢，实施成本高，适合对设计科学性和规范性较高的企业，在业务模式较固定的行业应用较好，比如金融和电信等行业。Kimball 模式适合快速迭代，实施成本低，能够较快交付任务。这种模式非常适应互联网行业的高速发展，也适合中小型企业。 维度建模 事实表，维度表，实体表 事实表 事实表其实质就是通过各种维度和一些指标值的组合来确定一个事实的，比如通过时间维度，地域组织维度，指标值可以去确定在某时某地的一些指标值怎么样的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。 维度表 维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，它里面的数据就是一些日，周，月，季，年，日期等数据，维度表只能是事实表的一个分析角度。 实体表 实体表就是一个实际对象的表，实体表它放的数据一定是一条条客观存在的事物数据，比如说设备 ，它就是客观存在的，所以可以将其设计一个实体表。 Kimball提出了维度建模方法，这个也是企业中最常见的方法，将表分为事实表和维度表。维度模型关注的重点是如何使最终用户访问数据仓库更容易，并有较高的性能。 事实表可以看成是日志和业务数据，包括维度字段，以及各个度量值；实体表是一种特殊的事实表；而维度表可以看成是字典表。他们之间通过主键和外键关联。 在维度建模方法体系中，维度是描述事实的角度，如日期、渠道、服区id等，事实是要度量的指标，如注册人数、充值金额、登录次数等。 星型模型，雪花模型，星座模型 星型模型 一个事实表对应多个维度表，标准的星型模型只有一层。 雪花模型 一个事实表对应多个维度表，一个维度表又可能包含多个维度表。 星座模型 基于多个事实表，并且他们共享了某些维度表。 模型如何选择？星座模型只和业务需求有关系，同设计并无直接联系。星型模型是性能优先，而雪花模型是灵活优先。实际中，数据仓库更倾向于星型模型，尤其是hadoop体系，减少关联层级可以减少join操作，也就是减少shuffle操作。 范式建模 范式建模法主要由Inmon所提倡，主要运用于传统数仓之中，一般传统的数仓是建立在关系型数据库之上的，不是大数据平台下。它解决关系型数据库的数据存储，利用的一种技术层面上的方法。目前范式建模法大部分采用的是三范式建模法。 第一范式（1NF）：属性都是原子性的，即数据库表的每一列都是不可分割的原子数据项。 第二范式（2NF）：在1NF的基础上，实体的属性完全依赖于主关键字，不能存在仅依赖主关键字一部分的属性，也就是不存在局部依赖。 第三范式（3NF）：在2NF的基础上，任何非主属性不依赖于其它非主属性，也就是不存在传递依赖。 实体建模 在数据系统中，将数据抽象为“实体”、“属性”、“关系”来表示数据关联和事物描述，这种对数据的抽象建模通常被称为实体关系模型。 实体：通常为参与到过程中的主体，客观存在的，比如游戏道具、坐骑、等级、武器装备，此实体非数据库表的实体表。 属性：对主体的描述、修饰即为属性，比如游戏道具的属性有道具名称、道具类型、经验值、价值金币的额度、解封的等级等。 关系：现实的物理事件是依附于实体的，比如获得道具并放入背包事件，依附实体商品、货位，就会有“库存”的属性产生；用户购买道具，依附实体用户、道具，就会有“购买数量”、“金额”的属性产品。 当这些实体建立关系的时候需要根据主键来进行关联，关联的时候就会产生1对1、1对多、多对多的关系。 ETL 数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。 抽取 主要是针对各个业务系统及不同服务器的分散数据，充分理解数据定义后，规划需要的数据源及数据定义，制定可操作的数据源，制定增量抽取和缓慢渐变的规则。 清洗 主要是针对系统的各个环节可能出现的数据二义性、重复、不完整、违反业务规则等数据质量问题，允许通过数据抽取设定的数据质量规则，将有问题的记录先剔除出来，根据实际情况调整相应的清洗操作。 转换 主要是针对数据仓库建立的模型，通过一系列的转换来实现将数据从业务模型到分析模型，通过ETL工具可视化拖拽操作可以直接使用标准的内置代码片段功能、自定义脚本、函数、存储过程以及其他的扩展方式，实现了各种复杂的转换，并且支持自动分析日志，清楚的监控数据转换的状态并优化分析模型。 装载 主要是将经过转换的数据装载到数据仓库里面，可以通过直连数据库的方式来进行数据装载，可以充分体现高效性。在应用的时候可以随时调整数据抽取工作的运行方式，可以灵活的集成到其他管理系统中。 目前有很多的ETL工具，比如SEDWA、kettle、OracleGoldengate、informatica、talend等等。这些工具主要是用于传统数仓。基于大数据平台的，少部分公司有用kettle的，其他的工具就很少有公司在用。 因为实际上对于ETL这个流程概念来讲，不同的业务场景有不同的操作流程，有些公司是在数据载入大数据平台之前进行这一步的，有些公司是在数据载入大数据平台之后才有ETL这个操作的，这个就要根据具体的实际场景，看在哪个阶段使用比较合适。 数据仓库的缓慢变化维 缓慢变化维度（slowly changing dimension, SCD）就是数据仓库维度表中，那些随时间变化比较不明显，但仍然会发生变化的维度。由于维度变化缓慢，且不知道究竟何时可能会发生变化（可能几个月或者几年有可能更新一次），所以我们在设计数据仓库的数据同步的时候，对于一些缓慢变化维度，就比较棘手，因为我们不知道该何时去同步这些变化了的维度。 重写覆盖属性值 当一个维度值的源发生变化，并且不需要在星型模式中保留变化历史时，通常采用新数据来覆盖旧数据 ，这个方法有个前提，那就是用户不关心这个数据的变化或者这个数据是错误数据 。这样的处理使属性所反映的总是最新的赋值 。 添加维度行（推荐） 保留事实的历史环境 ，并插入新的维度行 。 增加维度列 用不同的字段来保存不同的值，实际上就是在后面添加一个字段，这个字段用来保存变化后的当前值，而原来的值则被称为变化前的值，总的来说这种方法通过添加字段来保存变化后的痕迹 。 添加历史表 另外建一个表来保存历史记录，这种方式就是将历史数据与当前数据完全分开来，在维度中只保存当前的数据 。 混合模式之可预见的多重变化 非常规混合模型 给出一个版本号来标识数据是否为当前存储值，如果是，那么版本号为0；如果不是，那么版本号为非0。当插入数据的时候就会对之前的数据版本号进行修改，每插入一次，对应的历史记录的版本号就会增加一，这样用户就可以通过版本号来查询指定历史数据。 混合模式之不可预见的单重变化 数据仓库的元数据管理 元数据（Meta Data），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及 ETL 的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿了数据仓库的整个生命周期，使用元数据驱动数据仓库的开发，使数据仓库自动化，可视化。 元数据类型 元数据可分为技术元数据、业务元数据和管理过程元数据。 技术元数据为开发和管理数据仓库的 IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。主要包含以下作用 数据仓库结构的描述，包括仓库模式、视图、维、层次结构和导出数据的定义，以及数据集市的位置和内容； 业务系统、数据仓库和数据集市的体系结构和模式 汇总用的算法，包括度量和维定义算法，数据粒度、主题领域、聚集、汇总、预定义的查询与报告； 由操作环境到数据仓库环境的映射，包括源数据和它们的内容、数据分割、数据提取、清理、转换规则和数据刷新规则、安全（用户授权和存取控制）。 业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。 企业概念模型：这是业务元数据所应提供的重要的信息，它表示企业数据模型的高层信息、整个企业的业务概念和相互关系。以这个企业模型为基础，不懂数据库技术和SQL语句的业务人员对数据仓库中的数据也能做到心中有数。 多维数据模型：这是企业概念模型的重要组成部分，它告诉业务分析人员在数据集市当中有哪些维、维的类别、数据立方体以及数据集市中的聚合规则。这里的数据立方体表示某主题领域业务事实表和维表的多维组织形式。 业务概念模型和物理数据之间的依赖：以上提到的业务元数据只是表示出了数据的业务视图，这些业务视图与实际的数据仓库或数据库、多维数据库中的表、字段、维、层次等之间的对应关系也应该在元数据知识库中有所体现。 管理过程元数据指描述管理领域相关的概念、关系和规则的数据，主要包括管理流程、人员组织、角色职责等信息。 元数据功能 血缘分析：向上追溯元数据对象的数据来源。血缘分析可以帮助您轻松回答：'我正在查看的报告数据来源是什么？'以及'对当前分析的数据应用了哪些转换处理？'等问题。这样的机制及对这些问题的回答确保了对所分析的数据更高的信任水平，并有助于实现许多行业(包括医疗、金融、银行和制造业等)对所呈现数据的特殊监管及合规性要求。 影响分析：向下追溯元数据对象对下游的影响。影响分析可以让您轻松应对变更可能产生的影响，自动识别与其相关的依赖项和潜在的影响还可以跟踪所有对象及其依赖关系，最后我们还提供数据全生命周期的可视化显示。例如，如果您的某一信息系统中准备将“销售额”从包含税费更改为不包括税费，则SE-DWA将自动显示所有使用了“销售金额”字段，以便您可以确定有哪些工作需要完成，并且建议您在更改前完成该工作。 同步检查：检查源表到目标表的数据结构是否发生变更。 指标一致性分析：定期分析指标定义是否和实际情况一致。 实体关联查询：事实表与维度表的代理键自动关联。 元数据应用 ETL自动化管理：使用元数据信息自动生成物理模型，ETL程序脚本，任务依赖关系和调度程序。 数据质量管理：使用数据质量规则元数据进行数据质量测量。数据质量根据设定的规则帮助您过滤出有问题的数据，并智能分析数据质量缺陷。 数据安全管理：使用元数据信息进行报表权限控制。可以方便查看用户和访问权限，并启用对象级和行级安全管理。对象级安全性确保通过身份验证的用户只能访问他们被授权查看的数据、表或列，其它数据则不可见。基于行的安全性会更进一步，可以限制特定的组成员只可以访问表中特定的数据。 数据标准管理：使用元数据信息生成标准的维度模型。 数据接口管理：使用元数据信息进行接口统一管理。多种数据源接入，并提供多种插件对接最流行的源系统。应该可以简单方便获取数据。 项目文档管理：使用元数据可以自动、方便的生成的健壮全面的项目文档，其以帮助您应对各种对于数据合规性要求。读取元数据模型，并生成pdf格式的描述文件。生成文档您查看每个对象的名称、设置、描述和代码。 数据语义管理：业务用户在自助服务分析中面临的挑战他们不了解数据仓库从而无法正确解释数据，使用元数据可以语义层建模，使用易于业务用户理解的描述来转换数据。 Hive 概念 Hive是基于Hadoop的一个数据仓库工具。 可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。 其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储支持，即Hive可以理解为一个将SQL转换为MapReduce任务的工具，甚至更进一步可以说Hive就是一个MapReduce的客户端。 与数据库区别 对比项 Hive RDBMS 查询语言 HQL SQL 数据存储 HDFS Raw Device or local FS 执行器 MapReduce Executor 数据插入 支持批量导入/单条插入 支持批量导入/单条插入 数据操作 覆盖追加 行级更新删除 处理数据规模 大 小 执行延迟 高 低 分区 支持 支持 索引 0.8版本之后加入简单索引 支持复杂索引 扩展性 高（好） 有限（差） 数据加载模式 读时模式（快） 写时模式（慢） 应用场景 海量数据查询 实时查询 Hive 具有 SQL 数据库的外表，但应用场景完全不同。 Hive 只适合用来做海量离线数据统计分析，也就是数据仓库。 优缺点 优点 操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。 避免了去写MapReduce，减少开发人员的学习成本。 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 缺点 Hive 的查询延迟很严重 Hive 不支持事务 架构原理 用户接口：Client CLI（hive shell） JDBC/ODBC（java访问hive） WEBUI（浏览器访问hive，可以使用HUE） 元数据：Metastore 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore Hadoop集群 使用HDFS进行存储，使用MapReduce进行计算 Driver：驱动器 解析器（SQL Parser） 将SQL字符串转换成抽象语法树AST 对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误 编译器（Physical Plan）：将AST编译生成逻辑执行计划 优化器（Query Optimizer）：对逻辑执行计划进行优化 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说默认就是MapReduce任务 交互方式 Hive交互式shell 不推荐 启动 hive CLI # Hive CLI is deprecated and migration to Beeline is recommended. hive 执行语句 -- 列出数据库 show databases; -- 退出 quit; JDBC服务 推荐 启动 hiveserver2 服务 # 前台启动 hive --service hiveserver2 # nohup 不挂断的运行 # & 后台运行 # 0 – stdin (standard input) | 1 – stdout (standard output) | 2 – stderr (standard error) # 将2重定向到&1，&1再重定向到文件中 nohup hive --service hiveserver2 > /home/hadoop/hiveserver2log/hs2.log 2>&1 & # 检查后台服务，会有一个RunJar进程 jps beeline连接hiveserver2服务 # 启动客户端 beeline 连接数据库服务 -- 输入用户名hadoop，密码hadoop(首次需要设置) !connect jdbc:hive2://node03:10000 -- 列出数据库 show databases; -- 查看帮助 help -- 退出 !quit Hive命令 数仓搭建好后，执行脚本 执行HQL语句 # 使用 –e 参数来直接执行hql语句 hive -e \"show databases\" 执行HQL脚本 创建脚本 vi myhive.hql 脚本内容 create database if not exists myhive; 执行脚本 # 执行 hive -f myhive.hql # 检查 hive -e \"show databases\" 数据类型 基本数据类型 类型名称 描述 举例 boolean true/false true tinyint 1字节的有符号整数 1 smallint 2字节的有符号整数 1 int 4字节的有符号整数 1 bigint 8字节的有符号整数 1 float 4字节单精度浮点数 1.0 double 8字节单精度浮点数 1.0 string 字符串（不设长度） “abc” varchar 字符串（1-65355长度，超长截断） “abc” timestamp 时间戳 1563157873 date 日期 20190715 复合数据类型 类型名称 描述 定义 array 一组有序的字段，字段类型必须相同 col array map 一组无序的键值对 col map struct 一组命名的字段，字段类型可以不同 col struct array类型字段的元素访问方式 准备数据 t_array.txt 1 zhangsan beijing,shanghai 2 lisi shanghai,tianjin,wuhan 建表 -- 建表 -- field间空格分隔 -- array用,分隔 create table myhive.t_array(id string,name string,locations array) row format delimited fields terminated by ' ' collection items terminated by ','; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/t_array.txt' into table myhive.t_array; -- 查询 select * from myhive.t_array; -- 通过下标获取元素 select id,name,locations[0],locations[1] from myhive.t_array; -- 记录的数组元素若不存在，则为NULL select id,name,locations[0],locations[1],locations[2] from myhive.t_array; map类型字段的元素访问方式 准备数据 t_map.txt 1 name:zhangsan#age:30 2 name:lisi#age:40 建表 -- 建表 -- field间空格分隔 -- map中的每一个kv对以#分隔（本质是集合），kv以:分隔 create table myhive.t_map(id string,info map) row format delimited fields terminated by ' ' collection items terminated by '#' map keys terminated by ':'; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/t_map.txt' into table myhive.t_map; -- 查询 select * from myhive.t_map; -- 通过键获取值 -- 只能单独访问map的k或v，不能直接访问集合kv对 select id,info['name'],info['age'] from t_map; struct类型字段的元素访问方式 准备数据 t_struct.txt 1 zhangsan:30:beijing 2 lisi:40:shanghai 建表 -- 建表 create table myhive.t_struct(id string,info struct) row format delimited fields terminated by ' ' collection items terminated by ':' ; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/t_struct.txt' into table myhive.t_struct; -- 查询 select * from myhive.t_struct; -- 类似对象获取属性方法 select id,info.name,info.age,info.address from myhive.t_struct; DDL（Data Definition Language） 数据库DDL操作 创建数据库 -- 重复创建失败 Database test already exists create database test; -- 不存在才创建 -- 默认hdfs存储路径：/user/hive/warehouse/test.db -- 在hdfs中数据库映射为目录 create database if not exists test; 列出数据库 -- 列出所有数据库 show databases; -- 模糊查询数据库 show databases like '*t*'; 查询数据库信息 -- 数据库信息 desc database test; -- 数据库扩展信息 desc database extended test; 切换数据库 -- 切换到当前数据库 use test; 删除数据库 -- 删除存在数据库 -- 删除不存在的数据库失败 Database does not exist: test drop database test; -- 存在才删除 drop database if exists test; -- 当数据库有表存在时，需要级联强制删除 -- 慎用 drop database if exists myhive cascade; 表DDL操作 创建内部表 直接建表 use myhive; -- 在hdfs中表映射为目录 create table stu(id int, name string); -- 可以通过 insert into 向hive表中插入数据 -- 但不建议这么做，因为每个 insert into 转换成MapReduce后会生成一个小文件 -- 在hdfs中表数据映射为文件 insert into stu(id,name) values(1,\"zhangsan\"); insert into stu(id,name) values(2,\"lisi\"); -- 查询表数据 select * from stu; 查询建表 -- 通过 AS 查询语句完成建表，将子查询的结果存入新表 -- hdfs只有一个文件 create table if not exists myhive.stu1 as select id, name from stu; like建表 -- 根据已存在表的结构建表，没有数据 create table if not exists myhive.stu2 like stu; 创建内部表并指定字段之间的分隔符，指定文件的存储格式，以及数据存放的位置 -- 默认 \\001（非打印字符）分隔 field -- 自定义 \\t 分隔 field create table if not exists myhive.stu3(id int, name string) row format delimited fields terminated by '\\t' stored as textfile location '/user/stu3'; -- hdfs文件以\\t分隔存储每行数据的各个字段 insert into myhive.stu3(id,name) values(1,\"zhangsan\"); 创建外部表 外部表加载hdfs其他路径下已存在的数据文件，因此外部表不会独占数据文件，当删除表时，不会删除相应的数据文件 创建外部表需要加 external 关键字 location 字段可以指定，也可以不指定 当不指定location时，默认存放在指定数据库位置下。若没有指定数据库，则保存在default数据库下 当指定location时，使用location作为数据目录，数据库下不会再创建相应表的文件夹 create external table myhive.teacher (t_id string, t_name string) row format delimited fields terminated by '\\t'; 插入数据 通过 insert into 方式不推荐 通过 load data 方式加载数据到内部表或外部表 -- 加载本地文件 -- 拷贝 load data local inpath '/home/hadoop/hivedatas/teacher.csv' into table myhive.teacher; -- 加载hdfs文件 -- 剪切 -- overwrite 覆盖原有数据；否则追加 load data inpath '/hivetest/teacher.csv' overwrite into table myhive.teacher; 内部表与外部表的互相转换 内部表删除后，表的元数据和真实数据都被删除 外部表删除后，仅仅只是把该表的元数据删除，真实数据还在，后期可以恢复 使用时机 内部表由于删除表的时候会同步删除HDFS的数据文件，所以确定如果一个表仅仅是你独占使用，其他人不使用的时候就可以创建内部表，如果一个表的文件数据其他人也要使用，那么就创建外部表 外部表用在数据仓库的ODS层 内部表用在数据仓库的DW层 -- 内部表转换为外部表 -- EXTERNAL_TABLE alter table stu set tblproperties('EXTERNAL'='TRUE'); -- 外部表转换为内部表 alter table teacher set tblproperties('EXTERNAL'='FALSE'); 创建分区表 如果hive当中所有的数据都存入到一个目录下，那么在使用MR计算程序的时候，读取整个目录下面的所有文件来进行计算（全量扫描，性能低），就会变得特别慢，因为数据量太大了 实际工作中一般都是计算前一天的数据（日增），所以我们只需要将前一天的数据挑出来放到一个目录下面即可，专门去计算前一天的数据 这样就可以使用hive当中的分区表，通过分目录的形式，将每一天的数据都分成为一个目录，然后我们计算数据的时候，通过指定前一天的目录即可只计算前一天的数据 在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天，或者每小时进行切分成一个个的小的文件，这样去操作小的文件就会容易得多 创建分区表 -- 按month分区，不需要是表字段 -- 分区对应数据库表的一个字段，分区下所有数据的分区字段值相同 -- load data 之后才会出现分区文件夹 create table myhive.score(s_id string, c_id string, s_score int) partitioned by (month string) row format delimited fields terminated by '\\t'; -- 按year、month和day分区 create table myhive.score1 (s_id string, c_id string, s_score int) partitioned by (year string, month string, day string) row format delimited fields terminated by '\\t'; 加载数据 -- score -- month=201806对应hdfs的文件夹名 load data local inpath '/home/hadoop/hivedatas/score.csv' into table myhive.score partition (month='201806'); -- score1 -- score1/year=2018/month=06/day=01 load data local inpath '/home/hadoop/hivedatas/score.csv' into table myhive.score1 partition (year='2018', month='06', day='01'); 查询表分区 show partitions myhive.score; 添加表分区 -- 添加之后就可以在hdfs看到相应的文件夹 alter table myhive.score add partition(month='201805'); -- 同时添加多个分区 alter table myhive.score add partition(month='201804') partition(month='201803'); 删除表分区 alter table myhive.score drop partition(month='201806'); 综合举例 -- hdsf 创建日期目录，每日增加日期文件夹和数据 hdfs -mkdir /hivetest/day=20180607 -- 上传数据 hdfs dfs -put score.csv /hivetest/day=20180607 -- 创建外部分区表，同时指定数据位置 -- 表删除后，实际数据不删除 create external table myhive.score2(s_id string, c_id string, s_score int) partitioned by (day string) row format delimited fields terminated by '\\t' location '/hivetest'; -- 可以观察到虽然创建表成功，但没有创建相应的分区，也就是没有相应的MetaStore元数据 show partitions myhive.score2; -- 表数据是空的 select * from myhive.score2; -- 解决办法：1、添加表分区(繁琐)；2、metastore check repair命令自动添加元数据 msck repair table myhive.score2; -- day=20180607 show partitions myhive.score2; -- 数据加载成功 select * from myhive.score2; 创建分桶表 分桶是相对分区进行更细粒度的划分 Hive表或分区表可进一步的分桶 分桶将整个数据内容按照某列取hash值，对桶的个数取模的方式决定该条记录存放在哪个桶（文件）当中。所以必须进过一次MapReduce，也就是为什么分桶表的数据都是从结果集中以insert的方式导入。 作用 取样sampling更高效 提升某些查询操作效率，例如map side join 开启参数支持 系统根据表定义自动分桶（推荐） -- 开启对分桶表的支持 -- set hive.enforce.bucketing; 可以查询是否支持分桶，默认是false set hive.enforce.bucketing=true; 参数设定 -- 设置与桶相同的reduce个数（默认只有一个reduce） set mapreduce.job.reduces=4; -- 或 -- set mapreduce.reduce.tasks = 4; -- 在后续插入数据时，select + cluster by 创建分桶表 -- 创建分桶表 -- 分桶字段是表的字段 create table myhive.user_buckets_demo(id int, name string) clustered by(id) into 4 buckets row format delimited fields terminated by '\\t'; -- 创建普通表 create table myhive.user_demo(id int, name string) row format delimited fields terminated by '\\t'; 准备数据 buckets.txt 1 anzhulababy1 2 anzhulababy2 3 anzhulababy3 4 anzhulababy4 5 anzhulababy5 6 anzhulababy6 7 anzhulababy7 8 anzhulababy8 9 anzhulababy9 10 anzhulababy10 加载数据 -- 普通表加载数据 load data local inpath '/home/hadoop/hivedatas/buckets.txt' overwrite into table myhive.user_demo; -- 加载数据到分桶表 -- 可以在hdsf中观察user_buckets_demo表所属的文件夹下共有4个文件(对应4个ReduceTask) insert into table myhive.user_buckets_demo select * from myhive.user_demo; 抽样查询分桶表的数据 TABLESAMPLE语法：TABLESAMPLE (BUCKET x OUT OF y [ON colname]) 。其中，x表示从第几个桶开始采样数据，桶序号从1开始，y表示桶数，colname表示每一行被采样的列。 -- 将user_demo以id作为采样列，划分为两个桶，返回第一个桶的数据 -- 假设x=1，如果桶数多于y，则每y桶取第一个；如果桶数小于y，则将桶数再按照y进行平均划分，然后再取第一个 select * from myhive.user_demo tablesample(bucket 1 out of 2 on id); -- 以随机数作为采样列，因此每一次返回的数据不同 select * from myhive.user_demo tablesample(bucket 1 out of 2 on rand()); -- 显然，对 user_demo 采样，需对全表扫描。如果该表事先就是分桶表的话，采样效率会更高 -- user_buckets_demo 本身是分桶表，共有4桶 -- y 取值2，含义是分两桶，取第一桶采样数据。但表本身有4桶，共取两桶数据 4/2=2 作为采样数据，分别是第一桶和第三桶 -- 对数据除以4取余数，值为0，1，2，3 -- 第1桶 [0] 4 8 -- 第2桶 [1] 1 5 9 -- 第3桶 [2] 2 6 10 -- 第4桶 [3] 3 7 select * from myhive.user_buckets_demo tablesample(bucket 1 out of 2); -- 4/8=1/2 取第一桶的1/2作为采样数据 select * from myhive.user_buckets_demo tablesample(bucket 1 out of 8); 删除表 drop table myhive.stu2; 修改表结构信息 修改表名 use myhive; alter table teacher2 rename to teacher1; 增加列 use myhive; -- 已有记录的新增字段值为NULL alter table stu1 add columns(address string,age int); 修改列 use myhive; alter table stu1 change column address address_id int; 列出数据库表 use myhive; show tables; 查询表结构信息 -- 简要信息，只有字段名、类型和描述 desc myhive.stu; -- 详细信息 -- MANAGED_TABLE 内部表 desc formatted myhive.stu; DML（Data Manipulation Language） 数据导入 直接向表中插入数据 强烈不建议 create table myhive.score3 like score; -- 生成MR，对应hdfs一个小文件 insert into table myhive.score3 partition(month ='201807') values ('001','002','100'); 通过load加载数据 重要 -- overwrite只覆盖指定分区数据 -- 不会生成MR load data local inpath '/home/hadoop/hivedatas/score.csv' overwrite into table myhive.score3 partition(month='201806'); 通过查询加载数据 重要 create table myhive.score5 like score; -- 生成MR，所有数据对应一个文件 -- overwrite覆盖原有数据 insert overwrite table myhive.score5 partition(month='201806') select s_id,c_id,s_score from myhive.score3; 通过查询创建表并加载数据 create table myhive.score6 as select * from score; 创建表时指定location create external table myhive.score7 (s_id string,c_id string,s_score int) row format delimited fields terminated by '\\t' location '/hivetest/score'; 上传数据文件 # 上传文件到指定位置 hdfs dfs -put score.csv /hivetest/score 通过导入导出的数据 create table myhive.teacher2 like teacher; -- hdfs目录结构： -- /hivetest/teacher/_metadata -- /hivetest/teacher/data/teacher.csv export table myhive.teacher to '/hivetest/teacher'; -- 导入数据 import table myhive.teacher2 from '/hivetest/teacher'; 数据导出 insert 导出 -- 导出到本地 -- 生成MR，结果字段默认分隔为'\\001' insert overwrite local directory '/home/hadoop/hivedatas/stu' select * from myhive.stu1; -- 格式化输出结果 insert overwrite local directory '/home/hadoop/hivedatas/stu2' row format delimited fields terminated by ',' select * from myhive.stu1; -- 导出到hdfs insert overwrite directory '/hivetest/export/stu' row format delimited fields terminated by ',' select * from myhive.stu1; Hive 命令 -- 字段间以 \\t 分隔 hive -e 'select * from myhive.stu1;' > /home/hadoop/hivedatas/student.txt export导出到HDFS export table myhive.stu1 to '/hivetest/student'; 静态分区 需手动指定分区 创建分区表 create table myhive.order_partition( order_number string, order_price double, order_time string) partitioned BY(month string) row format delimited fields terminated by '\\t'; 准备数据 order.txt 10001 100 2019-03-02 10002 200 2019-03-02 10003 300 2019-03-02 10004 400 2019-03-03 10005 500 2019-03-03 10006 600 2019-03-03 10007 700 2019-03-04 10008 800 2019-03-04 10009 900 2019-03-04 加载数据 -- 加载数据时手动指定分区 -- 执行成功后，分区和数据全部建立 load data local inpath '/home/hadoop/hivedatas/order.txt' overwrite into table myhive.order_partition partition(month='2019-03'); -- 查询数据 select * from myhive.order_partition where month='2019-03'; 动态分区 利用中间表数据导入时自动创建分区 创建表 -- 创建普通表 create table myhive.t_order( order_number string, order_price double, order_time string) row format delimited fields terminated by '\\t'; -- 创建目标分区表 create table myhive.order_dynamic_partition( order_number string, order_price double) partitioned BY(order_time string) row format delimited fields terminated by '\\t'; 准备数据 order_partition.txt 注意数据格式的规则正确性，否则会出现异常分区，导致查询数据出现问题 10001 100 2019-03-02 10002 200 2019-03-02 10003 300 2019-03-02 10004 400 2019-03-03 10005 500 2019-03-03 10006 600 2019-03-03 10007 700 2019-03-04 10008 800 2019-03-04 10009 900 2019-03-04 加载数据 -- 向普通表加载数据 load data local inpath '/home/hadoop/hivedatas/order_partition.txt' overwrite into table myhive.t_order; -- 支持自动分区需设置参数 -- 自动分区 set hive.exec.dynamic.partition=true; -- 非严格模式 set hive.exec.dynamic.partition.mode=nonstrict; -- 加载数据 insert into table myhive.order_dynamic_partition partition(order_time) select order_number, order_price, order_time from t_order; -- 查询数据 select * from myhive.order_dynamic_partition where order_time='2019-03-02'; select * from myhive.order_dynamic_partition where order_time='2019-03-03'; select * from myhive.order_dynamic_partition where order_time='2019-03-04'; 查询数据 基本 SQL 语言大小写不敏感 SQL 可以写在一行或者多行 关键字不能被缩写也不能分行 各子句一般要分行写 使用缩进提高语句的可读性 算术运算符 运算符 描述 A+B A和B 相加 A-B A减去B A*B A和B 相乘 A/B A除以B A%B A对B取余 A&B A和B按位取与 A|B A和B按位取或 A^B A和B按位取异或 ~A A按位取反 比较运算符 操作符 支持的数据类型 描述 A=B 基本数据类型 如果A等于B则返回true，反之返回false AB 基本数据类型 如果A和B都为NULL，则返回true，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL A<>B, A!=B 基本数据类型 A或者B为NULL则返回NULL；如果A不等于B，则返回true，反之返回false A 基本数据类型 A或者B为NULL，则返回NULL；如果A小于B，则返回true，反之返回false A 基本数据类型 A或者B为NULL，则返回NULL；如果A小于等于B，则返回true，反之返回false A>B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于B，则返回true，反之返回false A>=B 基本数据类型 A或者B为NULL，则返回NULL；如果A大于等于B，则返回true，反之返回false A [NOT] BETWEEN B AND C 基本数据类型 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为true，反之为false。如果使用NOT关键字则可达到相反的效果。 A IS NULL 所有数据类型 如果A等于NULL，则返回true，反之返回false A IS NOT NULL 所有数据类型 如果A不等于NULL，则返回true，反之返回false IN(数值1, 数值2) 所有数据类型 使用 IN运算显示列表中的值 A [NOT] LIKE B STRING 类型 B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。like不是正则，而是通配符 A RLIKE B, A REGEXP B STRING 类型 B是一个正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。 逻辑运算符 操作符 操作 描述 A AND B 逻辑并 如果A和B都是true则为true，否则false A OR B 逻辑或 如果A或B或两者都是true则为true，否则false NOT A 逻辑否 如果A为false则为true，否则false 查询 -- 全表查询 select * from myhive.stu1; -- 选择特定字段查询 select id,name from myhive.stu1; -- 重命名字段 select id,name as stuName from myhive.stu1; -- as可以省略 select id,name stuName from myhive.stu1; -- 限制返回行数 select * from myhive.score limit 5; -- 条件过滤 select * from myhive.score where s_score > 60; 函数 -- 求总行数 select count(*) cnt from myhive.score; -- 求某一字段的最大值 select max(s_score) from myhive.score; -- 求某一字段的最小值 select min(s_score) from myhive.score; -- 求字段值的总和 select sum(s_score) from myhive.score; -- 求字段值的平均数 select avg(s_score) from myhive.score; 分组 Group By语句通常会和聚合函数一起使用，按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。 对多个字段同时进行group by。当所有group by字段全部都相同时，才会被合并为一条记录。 减少数据量（同一用户多次登录） 可以满足更多的查询需求（针对不同维度进行分析） group by -- 先按s_id分组,在对字段s_score求平均数 select s_id, avg(s_score) from myhive.score group by s_id; having having 与 where 不同点 where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据 where后面不能使用分组函数，而having后面可以使用分组函数 having只用于group by分组统计语句 select s_id, avg(s_score) as avgScore from myhive.score group by s_id having avgScore > 60; 连接 Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。 表的别名 使用别名可以简化查询 使用表名前缀可以提高执行效率 -- 创建表 create table myhive.course (c_id string, c_name string, t_id string) row format delimited fields terminated by '\\t'; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/course.csv' overwrite into table myhive.course; -- join查询 select * from myhive.teacher t join myhive.course c on t.t_id = c.t_id; 内连接 inner join（简写：join） 只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来 use myhive; select * from teacher t inner join course c on t.t_id = c.t_id; 左外连接 left outer join（left join） join操作符左边表中符合where子句的所有记录将会被返回 如果右边表的指定字段没有符合条件的值，就使用null值替代 use myhive; select * from teacher t left outer join course c on t.t_id = c.t_id; 右外连接 right outer join（right join） join操作符右边表中符合where子句的所有记录将会被返回 如果左边表的指定字段没有符合条件的值，就使用null值替代 use myhive; select * from teacher t right outer join course c on t.t_id = c.t_id; 满外连接 full outer join（full join） 将会返回所有表中符合where语句条件的所有记录 如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代 use myhive; select * from teacher t full outer join course c on t.t_id = c.t_id; 多表连接 use myhive; select * from teacher t left join course c on t.t_id = c.t_id left join score s on c.c_id = s.c_id left join stu1 on s.s_id = stu1.id; 排序 order by 全局排序 全局排序，只有一个reduce asc (ascend) 升序 （默认）、desc (descend) 降序 order by 子句在select语句的结尾 use myhive; -- 降序 select * from score s order by s_score desc; -- 聚合函数别名排序 select s_id, avg(s_score) avgscore from score group by s_id order by avgscore desc; sort by 局部排序 每个reducer内部进行排序，对全局结果集来说不是排序 use myhive; -- 设置参数 set mapreduce.job.reduces=3; -- 3个ReduceTask内部有序，而对于整个数据结果是无序的 select * from score s sort by s.s_score; distribute by + sort by 分区排序（MR） 类似MR中partition，采用hash算法，在map端将查询的结果中指定字段的hash值相同的结果分发到对应的reduce文件中 结合sort by使用 distribute by 语句要写在 sort by 语句之前 use myhive; set mapreduce.job.reduces=3; -- 期望3个分区文件，且内部有序 insert overwrite local directory '/home/hadoop/hivedatas/distribute' row format delimited fields terminated by '\\t' select * from score distribute by s_id sort by s_score; cluster by 桶排序 当 distribute by 和 sort by 字段相同时，可以使用 cluster by 方式代替 use myhive; insert overwrite local directory '/home/hadoop/hivedatas/cluster' row format delimited fields terminated by '\\t' select * from score cluster by s_score; 多维分析 grouping sets use game_center; select channel_id,role_sex,count(1) as total_person from ods_role_create group by channel_id,role_sex grouping sets((channel_id,role_sex),(channel_id),(role_sex)); +-------------+-----------+---------------+--+ | channel_id | role_sex | total_person | +-------------+-----------+---------------+--+ | NULL | 0 | 102692 | | NULL | 1 | 82487 | | 1 | NULL | 172018 | | 1 | 0 | 94226 | | 1 | 1 | 77792 | | 2 | NULL | 13161 | | 2 | 0 | 8466 | | 2 | 1 | 4695 | +-------------+-----------+---------------+--+ # 等价于 select channel_id,role_sex,count(1) as total_person from ods_role_create group by channel_id,role_sex union all select channel_id,null as role_sex,count(1) as total_person from ods_role_create group by channel_id union all select null as channel_id,role_sex,count(1) as total_person from ods_role_create group by role_sex; with cube use game_center; select channel_id,role_sex,count(1) as total_person from ods_role_create group by channel_id,role_sex with cube; +-------------+-----------+---------------+--+ | channel_id | role_sex | total_person | +-------------+-----------+---------------+--+ | NULL | NULL | 185179 | | NULL | 0 | 102692 | | NULL | 1 | 82487 | | 1 | NULL | 172018 | | 1 | 0 | 94226 | | 1 | 1 | 77792 | | 2 | NULL | 13161 | | 2 | 0 | 8466 | | 2 | 1 | 4695 | +-------------+-----------+---------------+--+ # 等价于 select channel_id,role_sex,count(1) as total_person from ods_role_create group by channel_id,role_sex grouping sets ((channel_id,role_sex),(channel_id),(role_sex),()); with rollup use game_center; # 从右至左递减维度分析 select channel_id,role_sex,count(1) as total_person from ods_role_create group by channel_id,role_sex with rollup; +-------------+-----------+---------------+--+ | channel_id | role_sex | total_person | +-------------+-----------+---------------+--+ | NULL | NULL | 185179 | | 1 | NULL | 172018 | | 1 | 0 | 94226 | | 1 | 1 | 77792 | | 2 | NULL | 13161 | | 2 | 0 | 8466 | | 2 | 1 | 4695 | +-------------+-----------+---------------+--+ # 等价于 select channel_id,role_sex,count(1) as total_person from ods_role_create group by channel_id,role_sex grouping sets ((channel_id,role_sex),(channel_id),()); 参数传递 配置方式 配置文件 hive-site.xml 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml 默认配置文件：$HIVE_CONF_DIR/hive-default.xml 用户自定义配置会覆盖默认配置 对本机启动的所有hive进程有效 命令行参数 启动hive客户端的时候可以设置参数 --hiveconf param=value 对本次启动的Session有效（对于Server方式启动，则是所有请求的Sessions） 参数声明 进入客户端以后设置的一些参数 set set param=value; 作用域是session级的 上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。 使用变量传递参数 hive0.9以及之前的版本不支持传参 hive1.0版本之后支持 hive -f 传递参数 在hive当中我们一般可以使用 hivevar 或 hiveconf 来进行参数的传递 hiveconf hive 执行上下文的属性（配置参数），可覆盖覆盖hive-site.xml（hive-default.xml）中的参数值，如用户执行目录、日志打印级别、执行队列等。 传值 hive --hiveconf key=value 使用 # 使用hiveconf作为前缀 ${hiveconf:key} hivevar hive 运行时的变量 替换 传值 hive --hivevar key=value 使用 # 使用hivevar前缀 ${hivevar:key} # 不使用前缀 ${key} define define与hivevar用途完全一样，简写 -d 自定义函数 开发程序 需要继承UDF类，同时需要提供evaluate方法，由hive框架反射调用用户自定义逻辑 import org.apache.hadoop.hive.ql.exec.UDF; import org.apache.hadoop.io.Text; public class UpCaseUDF extends UDF { // 默认情况下UDF需要提供evaluate方法，hive默认调用 public Text evaluate(Text text) { if (text == null) return null; return new Text(text.toString().toUpperCase()); } } 程序发布 上传程序到hive的任意目录 scp hadoop-hive-example-1.0-SNAPSHOT.jar hadoop@node03:/home/hadoop 使用自定义函数 临时函数 注册临时函数只对当前session有效 -- 向hive客户端添加jar，只对当前session有效 add jar /home/hadoop/hadoop-hive-example-1.0-SNAPSHOT.jar; -- 注册自定义函数 create temporary function myupcase as 'com.sciatta.hadoop.hive.example.func.UpCaseUDF'; -- 查看是否注册成功 show functions like 'my*'; -- 测试 select myupcase('abc'); 永久函数 use myhive; -- 只对当前session有效 -- 在hive-site.xml配置hive.aux.jars.path使得jar永久有效 add jar /home/hadoop/hadoop-hive-example-1.0-SNAPSHOT.jar; -- 查看添加的jar list jars; -- 注册永久函数 create function myupcase as 'com.sciatta.hadoop.hive.example.func.UpCaseUDF'; -- 测试 select myupcase('abc'); -- 退出后检查函数是否仍然存在 show functions like 'my*'; -- 删除永久函数 drop function myupcase; SerDe Serde是Serializer/Deserializer的简写。hive使用Serde进行行对象的序列化与反序列化。最后实现把文件内容映射到 hive 表中的字段数据类型。 Hive 中内置org.apache.hadoop.hive.serde2库，内部封装了很多不同的SerDe类型。hive创建表时， 通过自定义的SerDe或使用Hive内置的SerDe类型指定数据的序列化和反序列化方式。 -- 使用 ROW FORMAT 参数说明SerDe的类型 -- 如果 ROW FORMAT 没有指定 或者 指定了 ROW FORMAT DELIMITED就会使用native Serde CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] SerDe类型 Avro (Hive 0.9.1 and later) ORC (Hive 0.11 and later) RegEx Thrift Parquet (Hive 0.13 and later) CSV (Hive 0.14 and later) MultiDelimitSerDe 数据压缩 Map输出阶段压缩 减少MapTask和ReduceTask间的数据传输量 -- 开启hive中间传输数据压缩功能 set hive.exec.compress.intermediate=true; -- 开启mapreduce中map输出压缩功能 set mapreduce.map.output.compress=true; -- 设置mapreduce中map输出数据的压缩方式 set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec; Reduce输出阶段压缩 -- 开启hive最终输出数据压缩功能 set hive.exec.compress.output=true; -- 开启mapreduce最终输出数据压缩 set mapreduce.output.fileoutputformat.compress=true; -- 设置mapreduce最终数据输出压缩方式 set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec; -- 设置mapreduce最终数据输出压缩为块压缩 set mapreduce.output.fileoutputformat.compress.type=BLOCK; 文件存储格式 存储方式 Hive支持的存储数据的格式主要有： 行式存储 TEXTFILE 默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用（系统自动检查，执行查询时自动解压），但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。 SEQUENCEFILE 列式存储 ORC Orc（Optimized Row Columnar）是hive 0.11版里引入的新的存储格式。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。 一个orc文件可以分为若干个Stripe，一个stripe可以分为三个部分 Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引只是记录某行的各字段在Row Data中的offset。 Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。 Stripe Footer：存的是各个stripe的元数据信息。 每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等。 每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。 PARQUET Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。 行式存储的特点： 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。 列式存储的特点： 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 主流文件存储格式对比实验 测试文件：19M log.data 压缩比 TEXTFILE -- 创建表 use myhive; create table log_text ( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE; -- 加载数据 load data local inpath '/home/hadoop/hivedatas/log.data' into table log_text; 通过命令 hdfs dfs -du -h /user/hive/warehouse/myhive.db/log_text/log.data 查看 18.1 M ORC 默认使用zlib压缩 -- 创建表 create table log_orc ( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS ORC; -- 加载数据 insert into table log_orc select * from log_text; 通过命令 hdfs dfs -du -h /user/hive/warehouse/myhive.db/log_orc/000000_0 查看 2.8 M PARQUET -- 创建表 create table log_parquet ( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS PARQUET; -- 加载数据 insert into table log_parquet select * from log_text; 通过命令 hdfs dfs -du -h /user/hive/warehouse/myhive.db/log_parquet/000000_0 查看 13.1 M 总结：ORC > PARQUET > TEXTFILE 查询速度 -- 1 row selected (13.533 seconds) select count(*) from log_text; -- 1 row selected (13.03 seconds) select count(*) from log_orc; -- 1 row selected (14.266 seconds) select count(*) from log_parquet; 总结：ORC > TEXTFILE > PARQUET ORC结合压缩对比试验 ORC支持三种压缩 ZLIB，SNAPPY 和 NONE。默认是 ZLIB。 ORC存储方式的压缩： Key Default Notes orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY) orc.compress.size 262,144 number of bytes in each compression chunk orc.stripe.size 67,108,864 number of bytes in each stripe orc.row.index.stride 10,000 number of rows between index entries (must be >= 1000) orc.create.index true whether to create row indexes orc.bloom.filter.columns \"\" comma separated list of column names for which bloom filter should be created orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must >0.0 and 非压缩 -- 建表 create table log_orc_none( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS orc tblproperties (\"orc.compress\"=\"NONE\"); -- 导入数据 insert into table log_orc_none select * from log_text; 占用空间：7.7 M SNAPPY压缩 -- 建表 create table log_orc_snappy( track_time string, url string, session_id string, referer string, ip string, end_user_id string, city_id string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS orc tblproperties (\"orc.compress\"=\"SNAPPY\"); -- 导入数据 insert into table log_orc_snappy select * from log_text; 占用空间：3.8 M ZLIB（默认） 占用空间：2.8 M 数据存储和压缩 在实际的项目开发当中，hive表的数据存储格式一 般选择：orc 或 parquet。压缩方式一般选择 snappy。 数据仓库的拉链表 什么是拉链表 拉链表是针对数据仓库设计中表存储数据的方式而定义的，顾名思义，所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有变化的信息。 在数据分析中，有时会需要维护一些历史状态，比如订单状态变化、评分变化等，为了保存下来这些状态变化的路径，可以通过拉链表实现。 使用场景 拉链表主要用于解决数仓当中一些缓慢变化维的数据，需要保存历史各个版本 数据量比较大，但业务要求每次需要查询全量历史，每天存储一份全量数据太占用存储空间 记录变更不大，比如只有状态和更新时间有变动，其它字段都不变 实现过程 在记录末尾增加start_date和end_date字段来实现。start_date表示该条记录的生命周期开始时间，end_date表示该条记录的生命周期结束时间。 同一ID按时间排序后，如果有较新的记录，则当前记录的end_date等于较新记录的start_date-1；如果没有较新的记录，则当前记录的end_date等于一个默认值，比如9999-12-31，表示该条记录目前处于有效状态。 如果查询当前所有有效的记录，则 select * from user where end_date = '9999-12-31'。每一个交易记录应该只有唯一一条有效记录。 如果查询2017-01-02的历史快照（当天的交易订单情况，每一个交易应只有唯一记录），则 select * from user where start_date = '2017-01-02'。（注意特殊日期9999-12-31） start_date 大于2017-01-02，还没有产生交易 等于2017-01-02，当天开始生命周期，可能是此日期创建交易，或有做过变更 小于2017-01-02，快照之前就开始生命周期 end_date >= '2017-01-02' ， 小于2017-01-02，快照之前已经结束生命周期，不需要展示 等于2017-01-02，在此日期结束生命周期 大于2017-01-02，可能不是最新的记录，如 end_date = '2017-01-09' ，必然会有至少一条记录的 start_date = '2017-01-10' ，此记录需要展示，也就是说在2017-01-02此交易状态未变化；也可能是最新记录，则此时 end_date = '9999-12-31' 符合规则举例 | start_date | end_date | 说明 | | ---------- | ---------- | ------------------------------------------------------------ | | 2017-01-01 | 2017-01-02 | 1月1日开始生命周期，1月2日结束生命周期 | | 2017-01-01 | 2017-01-09 | 1月1日开始生命周期，1月2日状态未变化；但1月9日结束生命周期，但不影响快照展示 | | 2017-01-01 | 9999-12-31 | 1月1日开始生命周期，1月2日状态未变化，是最新记录 | 01月02日开始生命周期情况，也有如上三种情况，分析方法相同。 举例 以订单表为例 只考虑实现，不考虑性能 时间粒度：天 day；即每一个订单每天最多一条最新记录 建模之前需要按照Kimball思想“四步走”战略 准备数据 每一天的增量数据（modifiedtime作为条件）可以由sqoop导入到hive表中 orderid createtime modifiedtime status 2016-08-20 1 2016-08-20 2016-08-20 创建 2 2016-08-20 2016-08-20 创建 3 2016-08-20 2016-08-20 创建 2016-08-21 1 2016-08-20 2016-08-21 支付 2 2016-08-20 2016-08-21 完成 4 2016-08-21 2016-08-21 创建 2016-08-22 1 2016-08-20 2016-08-22 完成 3 2016-08-20 2016-08-22 支付 4 2016-08-21 2016-08-22 支付 5 2016-08-22 2016-08-22 创建 创建hive表 create database if not exists chain_action; use chain_action; -- 临时表，导入每一天的增量数据 drop table if exists ods_orders_tmp; CREATE TABLE ods_orders_tmp ( orderid INT, createtime STRING, modifiedtime STRING, status STRING ) row format delimited fields terminated by '\\t'; -- 全量表，按天分区 drop table if exists ods_orders_inc; CREATE TABLE ods_orders_inc ( orderid INT, createtime STRING, modifiedtime STRING, status STRING ) PARTITIONED BY (day STRING) row format delimited fields terminated by '\\t'; -- 拉链表，保存全量数据 drop table if exists dw_orders_his; CREATE TABLE dw_orders_his ( orderid INT, createtime STRING, modifiedtime STRING, status STRING, start_date STRING, end_date STRING ) row format delimited fields terminated by '\\t'; 数据脚本 chain_action.sh #!/bin/bash HIVEBIN=/bigdata/install/hive-1.1.0-cdh5.14.2/bin/hive if [ -n \"$1\" ] ; then import_date=$1 else echo \"请指定导入日期\" exit 1 fi sql=\" use chain_action; set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.exec.mode.local.auto=true; set hive.exec.mode.local.auto.inputbytes.max=262144; set hive.exec.mode.local.auto.input.files.max=5; -- 临时表加载数据，只保存当前导入的数据 load data local inpath '/home/hadoop/hivedatas/order_chain/${import_date}' overwrite into table ods_orders_tmp; -- 全量表导入，按天分区 -- 包括：1、当天创建；2、当天修改之前创建的数据 insert overwrite table ods_orders_inc partition (day = '${import_date}') select orderid, createtime, modifiedtime, status from ods_orders_tmp where (createtime = '${import_date}' and modifiedtime = '${import_date}') OR modifiedtime = '${import_date}'; -- 导入拉链表前，先导入到拉链表临时表 drop table if exists dw_orders_his_tmp; -- 计算拉链表临时表（dw_orders_his + ods_orders_inc） -- union all 的第一个查询（作用是更新拉链表已有数据并且匹配增量数据，更新拉链表最后一条记录的声明周期结束日期），以dw_orders_his为准，和ods_orders_inc增量数据匹配，如果匹配表明数据被修改，因此dw_orders_his最后一条数据的end_date更新为导入日期-1（生命周期结束），不是最后一条数据的仍为原来日期；如果不匹配，即增量数据没有修改交易，则仍为原来日期 -- union all 的第二个查询（作用是更新最新交易生命周期开始和结束日期为最新交易记录），以ods_orders_inc为准，因为全部都是最新数据（有效期范围数据），所以end_date更新为9999-12-31，start_date为当天修改日期作为生命周期的开始日期 CREATE TABLE dw_orders_his_tmp AS SELECT orderid, createtime, modifiedtime, status, start_date, end_date FROM ( SELECT a.orderid, a.createtime, a.modifiedtime, a.status, a.start_date, CASE WHEN b.orderid IS NOT NULL AND a.end_date > '${import_date}' THEN date_add(b.modifiedtime,-1) ELSE a.end_date END AS end_date FROM dw_orders_his a left outer join (SELECT * FROM ods_orders_inc WHERE day = '${import_date}') b ON (a.orderid = b.orderid) UNION ALL SELECT orderid, createtime, modifiedtime, status, modifiedtime AS start_date, '9999-12-31' AS end_date FROM ods_orders_inc WHERE day = '${import_date}' ) x ORDER BY orderid, start_date; -- 灌入拉链表 INSERT overwrite TABLE dw_orders_his SELECT * FROM dw_orders_his_tmp; \" $HIVEBIN -e \"$sql\" 执行脚本 # 2016-08-20 sh chain_action.sh 2016-08-20 # 2016-08-21 sh chain_action.sh 2016-08-21 # 2016-08-22 sh chain_action.sh 2016-08-22 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hive/Hive调优.html":{"url":"src/bigdata/hive/Hive调优.html","title":"Hive调优","keywords":"","body":"Fetch抓取 Fetch抓取是指Hive中对某些情况的查询可以不必使用MapReduce计算。这些情况包括： 全局查找 select * from score; 字段查找 select s_id from score; 限制查找 select s_id from score limit 3; 由参数 hive.fetch.task.conversion 控制，老版本默认是 minimal，新版本默认是 more。当参数值为 more时，以上查询不使用MapReduce，直接读取存储目录下的文件。 而当 set hive.fetch.task.conversion=none; 时，以上查询会使用MapReduce。 本地模式 在Hive客户端测试时，默认情况下是启用hadoop的job模式把任务提交到集群中运行，这样会导致计算非常缓慢；可以通过本地模式在单台机器上处理任务。对于小数据集，执行时间可以明显被缩短。 -- 开启本地运行模式，并执行查询语句 set hive.exec.mode.local.auto=true; -- 设置local mr的最大输入数据量，当输入数据量小于这个值时采用 local mr 的方式 -- 默认为134217728，即128M set hive.exec.mode.local.auto.inputbytes.max=50000000; -- 设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式 -- 默认为4 set hive.exec.mode.local.auto.input.files.max=5; -- 执行查询的sql语句 select * from teacher cluster by t_id; -- 关闭本地运行模式 set hive.exec.mode.local.auto=false; 表优化 小表和大表 join 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率。新版的hive已经对小表 join 大表和大表 join 小表进行了优化。小表放在左边和右边已经没有明显区别 可以使用map join让小的维度表（1000条以下的记录条数）先加载内存，在map端完成join操作 -- 开启mapjoin参数（默认是true） set hive.auto.convert.join = true; -- 大小表阈值 set hive.mapjoin.smalltable.filesize=26214400; 多个表关联时，最好分拆成小段，避免大sql（无法控制中间Job） 大表 join 大表 空 key 过滤 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够 如果是由于异常数据key造成，可以过滤异常数据 空 key 转换 有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时可以为表中key为空的字段赋一个随机值，使得数据随机均匀地分不到不同的reducer上 group by 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就会发生数据倾斜 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果 开启Map端聚合参数设置 -- 是否在Map端进行聚合，默认为True set hive.map.aggr = true; -- 在Map端进行聚合操作的条目数目 set hive.groupby.mapaggr.checkinterval = 100000; -- 有数据倾斜的时候进行负载均衡（默认是false） -- 当选项设定为true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。 set hive.groupby.skewindata = true; count(distinct) 数据量小的时候无所谓，数据量大的情况下，由于count distinct 操作需要用一个reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般count distinct使用，先group by再count的方式替换 select count(ip) from (select ip from log_text group by ip) t; 笛卡尔积 尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件 Hive只能使用1个reducer来完成笛卡尔积 使用分区剪裁、列剪裁 尽可能早地过滤掉尽可能多的数据量，避免大量数据流入外层SQL。 列剪裁 只获取需要的列的数据，减少数据输入。 分区裁剪 分区在hive实质上是目录，分区裁剪可以方便直接地过滤掉大部分数据。 尽量使用分区过滤 并行执行 把一个sql语句中没有相互依赖的阶段并行去运行。提高集群资源利用率。 -- 开启并行执行 set hive.exec.parallel=true; -- 同一个sql允许最大并行度，默认为8 set hive.exec.parallel.thread.number=16; 严格模式 Hive提供了严格模式，可以防止用户执行效率极差的查询。 -- 设置非严格模式（默认） set hive.mapred.mode=nonstrict; -- 设置严格模式 set hive.mapred.mode=strict; 开启严格模式可以禁止3种类型的查询： 对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行 对于使用了order by语句的查询，要求必须使用limit语句 限制笛卡尔积的查询 JVM重用 JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。 Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。 可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。 mapreduce.job.jvm.numtasks 10 How many tasks to run per jvm. If set to -1, there is no limit. 也可以在hive当中设置 set mapred.job.reuse.jvm.num.tasks=10; 这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个Reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。 推测执行 在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。 可以在Hadoop的mapred-site.xml文件中进行配置。 mapreduce.map.speculative true If true, then multiple instances of some map tasks may be executed in parallel. mapreduce.reduce.speculative true If true, then multiple instances of some reduce tasks may be executed in parallel. 注意，如果用户输入数据本身很大，需要长时间执行MapTask或者ReduceTask。如果开启此参数，可能会造成更多资源的浪费。 压缩 使用压缩的优势是可以最小化所需要的磁盘存储空间，以及减少磁盘和网络io操作 Hive表中间数据压缩 -- 开启hive中间传输数据压缩功能 set hive.exec.compress.intermediate=true; -- 开启mapreduce中map输出压缩功能 set mapreduce.map.output.compress=true; -- 设置mapreduce中map输出数据的压缩方式 set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec; Hive表最终输出结果压缩 -- 开启hive最终输出数据压缩功能 set hive.exec.compress.output=true; -- 开启mapreduce最终输出数据压缩 set mapreduce.output.fileoutputformat.compress=true; -- 设置mapreduce最终数据输出压缩方式 set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec; -- 设置mapreduce最终数据输出压缩为块压缩 set mapreduce.output.fileoutputformat.compress.type=BLOCK; 使用EXPLAIN（执行计划） hive将sql解释为多个MapReduce，可以通过explain查看执行计划，了解MapReduce的执行顺序。大致顺序是 from... where... select... group by... having... order by... EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query 合理设置MapTask数量 MapTask数量的决定因素 文件个数 文件大小 可以通过 computeSliteSize(Math.max(minSize, Math.min(maxSize, blocksize))) 来调整切片大小，继而调整MapTask数量。 -- minsize（切片最小值）参数调的比blockSize大，则可以让切片变大，MapTask数量变少 set mapreduce.input.fileinputformat.split.minsize=1; -- maxsize（切片最大值）参数如果调到比blocksize小，则可以让切片变小，MapTask数量变多 set mapreduce.input.fileinputformat.split.maxsize=256000000; MapTask数越多越好？ 如果一个job有大量小文件，则每个小文件也会被当做一个块，用一个MapTask来完成，而一个MapTask启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的MapTask数是受限的。 解决 减少MapTask数 JVM重用 小文件合并 set mapred.max.split.size=112345600; set mapred.min.split.size.per.node=112345600; set mapred.min.split.size.per.rack=112345600; set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 每个MapTask处理接近128m的文件块，就是最优的？ 如果字段少，记录多，而且Map逻辑复杂，用一个MapTask处理肯定是比较耗时的 解决 增大MapTask数 合理设置ReduceTask数量 调整ReduceTask数量的方法 通过公式调整 -- 参数1：每个Reduce处理的数据量默认是256MB set hive.exec.reducers.bytes.per.reducer=256000000; -- 参数2：每个任务最大的reduce数，默认为1009 set hive.exec.reducers.max=1009; -- N 为ReduceTask数量 N=min(参数2，总输入数据量/参数1) 设置ReduceTask数量 -- 设置每一个job中reduce个数 set mapreduce.job.reduces=3; ReduceTask数越多越好？ 过多的启动和初始化reduce会消耗时间和资源 同时过多的reduce会生成很多个文件，也有可能出现小文件问题 合并小文件 在Map-only的任务结束时合并小文件 set hive.merge.mapfiles = true 在Map-Reduce的任务结束时合并小文件 set hive.merge.mapredfiles = true 合并文件大小 set hive.merge.size.per.task = 256*1000*1000 输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件合并 set hive.merge.smallfiles.avgsize=16000000 排序 对于排序问题，能够不用全局排序就一定不要使用全局排序order by（只有一个ReduceTask，效率低），如果一定要使用order by一定要加上limit。 MapTask尽量多处理数据 能够在MapTask处理完成的任务，尽量在MapTask多处理任务，避免数据通过shuffle到ReduceTask，通过网络拷贝导致性能低下。 map aggr map join 尽量减少IO操作 多表插入 Hive支持多表插入，可以在同一个查询中使用多个insert子句，这样的好处是我们只需要扫描一遍源表就可以生成多个不相交的输出。可以减少表的扫描，从而减少 JOB 中 MR的 STAGE 数量，达到优化的目的。 -- 多表插入的关键点在于将所要执行查询的表语句 \"from 表名\"，放在最开头位置 from test1 insert overwrite table test2 partition (age) select name,address,school,age insert overwrite table test3 select name,address 一次计算，多次使用 提高代码可读性，简化SQL 一次分析，多次使用 -- with as就类似于一个视图或临时表，可以用来存储一部分的sql语句作为别名，不同的是 with as 属于一次性的，而且必须要和其他sql一起使用才可以 WITH t1 AS ( SELECT * FROM carinfo ), t2 AS ( SELECT * FROM car_blacklist ) SELECT * FROM t1, t2 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hbase/HBase集群安装部署.html":{"url":"src/bigdata/hbase/HBase集群安装部署.html","title":"HBase集群安装部署","keywords":"","body":"先决条件 安装对应版本的hadoop集群并启动 安装对应版本的zookeeper集群并启动 HBase HA 服务规划 IP HMaster 备份HMaster HRegionServer 192.168.1.100 node01 √ √ 192.168.1.101 node02 √ √ 192.168.1.102 node03 √ 安装 # 上传压缩包到node01 scp hbase-1.2.0-cdh5.14.2.tar.gz hadoop@192.168.2.100:/bigdata/soft # 解压缩 tar -xzvf hbase-1.2.0-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置 hbase-env.sh cd /bigdata/install/hbase-1.2.0-cdh5.14.2/conf vi hbase-env.sh export JAVA_HOME=/bigdata/install/jdk1.8.0_141 export HBASE_MANAGES_ZK=false hbase-site.xml vi hbase-site.xml hbase.rootdir hdfs://node01:8020/hbase hbase.cluster.distributed true hbase.master.port 16000 hbase.zookeeper.quorum node01,node02,node03 hbase.zookeeper.property.clientPort 2181 hbase.zookeeper.property.dataDir /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas zookeeper.znode.parent /hbase regionservers vi regionservers 指定HBase集群的从节点；原内容清空，添加如下三行 node01 node02 node03 back-masters 创建back-masters配置文件，包含备份HMaster节点的主机名，每个机器独占一行，实现HMaster的高可用 vi backup-masters 将node02作为备份的HMaster节点 node02 分发 cd /bigdata/install scp -r hbase-1.2.0-cdh5.14.2/ node02:$PWD scp -r hbase-1.2.0-cdh5.14.2/ node03:$PWD 创建软连接 注意：三台机器均做如下操作 因为HBase集群需要读取hadoop的core-site.xml、hdfs-site.xml的配置文件信息，所以我们三台机器都要执行以下命令，在相应的目录创建这两个配置文件的软连接 ln -s /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/core-site.xml /bigdata/install/hbase-1.2.0-cdh5.14.2/conf/core-site.xml ln -s /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/hdfs-site.xml /bigdata/install/hbase-1.2.0-cdh5.14.2/conf/hdfs-site.xml 添加HBase环境变量 注意：三台机器均做如下操作 sudo vi /etc/profile 文件末尾添加如下内容 export HBASE_HOME=/bigdata/install/hbase-1.2.0-cdh5.14.2 export PATH=$PATH:$HBASE_HOME/bin 重新编译/etc/profile使环境变量立即生效 source /etc/profile HBase启动和停止 启动 # node01 执行 # 启动 hdfs start-dfs.sh # node01 node02 node03 分别执行 # 启动 zookeeper # 检查 zkServer.sh status zkServer.sh start # node01 执行 # 启动 hbase start-hbase.sh 停止 # node01执行 # 停止 hbase stop-hbase.sh # node01 node02 node03 分别执行 # 停止 zookeeper zkServer.sh stop # node01 执行 # 停止 hdfs stop-dfs.sh 访问web页面 http://node01:60010 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/hbase/大数据数据库之HBase.html":{"url":"src/bigdata/hbase/大数据数据库之HBase.html","title":"大数据数据库之HBase","keywords":"","body":"核心概念 概念 HBase基于Google的BigTable论文，是建立的HDFS之上，提供高可靠性、高性能、列存储、可伸缩、实时读写的分布式数据库系统。 在需要实时读写随机访问超大规模数据集时，可以使用HBase。 特点 海量存储 可以存储大批量的数据 列式存储 HBase表的数据是基于列族进行存储的，列族是在列的方向上的划分 极易扩展 底层依赖HDFS，当磁盘空间不足的时候，只需要动态增加DataNode节点就可以了 可以通过增加服务器来对集群的存储进行扩容 高并发 支持高并发的读写请求 稀疏 稀疏主要是针对HBase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的 数据的多版本 HBase表中的数据可以有多个版本值，默认情况下是根据版本号去区分，版本号就是插入数据的时间戳 数据类型单一 所有的数据在HBase中是以字节数组进行存储 数据模型 rowkey 行键 table的主键，table中的记录按照rowkey的字典序进行排序 rowkey可以是任意字符串，最大长度是 64KB，实际应用中长度一般为 10-100bytes。 column family 列族 HBase表中的每个colum都归属于某个column family column family是表的schema的一部分（而column不是），即建表时至少指定一个列族 timestamp 时间戳 可以对表中的cell多次赋值，每次赋值操作的timestamp可看成cell值的版本号version numbe 一个cell可以有多个版本的值 column qualifier 列限定 列限定是表的某一列族下的一个列，用 列族名:列名 表示 属于某一个column family，类似于mysql当中创建的具体的列 cell 单元格 根据 {rowkey, column(=family+qualifier), timestamp} 可以映射到一个对应的cell，cell是HBase存储数据的具体地址 cell中的数据是没有类型的，全部是以字节数组进行存储 整体架构 Client Client是操作HBase集群的入口 对于管理类的操作，如表的增、删、改，Client通过RPC与HMaster通信完成 对于表数据的读写操作，Client通过RPC与HRegionServer通信读写数据 Client类型 HBase shell Java编程接口 Thrift、Avro、Rest等 ZooKeeper集群 实现了HMaster的高可用，多HMaster间进行主备选举 保存了HBase的元数据信息meta表，提供了HBase表中region寻址入口数据 对HMaster和HRegionServer实现了监控 ZooKeeper如何协调HMaster和HRegionServer工作？ 多HMaster会竞争创建ephemeral节点，而Zookeeper决定谁是第一个作为在线的HMaster，保证线上只有一个 HMaster。在线HMaster（active HMaster）会给Zookeeper发送心跳，不在线的待机HMaster（inactive HMaster）会监听active HMaster可能出现的故障并随时准备上线。 每个HRegionServer都会创建一个ephemeral 节点。HMaster会监控这些节点来发现可用的HRegionServer，同样它也会监控这些节点是否出现故障。 如果有一个HRegionServer或者HMastet出现故障或各种原因导致发送心跳失败，它们与Zookeeper的session就会过期，这个ephemeral节点就会被删除下线，监听者们就会收到这个消息。Active HMaster监听的是HRegionServer下线的消息，然后会恢复故障的HRegionServer以及它所负责的Region数据。而Inactive HMaster关心的则是active HMaster下线的消息，然后竞争上线变成active HMaster。 HMaster HBase集群也是主从架构，HMaster是主角色 负责Table的管理工作，管理Client对Table的增删改操作 负责Region的管理工作 在Region分裂后，负责将新Region分配到指定的HRegionServer 管理HRegionServer间的负载均衡，迁移region分布 当HRegionServer宕机后，负责将其上的region迁移 监控集群中所有HRegionServer（从Zookeeper获取通知信息） HRegionServer HRegionServer是从角色，负责管理一系列的Region（大约可以管理1000个Region） 响应Client的读写数据请求，通常情况下HRegionServer同DataNode同机部署，这样就可以实现数据的本地化 切分在运行过程中变大的Region Region HBase集群中分布式存储的最小单元 一个Region对应一个Table表的部分数据，也可以是全部数据；Table表根据rowkey的范围被水平拆分为多个Region，每个Region都包含了这个Region的start key和end key之间的所有行，然后被分配给集群中的HRegionServer来管理 HBase shell 命令 # 进入 hbase shell 交互客户端 hbase shell 基本命令 help 显示帮助 # 显示帮助 help # 查看具体命令的帮助 help 'create' exit 退出 # 退出shell exit # 当输入语法错误，导致client不工作 >` create 创建表 # 创建user表，info和data列族 # 默认一个版本 create 'user', 'info', 'data' # 创建user1表，同时指定列族的版本数 create 'user1', {NAME => 'info', VERSIONS => 3}, {NAME => 'data'} put 插入和更新单行数据 # 插入数据 # row key为rk0001，列族info中添加名为name的列，值为zhangsan # column qualifier 在插入数据时指定 put 'user', 'rk0001', 'info:name', 'zhangsan' put 'user', 'rk0001', 'info:gender', 'female' put 'user', 'rk0001', 'info:age', 20 put 'user', 'rk0001', 'data:pic', 'picture' put 'user', 'rk0002', 'info:name', 'fanbingbing' put 'user', 'rk0002', 'info:gender', 'female' put 'user', 'rk0002', 'info:nationality', '中国' # 更新数据 # cell有数据则更新，无数据则插入；本质还是插入数据，只不过查询数据时，显示最新版本号的数据 put 'user', 'rk0001', 'info:name', 'lisi' get 查询单行数据 # 获取user表中row key为rk0001的所有 cell 信息 get 'user', 'rk0001' # 过滤列族 get 'user', 'rk0001', 'info' get 'user', 'rk0001', 'info', 'data' get 'user', 'rk0001', {COLUMN => ['info', 'data']} # 过滤列 get 'user', 'rk0001', 'info:age' get 'user', 'rk0001', 'info:age', \"info:name\" get 'user', 'rk0001', {COLUMN => ['info:name', 'data:pic']} # 过滤值 get 'user', 'rk0001', {FILTER => \"ValueFilter(=, 'binary:zhangsan')\"} # 过滤值包含中文 get 'user', 'rk0002', {FILTER => \"ValueFilter(=, 'binary:中国')\"} # 过滤列限定名称 # 列名称包含a get 'user', 'rk0001', {FILTER => \"QualifierFilter(=,'substring:a')\"} scan 全表扫描 scan 'user' 全表扫描 scan 'user', {COLUMNS => 'info', STARTROW => 'rk0001', ENDROW => 'rk0003'} 范围扫描 # 查看user表的所有数据 # 全表扫描 scan 'user' # 过滤列族 scan 'user', {COLUMNS => 'info'} # RAW => true 已被标记为删除，但还没有被删除 # 不能包含列限定 scan 'user', {COLUMNS => 'info', RAW => true, VERSIONS => 5} scan 'user', {COLUMNS => ['info', 'data']} # 过滤列 scan 'user', {COLUMNS => 'info:name'} scan 'user', {COLUMNS => ['info:name', 'data:pic']} # 版本最新的5个 scan 'user', {COLUMNS => 'info:name', VERSIONS => 5} # 过滤列限定名称 scan 'user', {COLUMNS => ['info', 'data'], FILTER => \"(QualifierFilter(=,'substring:a'))\"} # 指定行键范围 [rk0001, rk0003) # 范围匹配，范围 -> Region scan 'user', {COLUMNS => 'info', STARTROW => 'rk0001', ENDROW => 'rk0003'} # row key 模糊查询 scan 'user', {FILTER => \"PrefixFilter('rk')\"} # 指定时间戳范围 [1581477006014, 1581477079033) scan 'user', {TIMERANGE => [1581477006014, 1581477079033]} delete 删除数据 # 注意当有多个版本时，自上向下逐个删除（逻辑删除，标记类型为Delete）；如一个cell有三个版本：ts3、ts2、ts1，第一次删除ts3版本，get查询时会显示ts2版本的数据 # 可以通过 RAW => true 查询标记为删除，但没有被物理删除的数据 delete 'user', 'rk0001', 'info:name' # 指定删除版本号 delete 'user', 'rk0001', 'info:name', 1581489923991 管理命令 status 显示服务器状态 status 'node01' whoami 显示当前用户 whoami list 显示数据库 list count 统计表行数据 count 'user' describe 显示表结构信息 describe 'user' exists 显示表是否存在 exists 'user' is_enabled 显示表是否启用 / is_disabled 显示表是否禁用 # 显示表是否启用 is_enabled 'user' # 显示表是否禁用 is_disabled 'user' alter 修改表结构 # 增加列族 alter 'user', NAME => 'data' # 等价 alter 'user', 'data' # 删除指定列族 alter 'user', NAME => 'data', METHOD => 'delete' alter 'user', 'delete' => 'data' # 修改列族 # 列族info版本数修改为5 alter 'user', NAME => 'info', VERSIONS => 5 enable 启用表 / disable 禁用表 # 禁用表 disable 'user' # 启用表 enable 'user' drop 删除表 # 先禁用，再删除 disable 'user1' drop 'user1' truncate 清空表数据 # 禁用表 -> 删除表 -> 创建表 truncate 'user' 核心原理 HBase数据存储原理 Region 一个HRegionServer负责管理多个Region 初始情况下，一个表只有一个Region或直接对表预分区。当随着数据增大，一个Region会分裂为两个Region 一个Region只对应一个表，而一个表可以有多个Region Store 一个Region包含多个Store，而一个column family对应一个Store 如果一个表中只有一个column family，那么Region中只会有一个Store；如果一个表有N个column family，那么Region中会有N个Store 为什么column family不应设置过多，最少一个，最多不超过两个？ 当列族过多且数据不均或均匀时，Region分裂，不同的列族会分裂到多个Region上造成某一列族数据过少，导致查询此列族上的数据可能会跨越多个Region，查询效率降低。 每一个列族都有一个MemoStore，导致内存消耗过多。 当一个Region中的Store刷写缓存或压缩时，其他Store会跟着一同操作，导致IO频繁。 MemStore 一个Store仅包含一个MemStore 写缓存。在写入数据时，会先写入MemStore缓存，当MemoStore达到某一个阈值后，会把数据刷写到磁盘 HFile 一个Store包含多个HFile。其中，StoreFile是HFile的抽象，最后是以HFile数据结构（有序KeyValue）存储在HDFS上 当每次MemStore超过一个阈值时，就会溢写到磁盘，对应就是生成一个HFile文件 HBase读数据流程 Client与Zookeeper连接，获取Meta表的位置信息，即Meta表存储在哪一个HRegionServer上。 HBase集群只有一张Meta表（B Tree），此表只有一个Region。保存了系统中所有Region的位置信息。结构如下： Key：table, region start key, region id Value：region server 可以通过 scan 'hbase:meta' 来查看Meta表信息 Client与Meta表所在的HRegionServer连接，进而获取请求rowkey所在Region的位置信息。 在Client缓存Meta表的位置信息，以及rowkey所在Region的位置信息，后续请求直接使用Meta Cache即可。除非Region迁移导致缓存失效，则需要重新获取相关位置信息并更新Client的Meta Cache。 Client同rowkey所在Region的HRegionServer连接，查找并定位所在的Region。首先在MemStore查找数据；如果没有，再从BlockCache上查找；如果没有，再到HFile上进行查找。 MemStore 写缓存 —— 写入MemoStore，但还未刷写到磁盘 BlockCache 读缓存，是 LRU（Least Recently Used）缓存 —— 已刷写到磁盘，近期读取过 HFile 数据持久化 —— 近期未读取，从持久化数据文件读取，同时放到BlockCache一份 从HFile读取到数据后，先写入到BlockCache中加快后续查找，然后再将结果返回给Client。 HBase写数据流程 Client与Zookeeper连接，获取Meta表的位置信息，即Meta表存储在哪一个HRegionServer上。 Client与Meta表所在的HRegionServer连接，进而获取请求rowkey所在Region的位置信息。（写/更新数据，需要找到所在region的startkey范围） Client同rowkey所在Region的HRegionServer连接，查找并定位所在的Region。首先在HLog上预写日志，然后写入MemStore缓存。 HLog也称为WAL，意为Write ahead log。类似mysql中的binlog，用来做灾难恢复时用，HLog记录数据的所有变更，相当于MemStore的一份快照。一旦MemStore数据丢失，就可以从HLog中恢复。 预写日志和写入MemStore的顺序不可调换，否则内存数据一旦丢失将无法恢复。 HLog以SequenceFile的形式存储，修改和删除数据本质都是增加，并且是在文件末尾顺序追加，因此磁盘响应速度很快，同时也可以解决HRegionServer崩溃导致MemStore数据丢失的问题。 返回Client确认写入成功。之后便可以查询此数据。 当MemStore达到阈值后，会将数据刷写到磁盘持久化，生成相应的HFile文件。同时，将HLog中的历史数据删除。 HBase的flush机制 MemStore 中累积了足够多的的数据后，整个有序数据集就会被写入一个新的 HFile 文件到 HDFS 上。HBase 为每个 Column Family 都创建一个 HFile，里面存储了具体的 Cell，也即 KeyValue 数据。随着时间推移，HFile 会不断产生，因为 KeyValue 会不断地从 MemStore 中被刷写到硬盘上。 注意这也是为什么 HBase 要限制 Column Family 数量的一个原因。每个 Column Family 都有一个 MemStore；如果Region的一个 MemStore 满了，该Region的所有的 MemStore 都会被刷写到硬盘。注意，刷写磁盘的基本单元是Region。 同时，它也会记录最后写入的数据的最大序列号（sequence number），这样系统就能知道目前为止哪些数据已经被持久化了。最大序列号是一个 meta 信息，被存储在每个 HFile 中，来表示持久化进行到哪条数据了，应该从哪里继续。当 Region 启动时，这些序列号会被读取，取其中最大的一个，作为基础序列号，后面的新的数据更新就会在该值的基础上递增产生新的序列号。这个序列号还可以用于从HLog中的什么位置开始恢复数据。 flush触发条件 MemoStore级别限制 当Region中任意一个MemStore的大小达到了上限（hbase.hregion.memstore.flush.size，默认128MB），会触发MemStore刷新。 hbase.hregion.memstore.flush.size 134217728 region级别限制 当Region中所有MemoStore的大小总和达到了上限（hbase.hregion.memstore.block.multiplier hbase.hregion.memstore.flush.size，默认 2 128M = 256M），会触发MemStore刷新。 hbase.hregion.memstore.flush.size 134217728 hbase.hregion.memstore.block.multiplier 2 Region Server级别限制 当HRegionServer中所有MemStore的大小总和超过低水位阈值hbase.regionserver.global.memstore.size.lower.limit*hbase.regionserver.global.memstore.size（前者默认值0.95），HRegionServer开始强制flush；首先刷新MemStore最大的Region，再执行次大的，依次执行。 如写入速度大于flush的速度，导致总MemStore大小超过高水位阈值hbase.regionserver.global.memstore.size（默认为JVM内存的40%），此时HRegionServer会阻塞更新并强制执行flush，直到总MemStore大小低于低水位阈值。 hbase.regionserver.global.memstore.size.lower.limit 0.95 hbase.regionserver.global.memstore.size 0.4 HLog数量上限 当一个HRegionServer中HLog数量达到上限（可通过参数hbase.regionserver.maxlogs配置）时，系统会选取最早的一个 HLog对应的一个或多个Region进行flush。 定期刷新 默认周期为1小时，确保MemStore不会长时间没有持久化。为避免所有的MemStore在同一时间都进行flush导致的问题，定期的flush操作有20000左右的随机延时。 手动flush 用户可以通过shell命令 flush ‘tablename’ 或者 flush ‘region name’ 分别对一个表或者一个Region进行flush。 flush 流程 为了减少flush过程对读写的影响，将整个flush过程分为三个阶段： prepare阶段（写阻塞，内存操作，时间短）：遍历当前Region中所有的MemStore，将MemStore中当前数据集CellSkipListSet做一个快照snapshot；然后再新建一个CellSkipListSet。后期写入的数据都会写入新的CellSkipListSet中。prepare阶段需要加锁updateLock对写请求阻塞，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。 flush阶段（涉及到磁盘IO，耗时）：遍历所有MemStore，将prepare阶段生成的snapshot持久化为临时文件，临时文件会统一放到目录.tmp下。这个过程因为涉及到磁盘IO操作，因此相对比较耗时。 commit阶段（耗时）：遍历所有MemStore，将flush阶段生成的临时文件移到指定的ColumnFamily目录下，针对HFile生成对应的storefile和Reader，把storefile添加到HStore的storefiles列表中，最后再清空prepare阶段生成的snapshot。 HBase的compact机制 hbase为了防止小文件过多，以保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。在hbase中主要存在两种类型的compaction合并： minor compaction 小合并 major compaction 大合并 minor compaction 小合并 将Store中多个HFile合并为一个HFile 在这个过程中会选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，对于TTL过期数据、更新的数据、删除的数据仅仅只是做了标记，并没有进行物理删除。一次Minor Compaction的结果是更少并且更大的StoreFile。这种合并的触发频率很高。 minor compaction触发条件由以下几个参数共同决定 hbase.hstore.compactionThreshold 3 hbase.hstore.compaction.max 10 hbase.hstore.compaction.min.size 134217728 hbase.hstore.compaction.max.size 9223372036854775807 major compaction 大合并 合并Store中所有的HFile为一个HFile 将所有的StoreFile合并成一个StoreFile，这个过程还会清理三类无意义数据：被删除的数据、TTL（time to live）过期数据、版本号超过设定版本号的数据。合并频率比较低，默认7天执行一次，并且性能消耗非常大（会遍历所有数据）。建议生产关闭，在应用空闲时间手动触发，防止出现在业务高峰期。 major compaction触发时间条件 hbase.hregion.majorcompaction 604800000 手动触发 # 使用major_compact命令 major_compact ‘tableName’ Region分裂 分裂流程 一开始每个 table 默认只有一个 Region。当一个 Region 逐渐变得大时，大小大于 hbase.hregion.max.filesize ，会分裂（split）成两个子 Region，每个子 Region 都包含了原来 Region 一半的数据，这两个子 Region 并行地在原来这个 HRegionServer 上创建，这个分裂动作会被报告给 HMaster。出于负载均衡的目的，HMaster 可能会将新的 Region 迁移给其它 HRegionServer。 hbase.hregion.max.filesize 10737418240 Splitting 一开始是发生在同一台 HRegionServer 上的，但是出于负载均衡的原因，HMaster 可能会将新的 Region 迁移 （注意是逻辑上的迁移，即将某个Region给另一个HRegionServer管理）到其他 HRegionServer，这会导致此时迁移到的HRegionServer需要访问离它比较远的 HDFS 数据，直到 major compaction 的到来，它会将那些远方的数据重新移回到离HRegionServer节点附近的地方。 分裂策略 手动指定 当一个table刚被创建的时候，Hbase默认分配一个Region给table。也就是说这个时候，所有的读写请求都会访问到同一个HRegionServer的同一个Region中，这个时候就达不到负载均衡的效果，集群中的其他HRegionServer就可能会处于空闲的状态。解决这个问题可以用pre-splitting，在创建table的时候就配置生成多个Region。 为什么要预分区？ 增加数据读写效率 负载均衡，防止数据倾斜 方便集群容灾调度Region 优化Map数量 预分区原理 每一个Region维护着startRowKey与endRowKey，如果加入的数据符合某个Region维护的rowKey范围，则该数据交给这个Region维护。 手动指定预分区 方式一 create 'person','info1','info2',SPLITS => ['1000','2000','3000','4000'] 共有5个Region | name | start key | end key | range | | ------- | --------- | ------- | ------------ | | Region1 | | 1000 | [, 1000) | | Region2 | 1000 | 2000 | [1000, 2000) | | Region3 | 2000 | 3000 | [2000, 3000) | | Region4 | 3000 | 4000 | [3000, 4000) | | Region5 | 4000 | | [4000, ) | 注意： 首行的start key和尾行的end key是空串，表示一张表的开始和结束位置 Region的rowkey范围包括start key，不包括end key，符合左闭右开 方式二 将分区规则创建于文件中 cd /bigdata/install vi splits # 文件内容 aaa bbb ccc ddd shell执行命令 create 'student', 'info', SPLITS_FILE => '/bigdata/install/splits' 方式三 HexStringSplit会将数据从 “00000000” 到 “FFFFFFFF” 之间的数据长度按照n等分之后算出每一段开始rowkey和结束rowkey，以此作为拆分点。 create 'mytable', 'base_info', 'extra_info', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'} 系统自动 ConstantSizeRegionSplitPolicy 0.94版本前，HBase region的默认切分策略 当region中最大的store大小超过某个阈值(hbase.hregion.max.filesize=10G)之后就会触发切分，一个region等分为2个region。 但是在生产线上这种切分策略却有相当大的弊端： 切分策略对于大表和小表（指HFile文件大小）没有明显的区分。 阈值(hbase.hregion.max.filesize)设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，形成热点，这对业务来说并不是什么好事。 如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的region，这对于集群的管理、资源使用、failover来说都不是一件好事。 IncreasingToUpperBoundRegionSplitPolicy 0.94版本~2.0版本默认切分策略 总体看和ConstantSizeRegionSplitPolicy思路相同 一个region中最大的store大小大于设置阈值就会触发切分。 但是这个阈值并不像ConstantSizeRegionSplitPolicy是一个固定的值，而是会在一定条件下不断调整，调整规则和region所属表在当前regionserver上的region个数有关系. region split阈值的计算公式是： 设regioncount：是region所属表在当前regionserver上的region的个数 阈值 = regioncount^3 128M 2，当然阈值并不会无限增长，最大不超过MaxRegionFileSize（10G）；当region中最大的store的大小达到该阈值的时候进行region split 例如： 第一次split阈值 = 1^3 256 = 256MB 第二次split阈值 = 2^3 256 = 2048MB（当两个中的一个region率先超过2048MB，则此region触发split） 第三次split阈值 = 3^3 256 = 6912MB 第四次split阈值 = 4^3 256 = 16384MB > 10GB，因此取较小的值10GB 后面每次split的size都是10GB了 也就是说前三次触发切分的阈值比较适合小表，避免热点问题，分发给三个HRegionServer管理。当满足第四次切分条件时，就需要满足大表条件。 特点 相比ConstantSizeRegionSplitPolicy，可以自适应大表、小表； 在集群规模比较大的情况下，对大表的表现比较优秀 但是，它并不完美，小表可能产生大量的小region，分散在各regionserver上（大合并后Region会迁移到其他的HRegionServer上） SteppingSplitPolicy 2.0版本默认切分策略 相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些 region切分的阈值依然和待分裂region所属表在当前regionserver上的region个数有关系 如果region个数等于1，切分阈值为flush size 128M * 2 否则为MaxRegionFileSize。 这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小region，而是适可而止。 KeyPrefixRegionSplitPolicy 根据rowKey的前缀对数据进行分区，这里是指定rowKey的前多少位作为前缀，比如rowKey都是16位的，指定前5位是前缀，那么前5位相同的rowKey在相同的region中。 DelimitedKeyPrefixRegionSplitPolicy 保证相同前缀的数据在同一个region中，例如rowKey的格式为：userideventtype_eventid，指定的delimiter为 ，则split的的时候会确保userid相同的数据在同一个region中。 DisabledRegionSplitPolicy 不启用自动拆分， 需要指定手动拆分 Region合并 Region的合并不是为了性能，而是出于维护的目的。如删除了大量的数据，这个时候每个Region都变得很小，存储多个Region浪费资源，这个时候可以把Region合并起来，进而可以减少一些Region服务器节点。 通过Merge类Region 创建一张hbase表 create 'test', 'info1', SPLITS => ['1000','2000','3000'] 关闭集群 stop-hbase.sh 执行命令 hbase org.apache.hadoop.hbase.util.Merge test region1 region2 通过online命令热合并Region 与冷合并不同的是，online_merge的传参是Region的hash值，而Region的hash值就是Region名称最后两个.之间的字符串部分 hbase shell 执行命令 merge_region 'region1_hash', 'region2_hash' 系统集成 与MapReduce集成 利用MapReduce的分布式计算，提高数据导入HBase表效率 与HRegionServer交互，通过集成HBase框架的TableMapper和TableReducer实现。 HBase表到HBase表 Hdfs文件到HBase表 写入HBase数据时，同直接调用API写数据流程类似，仍需要占用HRegionServer大量资源。 不与HRegionServer交互，通过MapReduce直接将数据输出为HBase识别的HFile文件格式，然后再通过BulkLoad的方式加载到HBase表中。 hadoop jar 在集群运行程序时可能会找不到hbase类，则需要做如下配置 一次生效（建议） export HBASE_HOME=/bigdata/install/hbase-1.2.0-cdh5.14.2/ export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2/ export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp` 永久生效 ```bash # 修改hadoop-env.sh # add hbase lib if [ -z $HBASE_HOME ]; then export HADOOP_CLASSPATH=${HADOOP_CLASSPATH} else export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:${HBASE_HOME}/lib'/*' fi # 配置即时生效 source hadoop-env.sh ``` 执行 com.sciatta.hadoop.hbase.example.mr.bulkload.HdfsToHBase ，借助 HFileOutputFormat2 输出hdfs文件 /test/hbase/huser/cf/21dc0757cbfe41a7bb8552818d0478ba 会生成目标指定列族 cf 下的HFile HBase在hdfs存储表默认在 /hbase/data/default 目录下，如 /hbase/data/default/user/36c7b176416c41bb1534676a2b50fdf9/info/03ccf327a86649ba9fee7ed734eafe56 user 为表名 36c7b176416c41bb1534676a2b50fdf9 为RegionId info 为列族名 03ccf327a86649ba9fee7ed734eafe56 是HFile 利用bulkload加载到表。清空 /test/hbase/huser/cf 目录下数据，转移到表相应Region的列族下 /hbase/data/default/user1/031ed91d57c19615d009b21fde809f57/cf/ae8b0ffabb374946922ef704b9f4a918_SeqId_5_ 与Hive集成 概述 Hive 数据仓库 Hive的本质相当于将HDFS中已经存储的文件在Mysql中做了一个映射，以方便使用HQL去管理查询 用于数据分析、清洗 Hive适用于离线的数据分析和清洗，延迟较高 基于HDFS、MapReduce Hive存储的数据依旧在DataNode上，编写的HQL语句最终转换为MapReduce代码执行 HBase 数据库 是一种面向列存储的非关系型数据库 用于存储结构化和非结构化的数据 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作 基于HDFS 数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理 延迟较低，接入在线业务使用 面对大量的企业数据，HBase可以支持单表大量数据的存储，同时提供了高效的数据访问速度 总结：Hive和HBase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，HBase是一种在Hadoop之上的 NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。 整合配置 HBase jar建立软连接到Hive的lib目录下 node03执行 ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-client-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop2-compat-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-hadoop-compat-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-it-1.2.0-cdh5.14.2.jar ln -s /bigdata/install/hbase-1.2.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar /bigdata/install/hive-1.1.0-cdh5.14.2/lib/hbase-server-1.2.0-cdh5.14.2.jar 修改配置 node03执行 hive-site.xml hive.zookeeper.quorum node01,node02,node03 hbase.zookeeper.quorum node01,node02,node03 hive-env.sh export HBASE_HOME=/bigdata/install/hbase-1.2.0-cdh5.14.2/ Hive分析结果保存到HBase表（hive -> hbase） Hive创建数据库和表 create database course; use course; create external table if not exists course.score(id int, cname string, score int) row format delimited fields terminated by '\\t' stored as textfile; 准备数据 hive-hbase 1 zhangsan 80 2 lisi 60 3 wangwu 30 4 zhaoliu 70 Hive加载数据 load data local inpath '/home/hadoop/hivedatas/hive-hbase' into table score; select * from score; 创建Hive内部表与HBase映射 -- hbase.columns.mapping hbase表的column要和hive表的field一一对应 create table course.hbase_score(id int, cname string, score int) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties(\"hbase.columns.mapping\" = \":key, cf:name, cf:score\") tblproperties(\"hbase.table.name\" = \"hbase_score\"); 向Hive内部表插入数据 insert overwrite table course.hbase_score select id, cname, score from course.score; 向HBase表插入数据 # 如果没有向hive映射的field赋值，则为null put 'hbase_score', '10', 'cf:name', 'rain' put 'hbase_score', '10', 'cf:score', 10 总结 存储 数据存储在HBase端，节省存储空间。数据量小的情况下存储在MemStore中，执行 flush 'hbase_score' 刷写到磁盘 同步 当向Hive内部表或HBase表插入数据时，两边都会同步数据 删除 删除Hive内部表，HBase映射表同步删除；但删除HBase映射表，Hive内部表不会同步删除，但查询时会提示HBase映射表不存在 Hive外部表映射HBase已存表进行分析（hbase -> hive） 创建Hive外部表映射HBase表 CREATE external TABLE course.hbase2hive(id int, name string, age int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key, info:name, info:age\") TBLPROPERTIES(\"hbase.table.name\" =\"user\"); 查询Hive外部表 select * from course.hbase2hive; 向Hive外部表插入数据 insert into table hbase2hive values(00000012, 'cat', 2); 向Hbase表插入数据 put 'user','00000011','info:name','wizard' 总结 存储 因为Hbase表映射的是Hive外部表，所以数据存储在HBase端 同步 当向Hive外部表或HBase表插入数据时，两边都会同步数据 删除 删除Hive外部表，不影响Hbase表；删除Hbase表，不会删除Hive外部表，但查询时会提示HBase表不存在，需要手动删除 HBase 的 RowKey 设计 设计原则 长度原则 RowKey 是一个二进制字节流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。建议越短越好，不要超过16个字节。 建议尽可能短（提高检索效率），但也不能太短，否则 RowKey 前缀重复的概率增大 设计过长会降低 MemStore 内存的利用率（key/value，RowKey是key的一部分）和HFile存储数据的效率 散列原则 建议将 RowKey 的高位作为散列字段，这样将提高数据均衡分布在每个 RegionServer ，以实现负载均衡 如果没有散列字段，首字段直接是时间信息。所有的数据都会集中在一个 RegionServer 上，这样在数据检索的时候负载会集中在个别的 RegionServer 上，造成热点问题，会降低查询效率 唯一原则 必须在设计上保证其唯一性，数据以key-value形式存储，相同rowkey会导致覆盖原有数据 排序原则 RowKey 是按照字典顺序排序存储的。因此，设计 RowKey 的时候，要充分利用这个排序的特点，可以将经常读取的数据存储到一块，将最近可能会被访问的数据存储到一块 热点问题 检索 HBase 记录首先要通过 RowKey 来定位数据行。当大量的 Client 访问 HBase 集群的一个或少数几个节点，造成少数 RegionServer 的读/写请求过多、负载过大，而其他 RegionServer 负载却很小，就造成了“热点”现象。 解决方案 预分区 预分区的目的让表的数据可以均衡的分散在集群中，而不是默认只有一个 Region 分布在集群的一个节点上。 加盐 这里所说的加盐不是密码学中的加盐，而是在 RowKey 的前面增加随机数，具体就是给 RowKey 分配一个随机前缀以使得它和之前的 RowKey 的前缀不同，保障数据在Regions间的负载均衡。 缺点：因为添加的是随机数，基于原RowKey查询时无法知道随机数是什么，那样在查询的时候就需要去各个可能的Regions中查找（全表扫描），Salting对于读取是利空的。并且加盐这种方式增加了读写时的吞吐量。 哈希 哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让 Client 重构完整的 RowKey，可以使用 get 操作准确获取某一个行数据，如 rowkey=MD5(username).subString(0,10)+时间戳 缺点：与 Reversing 类似，Hashing 也不利于 Scan，因为打乱了原RowKey的自然顺序。 反转 反转固定长度或者数字格式的 RowKey，这样可以使得 RowKey 中经常改变的部分（最没有意义的部分）放在前面。可以有效的随机 RowKey，但牺牲了RowKey 的有序性。 缺点：利于Get操作，但不利于Scan操作，因为数据在原RowKey上的自然顺序已经被打乱。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/auxiliaryframework/flume/Flume安装部署.html":{"url":"src/bigdata/auxiliaryframework/flume/Flume安装部署.html","title":"Flume安装部署","keywords":"","body":"安装 node03执行 scp flume-ng-1.6.0-cdh5.14.2.tar.gz hadoop@node03:/bigdata/soft tar -zxvf flume-ng-1.6.0-cdh5.14.2.tar.gz -C /bigdata/install/ 配置 flume-env.sh cd /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf cp flume-env.sh.template flume-env.sh vi flume-env.sh # 增加java环境变量 export JAVA_HOME=/bigdata/install/jdk1.8.0_141 /etc/profile sudo vi /etc/profile # 增加flume环境变量 export FLUME_HOME=/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin export PATH=$PATH:$FLUME_HOME/bin # 立即生效 source /etc/profile Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/auxiliaryframework/flume/Flume数据采集.html":{"url":"src/bigdata/auxiliaryframework/flume/Flume数据采集.html","title":"Flume数据采集","keywords":"","body":"基本介绍 概述 Flume 是一个分布式、可靠、高可用的海量日志采集、聚合和传输的系统。 Flume 可以采集文件，socket数据包、文件、文件夹、kafka 等各种形式源数据，又可以将采集到的数据（下沉sink）输出到HDFS、hbase、hive、kafka等众多外部存储系统中。 一般的采集需求，通过对 Flume 的简单配置即可实现。Flume 针对特殊场景也具备良好的自定义扩展能力，因此，flume 可以适用于大部分的日常数据采集场景。 运行机制 Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成。每一个 agent 相当于一个数据传递员，内部有三个组件： Source：采集组件，用于跟数据源对接，以获取数据 Sink：下沉组件，用于往下一级agent传递数据或者往最终存储系统传递数据 Channel：传输通道组件，用于从source将数据传递到sink 实战 采集网络端口数据并控制台打印 配置 netcat-memory-logger.conf # 定义这个agent中各组件的名字 a1.sources = r1 a1.sinks = k1 a1.channels = c1 # 描述和配置source组件：r1 a1.sources.r1.type = netcat a1.sources.r1.bind = 192.168.2.102 a1.sources.r1.port = 44444 # 描述和配置sink组件：k1 a1.sinks.k1.type = logger # 描述和配置channel组件，此处使用是内存缓存的方式 a1.channels.c1.type = memory # 默认该通道中最大的可以存储的event数量 a1.channels.c1.capacity = 1000 # 每次最大可以从source中拿到或者送到sink中的event数量 # 注意：capacity > trasactionCapacity a1.channels.c1.transactionCapacity = 100 # 描述和配置source channel sink之间的连接关系 a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 # -c 指定flume自身的配置文件所在目录 # -f netcat-memory-logger.conf 指定描述的采集方案 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f netcat-memory-logger.conf -n a1 -Dflume.root.logger=INFO,console 测试 # 安装telnet客户端模拟数据发送 sudo yum -y install telnet # 测试 # control+] 回到telnet命令窗口 # quit 退出 telnet node03 44444 采集目录文件到HDFS 配置 spooldir.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source # 注意，不能向监控目中重复放同名文件 a1.sources.r1.type = spooldir a1.sources.r1.spoolDir = /home/hadoop/flumedatas/spooldir a1.sources.r1.fileHeader = true # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://node01:8020/test/flume/spooldir/files/%y-%m-%d/%H%M/ a1.sinks.k1.hdfs.filePrefix = events- # ======== # 时间戳是否需要四舍五入，默认false，true影响所有基于时间的转义字符，除了%t a1.sinks.k1.hdfs.round = true # 时间戳四舍五入的倍数，小于当前时间 a1.sinks.k1.hdfs.roundValue = 10 # 时间戳四舍五入的单位，默认秒 a1.sinks.k1.hdfs.roundUnit = minute # 以上为每隔10分钟产生一个文件 # ======== # ======== # 触发滚动文件等待时间（秒），默认30，0不会基于时间滚动文件 # a1.sinks.k1.hdfs.rollInterval = 3 a1.sinks.k1.hdfs.rollInterval = 0 # 触发滚动文件的文件大小（字节），默认1024，0不会基于文件大小滚动文件 # a1.sinks.k1.hdfs.rollSize = 20 a1.sinks.k1.hdfs.rollSize = 0 # 触发滚动文件的事件数量（最小传输单位），默认10，0不会基于事件数量滚动文件 # a1.sinks.k1.hdfs.rollCount = 5 a1.sinks.k1.hdfs.rollCount = 0 # ======== # ======== # 刷写到HDFS前的事件数量，默认100 a1.sinks.k1.hdfs.batchSize = 1 # ======== a1.sinks.k1.hdfs.useLocalTimeStamp = true # 生成的文件类型，默认 SequenceFile，可用 DataStream 代替，为普通文本 a1.sinks.k1.hdfs.fileType = DataStream # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 Source：spooldir 监视一个目录（flume部署到数据源一端），只要目录中出现新文件，就会采集文件中的内容 采集完成的文件，会被agent自动添加一个后缀：COMPLETED HDFS 上的文件以 .tmp 作为后缀，当 flume agent 关闭 或者 满足新生成文件时，去掉 .tmp 后缀 所监视的目录中不允许重复出现相同文件名的文件（出错后不会继续运行） Sink：hdfs round、roundValue、roundUnit 控制多长时间生成一个文件，可以控制hdfs上小文件的数量。 rollInterval、rollSize、rollCount 控制滚动生成文件的时间间隔、文件大小、事件数量，当全部设置为0时，不会基于此三项生成文件。 启动 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f spooldir.conf -n a1 -Dflume.root.logger=INFO,console 测试 spooldir.sh #!/bin/bash i=0 while true do echo $((++i)) >> /home/hadoop/flumedatas/spooldir/$i; sleep 10; done sh spooldir.sh 采集文件到HDFS 配置 taillog.conf agent1.sources = source1 agent1.sinks = sink1 agent1.channels = channel1 # Describe/configure tail -F source1 agent1.sources.source1.type = exec agent1.sources.source1.command = tail -F /home/hadoop/flumedatas/taillog/access_log # Describe sink1 agent1.sinks.sink1.type = hdfs agent1.sinks.sink1.hdfs.path = hdfs://node01:8020/test/flume/taillog/%y-%m-%d/%H-%M agent1.sinks.sink1.hdfs.filePrefix = access_log agent1.sinks.sink1.hdfs.maxOpenFiles = 5000 agent1.sinks.sink1.hdfs.batchSize = 100 agent1.sinks.sink1.hdfs.fileType = DataStream agent1.sinks.sink1.hdfs.writeFormat = Text # 滚动生成新文件，hdfs.rollInterval没有设置，默认30s agent1.sinks.sink1.hdfs.rollSize = 102400 agent1.sinks.sink1.hdfs.rollCount = 1000000 # 每10分钟一个文件夹，hdfs.round必须设置为true，时间戳四舍五入才会生效；否则1分钟一个文件夹 agent1.sinks.sink1.hdfs.round = true agent1.sinks.sink1.hdfs.roundValue = 10 agent1.sinks.sink1.hdfs.roundUnit = minute agent1.sinks.sink1.hdfs.useLocalTimeStamp = true # Use a channel which buffers events in memory agent1.channels.channel1.type = memory agent1.channels.channel1.keep-alive = 120 agent1.channels.channel1.capacity = 500000 agent1.channels.channel1.transactionCapacity = 600 # Bind the source and sink to the channel agent1.sources.source1.channels = channel1 agent1.sinks.sink1.channel = channel1 tail -F 首次启动时，只加载文件最后10行（命令执行 或 flume执行） 常驻系统程序，监控文件末尾追加 Sink：hdfs 最下层文件夹是 小时-分钟 ，也就是说每分钟生成一个文件夹（hdfs.round=false）。观察测试数据发现，每分钟会有两个小文件，是因为 hdfs.rollInterval 默认30秒滚动生成新文件。 启动 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f taillog.conf -n agent1 -Dflume.root.logger=INFO,console 测试 taillog.sh #!/bin/bash while true do date >> /home/hadoop/flumedatas/taillog/access_log; sleep 0.5; done sh taillog.sh 断点续传 配置 taildir.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = TAILDIR # record the inode, the absolute path and the last position of each tailing file a1.sources.r1.positionFile = /home/hadoop/flumedatas/taildir_position.json # 可以有多个组（监控多个目录），以空格分隔，如：a1.sources.r1.filegroups = f1 f2 a1.sources.r1.filegroups = f1 a1.sources.r1.filegroups.f1 = /home/hadoop/flumedatas/taildir/.*log.* # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://node01:8020/test/flume/taildir/files/%y-%m-%d/%H%M/ a1.sinks.k1.hdfs.filePrefix = events- a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = minute a1.sinks.k1.hdfs.rollInterval = 3 a1.sinks.k1.hdfs.rollSize = 5000 a1.sinks.k1.hdfs.rollCount = 50000 a1.sinks.k1.hdfs.batchSize = 5000 a1.sinks.k1.hdfs.useLocalTimeStamp = true a1.sinks.k1.hdfs.fileType = DataStream # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 Sink：TAILDIR 可以监控文件夹下的所有文件，也可以监控文件内容 断点续传功能，原理是 taildir_position.json （自动生成）记录每一次生成事件的文件位置 启动 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f taildir.conf -n a1 -Dflume.root.logger=INFO,console 测试 # > 新增文件写入内容 # >> 追加内容到文件末尾 echo \"testlog1\" >> /home/hadoop/flumedatas/taildir/file.log echo \"testlog2\" >> /home/hadoop/flumedatas/taildir/file.log 级联Agent 安装 node03执行 scp -r apache-flume-1.6.0-cdh5.14.2-bin/ node02:$PWD node02执行 配置/etc/profile sudo vi /etc/profile # 增加flume环境变量 export FLUME_HOME=/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin export PATH=$PATH:$FLUME_HOME/bin # 立即生效 source /etc/profile 配置 node02 tail-avro-avro-logger.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/hadoop/flumedatas/taillog/access_log # Describe the sink # 数据发送 a1.sinks.k1.type = avro a1.sinks.k1.hostname = node03 a1.sinks.k1.port = 4141 a1.sinks.k1.batch-size = 10 # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 node03 avro-hdfs.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source # 数据接收 a1.sources.r1.type = avro a1.sources.r1.bind = node03 a1.sources.r1.port = 4141 # Describe the sink a1.sinks.k1.type = hdfs a1.sinks.k1.hdfs.path = hdfs://node01:8020/test/flume/avro/hdfs/%y-%m-%d/%H%M/ a1.sinks.k1.hdfs.filePrefix = events- a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = minute a1.sinks.k1.hdfs.rollInterval = 3 a1.sinks.k1.hdfs.rollSize = 20 a1.sinks.k1.hdfs.rollCount = 5 a1.sinks.k1.hdfs.batchSize = 1 a1.sinks.k1.hdfs.useLocalTimeStamp = true a1.sinks.k1.hdfs.fileType = DataStream # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 # node03 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f avro-hdfs.conf -n a1 -Dflume.root.logger=INFO,console # node02 flume-ng agent -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f tail-avro-avro-logger.conf -n a1 -Dflume.root.logger=INFO,console 测试 node02 taillog.sh while true do date >> /home/hadoop/flumedatas/taillog/access_log; sleep 0.5; done sh taillog.sh failover 故障转移 Flume NG 本身提供了Failover机制，可以自动切换和恢复。 规划 主机 角色 角色 node01 agent1 Web Server node02 collector1 AgentMstr1 node03 collector2 AgentMstr2 安装 node03执行 scp -r apache-flume-1.6.0-cdh5.14.2-bin/ node01:$PWD node01执行 配置/etc/profile sudo vi /etc/profile # 增加flume环境变量 export FLUME_HOME=/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin export PATH=$PATH:$FLUME_HOME/bin # 立即生效 source /etc/profile 配置 node01 agent.conf # agent1 name agent1.channels = c1 agent1.sources = r1 agent1.sinks = k1 k2 # set channel agent1.channels.c1.type = memory agent1.channels.c1.capacity = 1000 agent1.channels.c1.transactionCapacity = 100 # set sources agent1.sources.r1.channels = c1 agent1.sources.r1.type = exec agent1.sources.r1.command = tail -F /home/hadoop/flumedatas/taillogs/access_log # 拦截器是简单的插件式组件，设置在source和channel之间 # 多个拦截器组成了 interceptor chain 按声明顺序调用 # 静态拦截器 # 在header中增加kv对 agent1.sources.r1.interceptors = i1 i2 agent1.sources.r1.interceptors.i1.type = static agent1.sources.r1.interceptors.i1.key = Type agent1.sources.r1.interceptors.i1.value = LOGIN # 时间戳拦截器 # 在header中增加时间戳 agent1.sources.r1.interceptors.i2.type = timestamp # set sink1 agent1.sinks.k1.channel = c1 agent1.sinks.k1.type = avro agent1.sinks.k1.hostname = node02 agent1.sinks.k1.port = 52020 # set sink2 agent1.sinks.k2.channel = c1 agent1.sinks.k2.type = avro agent1.sinks.k2.hostname = node03 agent1.sinks.k2.port = 52020 # set sink group agent1.sinkgroups = g1 agent1.sinkgroups.g1.sinks = k1 k2 agent1.sinkgroups.g1.processor.type = failover # 先向优先级高的发送event # 注意：如果优先级相同，故障转移会失败 agent1.sinkgroups.g1.processor.priority.k1 = 10 agent1.sinkgroups.g1.processor.priority.k2 = 1 agent1.sinkgroups.g1.processor.maxpenalty = 10000 node02 collector.conf # set Agent name a1.sources = r1 a1.channels = c1 a1.sinks = k1 # set channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # set sources a1.sources.r1.type = avro a1.sources.r1.bind = node02 a1.sources.r1.port = 52020 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Collector a1.sources.r1.interceptors.i1.value = node02 a1.sources.r1.channels = c1 # set sink a1.sinks.k1.type=hdfs a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d a1.sinks.k1.hdfs.path= hdfs://node01:8020/test/flume/failover/node02 a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=TEXT a1.sinks.k1.hdfs.rollInterval=10 a1.sinks.k1.channel=c1 node03 collector.conf # set Agent name a1.sources = r1 a1.channels = c1 a1.sinks = k1 # set channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # set sources a1.sources.r1.type = avro a1.sources.r1.bind = node03 a1.sources.r1.port = 52020 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = static a1.sources.r1.interceptors.i1.key = Collector a1.sources.r1.interceptors.i1.value = node03 a1.sources.r1.channels = c1 # set sink a1.sinks.k1.type=hdfs a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d a1.sinks.k1.hdfs.path= hdfs://node01:8020/test/flume/failover/node03 a1.sinks.k1.hdfs.fileType=DataStream a1.sinks.k1.hdfs.writeFormat=TEXT a1.sinks.k1.hdfs.rollInterval=10 a1.sinks.k1.channel=c1 启动 # node03 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f collector.conf -Dflume.root.logger=DEBUG,console # node02 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f collector.conf -Dflume.root.logger=DEBUG,console # node01 启动 flume-ng agent -n agent1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f agent.conf -Dflume.root.logger=DEBUG,console 测试 # 发送数据 echo \"1\" >> /home/hadoop/flumedatas/taillogs/access_log # node02 收到数据（优先级高） # 关闭 node02 # 发送数据 echo \"2\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 收到数据（故障转移） # 恢复 node02 # 发送数据 echo \"3\" >> /home/hadoop/flumedatas/taillogs/access_log # node02 收到数据（优先级高） load balance 负载均衡 负载均衡是用于解决一台机器（一个进程）无法解决所有请求而产生的一种算法。Load balancing Sink Processor 能够实现 load balance 功能。 规划 主机 名称 角色 node01 client 采集数据，发送到node02和node03 node02 server 接收node01部分数据 node03 server 接收node01部分数据 配置 node01 load_banlancer_client.conf # agent name a1.channels = c1 a1.sources = r1 a1.sinks = k1 k2 # set channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # set source a1.sources.r1.channels = c1 a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/hadoop/flumedatas/taillogs/access_log # set sink1 a1.sinks.k1.channel = c1 a1.sinks.k1.type = avro a1.sinks.k1.hostname = node02 a1.sinks.k1.port = 52020 # set sink2 a1.sinks.k2.channel = c1 a1.sinks.k2.type = avro a1.sinks.k2.hostname = node03 a1.sinks.k2.port = 52020 # set sink group a1.sinkgroups = g1 a1.sinkgroups.g1.sinks = k1 k2 a1.sinkgroups.g1.processor.type = load_balance a1.sinkgroups.g1.processor.backoff = true # 轮询 a1.sinkgroups.g1.processor.selector = round_robin a1.sinkgroups.g1.processor.selector.maxTimeOut=10000 node02 load_banlancer_server.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = avro a1.sources.r1.channels = c1 a1.sources.r1.bind = node02 a1.sources.r1.port = 52020 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 node03 load_banlancer_server.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = avro a1.sources.r1.channels = c1 a1.sources.r1.bind = node03 a1.sources.r1.port = 52020 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 # node03 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f load_banlancer_server.conf -Dflume.root.logger=DEBUG,console # node02 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f load_banlancer_server.conf -Dflume.root.logger=DEBUG,console # node01 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f load_banlancer_client.conf -Dflume.root.logger=DEBUG,console 测试 # 轮询 # node02 echo \"1\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"2\" >> /home/hadoop/flumedatas/taillogs/access_log # node02 echo \"3\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"4\" >> /home/hadoop/flumedatas/taillogs/access_log # close node02 # node03 可以正常接收 # node03 echo \"5\" >> /home/hadoop/flumedatas/taillogs/access_log echo \"6\" >> /home/hadoop/flumedatas/taillogs/access_log echo \"7\" >> /home/hadoop/flumedatas/taillogs/access_log # start node02 # node02 node03 可以正常接收 # node02 echo \"8\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"9\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"10\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"11\" >> /home/hadoop/flumedatas/taillogs/access_log # node02 echo \"12\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"13\" >> /home/hadoop/flumedatas/taillogs/access_log # node02 echo \"14\" >> /home/hadoop/flumedatas/taillogs/access_log # node02 echo \"15\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"16\" >> /home/hadoop/flumedatas/taillogs/access_log # node02 echo \"17\" >> /home/hadoop/flumedatas/taillogs/access_log # node03 echo \"18\" >> /home/hadoop/flumedatas/taillogs/access_log 自定义服务组件 自定义Interceptor 需求 将数据第一位电话号码加密，第4位兴趣过滤 准备数据 vi user.txt 13901007610,male,30,sing,beijing 18600000035,male,40,dance,shanghai 13366666659,male,20,Swimming,wuhan 13801179888,female,18,dance,tianjin 18511111114,male,35,sing,beijing 13718428888,female,40,Foodie,shanghai 13901057088,male,50,Basketball,taiwan 13671057777,male,60,Bodybuilding,xianggang 开发 添加xml依赖 org.apache.flume flume-ng-core UserInterceptor.java public class UserInterceptor implements Interceptor { private int encrypted_field_index; private int out_index; public UserInterceptor(int encrypted_field_index, int out_index) { this.encrypted_field_index = encrypted_field_index; this.out_index = out_index; } @Override public void initialize() { } @Override public Event intercept(Event event) { String line = new String(event.getBody(), Charsets.UTF_8); String newLine = \"\"; String[] fields = line.split(\",\"); for (int i = 0; i intercept(List list) { List newList = new ArrayList(); for (Event e : list) { Event test = intercept(e); if (test != null) { newList.add(test); } } return newList; } @Override public void close() { } private String md5(String plainText) throws NoSuchAlgorithmException { byte[] secretBytes = null; MessageDigest md5 = MessageDigest.getInstance(\"MD5\"); md5.update(plainText.getBytes()); secretBytes = md5.digest(); String result = new BigInteger(1, secretBytes).toString(16); for (int i = 0; i 上传jar # 自定义 interceptor 上传到 plugins.d/mysqlsource/lib scp hadoop-flume-example-1.0-SNAPSHOT.jar hadoop@node03:/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/plugins.d/mysqlsource/lib 配置 user.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # 配置source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/hadoop/flumedatas/user/user.txt a1.sources.r1.channels = c1 a1.sources.r1.interceptors =i1 # 自定义interceptor需要指定包名 a1.sources.r1.interceptors.i1.type = com.sciatta.hadoop.flume.example.interceptor.UserInterceptor$Builder a1.sources.r1.interceptors.i1.encrypted_field_index=0 a1.sources.r1.interceptors.i1.out_index=3 # 配置channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # 配置sink a1.sinks.k1.type = hdfs a1.sinks.k1.channel = c1 a1.sinks.k1.hdfs.path = hdfs://node01:8020/test/flume/user/%Y-%m-%d/%H%M a1.sinks.k1.hdfs.filePrefix = events- a1.sinks.k1.hdfs.round = true a1.sinks.k1.hdfs.roundValue = 10 a1.sinks.k1.hdfs.roundUnit = minute a1.sinks.k1.hdfs.rollInterval = 5 a1.sinks.k1.hdfs.rollSize = 50 a1.sinks.k1.hdfs.rollCount = 10 a1.sinks.k1.hdfs.batchSize = 100 a1.sinks.k1.hdfs.useLocalTimeStamp = true a1.sinks.k1.hdfs.fileType = DataStream 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f user.conf -Dflume.root.logger=DEBUG,console 自定义Source 需求 自定义flume的source，实现从mysql数据库当中获取数据，将数据打印到控制台。 准备数据 create database if not exists mysqlsource default character set utf8; -- 拉取目标表位置信息 drop table if exists mysqlsource.flume_meta; CREATE TABLE mysqlsource.flume_meta ( source_tab varchar(255) NOT NULL, currentIndex varchar(255) NOT NULL, PRIMARY KEY (source_tab) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- 插入数据 insert into mysqlsource.flume_meta(source_tab,currentIndex) values ('student','4'); -- 拉取目标表 -- AUTO_INCREMENT 指定自增初始值 drop table if exists mysqlsource.student; CREATE TABLE mysqlsource.student( id int(11) NOT NULL AUTO_INCREMENT, name varchar(255) NOT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8; -- 向student表中添加测试数据 insert into mysqlsource.student(id,name) values (1,'zhangsan'),(2,'lisi'),(3,'wangwu'),(4,'zhaoliu'); 开发 添加xml依赖 org.apache.flume flume-ng-core mysql mysql-connector-java MySQLHelper.java public class MySQLHelper { private static final Logger LOG = LoggerFactory.getLogger(MySQLHelper.class); // 两次查询的时间间隔 private int runQueryDelay; // 开始id private int startFrom; // 当前id private int currentIndex; // 要操作的表 private String table; // 用户传入的查询的列 private String columnsToSelect; // 为定义的变量赋值（默认值），可在flume任务的配置文件中修改 private static final int DEFAULT_QUERY_DELAY = 10000; private static final int DEFAULT_START_VALUE = 0; private static final String DEFAULT_COLUMNS_SELECT = \"*\"; private static Connection conn = null; private static PreparedStatement ps = null; private static String connectionURL, connectionUserName, connectionPassword; static { Properties p = new Properties(); try { p.load(MySQLHelper.class.getClassLoader().getResourceAsStream(\"jdbc.properties\")); connectionURL = p.getProperty(\"dbUrl\"); connectionUserName = p.getProperty(\"dbUser\"); connectionPassword = p.getProperty(\"dbPassword\"); // 加载数据库驱动 Class.forName(p.getProperty(\"dbDriver\")); } catch (Exception e) { LOG.error(e.toString()); } } MySQLHelper(Context context) { // 有默认值参数：获取flume任务配置文件中的参数，读不到的采用默认值 this.columnsToSelect = context.getString(\"columns.to.select\", DEFAULT_COLUMNS_SELECT); this.runQueryDelay = context.getInteger(\"run.query.delay\", DEFAULT_QUERY_DELAY); this.startFrom = context.getInteger(\"start.from\", DEFAULT_START_VALUE); // 无默认值参数：获取flume任务配置文件中的参数 this.table = context.getString(\"table\"); // 校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常 checkMandatoryProperties(); // 创建数据库连接 conn = InitConnection(connectionURL, connectionUserName, connectionPassword); } MySQLHelper(Properties properties) { this.columnsToSelect = properties.getProperty(\"columns.to.select\", DEFAULT_COLUMNS_SELECT); this.runQueryDelay = getIntegerFromProperties(properties, \"run.query.delay\", DEFAULT_QUERY_DELAY); this.startFrom = getIntegerFromProperties(properties, \"start.from\", DEFAULT_START_VALUE); // 无默认值参数：获取flume任务配置文件中的参数 this.table = properties.getProperty(\"table\"); // 校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常 checkMandatoryProperties(); // 创建数据库连接 conn = InitConnection(connectionURL, connectionUserName, connectionPassword); } List> executeQuery() { try { // 每次执行查询时都要重新生成sql，因为id不同 String query = buildQuery(); // 存放结果的集合 List> results = new ArrayList<>(); if (ps == null) { // 初始化PrepareStatement对象 ps = conn.prepareStatement(query); } ResultSet result = ps.executeQuery(query); while (result.next()) { // 存放一条数据的集合（多个列） List row = new ArrayList<>(); // 将返回结果放入集合 for (int i = 1; i getAllRows(List> queryResult) { List allRows = new ArrayList<>(); if (queryResult == null || queryResult.isEmpty()) return allRows; StringBuilder row = new StringBuilder(); for (List rawRow : queryResult) { Object value; for (Object aRawRow : rawRow) { value = aRawRow; if (value == null) { row.append(\",\"); } else { row.append(aRawRow.toString()).append(\",\"); } } allRows.add(row.toString()); row = new StringBuilder(); } return allRows; } void close() { try { ps.close(); conn.close(); } catch (SQLException e) { e.printStackTrace(); } } private int getIntegerFromProperties(Properties properties, String key, int defaultValue) { if (properties.getProperty(key) == null) { return defaultValue; } return Integer.parseInt(properties.getProperty(key)); } private void checkMandatoryProperties() { if (table == null) { throw new ConfigurationException(\"property table not set\"); } if (connectionURL == null) { throw new ConfigurationException(\"connection.url property not set\"); } if (connectionUserName == null) { throw new ConfigurationException(\"connection.user property not set\"); } if (connectionPassword == null) { throw new ConfigurationException(\"connection.password property not set\"); } } private static Connection InitConnection(String url, String user, String pw) { try { Connection conn = DriverManager.getConnection(url, user, pw); if (conn == null) throw new SQLException(); return conn; } catch (SQLException e) { e.printStackTrace(); } return null; } private String buildQuery() { // 获取当前id currentIndex = getStatusDBIndex(startFrom); LOG.info(currentIndex + \"\"); // 以id作为offset return \"SELECT \" + columnsToSelect + \" FROM \" + table + \" where \" + \"id\" + \">\" + currentIndex; } private Integer getStatusDBIndex(int startFrom) { // 从 flume_meta 表中查询出当前的id，若有值，以此为准，否则，以传入参数 startFrom 为准 String dbIndex = queryFirstColumn(\"select currentIndex from flume_meta where source_tab='\" + table + \"'\"); if (dbIndex != null) { return Integer.parseInt(dbIndex); } // 如果没有数据，则说明是第一次查询或者数据表中还没有存入数据，返回最初传入的值 return startFrom; } private String queryFirstColumn(String sql) { ResultSet result; try { ps = conn.prepareStatement(sql); result = ps.executeQuery(); while (result.next()) { return result.getString(1); } } catch (SQLException e) { e.printStackTrace(); } return null; } private void execSql(String sql) { try { ps = conn.prepareStatement(sql); ps.execute(); } catch (SQLException e) { e.printStackTrace(); } } int getRunQueryDelay() { return runQueryDelay; } } MySQLSource.java public class MySQLSource extends AbstractSource implements Configurable, PollableSource { private static final Logger LOG = LoggerFactory.getLogger(MySQLSource.class); private MySQLHelper sqlSourceHelper; @Override public long getBackOffSleepIncrement() { return 0; } @Override public long getMaxBackOffSleepInterval() { return 0; } @Override public void configure(Context context) { // 初始化 sqlSourceHelper = new MySQLHelper(context); } @Override public PollableSource.Status process() throws EventDeliveryException { try { List events = new ArrayList<>(); HashMap header = new HashMap<>(); // 查询数据表 List> result = sqlSourceHelper.executeQuery(); // 如果有返回数据，则将数据封装为event if (!result.isEmpty()) { List allRows = sqlSourceHelper.getAllRows(result); Event event; for (String row : allRows) { event = new SimpleEvent(); event.setBody(row.getBytes()); event.setHeaders(header); events.add(event); } // 将event写入channel this.getChannelProcessor().processEventBatch(events); // 更新数据表中的offset信息 sqlSourceHelper.updateOffset2DB(result.size()); } // 等待时长 Thread.sleep(sqlSourceHelper.getRunQueryDelay()); return Status.READY; } catch (InterruptedException e) { LOG.error(\"Error processing row：\", e); return Status.BACKOFF; } } @Override public synchronized void stop() { LOG.info(\"Stopping sql source {} ...\", getName()); try { // 关闭资源 sqlSourceHelper.close(); } finally { super.stop(); } } } jdbc.properties dbUrl=jdbc:mysql://node03:3306/mysqlsource?useUnicode=true&characterEncoding=utf-8 dbUser=root dbPassword=root dbDriver=com.mysql.jdbc.Driver 上传jar # 自定义source 上传到 plugins.d/mysqlsource/lib scp hadoop-flume-example-1.0-SNAPSHOT.jar hadoop@node03:/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/plugins.d/mysqlsource/lib # 其依赖 上传到 plugins.d/mysqlsource/libext scp mysql-connector-java-5.1.38.jar /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/plugins.d/mysqlsource/libext 配置 mysqlsource.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source # 自定义source需要指定包名 a1.sources.r1.type = com.sciatta.hadoop.flume.example.source.MySQLSource a1.sources.r1.columns.to.select = * a1.sources.r1.run.query.delay=20000 a1.sources.r1.start.from=0 a1.sources.r1.table = student # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Describe the sink a1.sinks.k1.type = logger # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f mysqlsource.conf -Dflume.root.logger=DEBUG,console 测试 -- 插入3条数据，offset更新为7 insert into mysqlsource.student(name) values ('rain'); insert into mysqlsource.student(name) values ('yoyo'); insert into mysqlsource.student(name) values ('lucky'); -- 插入2条数据，offset更新为9 insert into mysqlsource.student(name) values ('1'); insert into mysqlsource.student(name) values ('2'); 自定义Sink 需求 自定义flume的sink，实现将数据过滤处理后写入到mysql数据库。 准备数据 drop table if exists mysqlsource.flume2mysql; CREATE TABLE mysqlsource.flume2mysql ( id INT(11) NOT NULL AUTO_INCREMENT, createTime VARCHAR(64) NOT NULL, content VARCHAR(255) NOT NULL, PRIMARY KEY (id) ) ENGINE=INNODB DEFAULT CHARSET=utf8; 开发 MySQLSink.java public class MySQLSink extends AbstractSink implements Configurable { private static final Logger LOG = LoggerFactory.getLogger(MySQLSink.class); private String connectionURL = \"\"; private String connectionUserName = \"\"; private String connectionPassword = \"\"; private String tableName = \"\"; Connection con = null; @Override public void configure(Context context) { Properties p = new Properties(); try { p.load(MySQLHelper.class.getClassLoader().getResourceAsStream(\"jdbc.properties\")); // 从properties中获得 connectionURL = p.getProperty(\"dbUrl\"); connectionUserName = p.getProperty(\"dbUser\"); connectionPassword = p.getProperty(\"dbPassword\"); // 从flume配置中获得 tableName = context.getString(\"table\"); // 加载数据库驱动 Class.forName(p.getProperty(\"dbDriver\")); } catch (Exception e) { LOG.error(e.toString()); } } @Override public synchronized void start() { try { con = DriverManager.getConnection(connectionURL, connectionUserName, connectionPassword); super.start(); LOG.info(\"start ok! connection=\" + con); } catch (Exception ex) { ex.printStackTrace(); } } @Override public synchronized void stop() { try { con.close(); LOG.info(\"stop ok!\"); } catch (SQLException e) { e.printStackTrace(); } super.stop(); } @Override public Status process() { Status status; // Start transaction Channel ch = getChannel(); Transaction txn = ch.getTransaction(); txn.begin(); try { Event event = ch.take(); if (event != null) { // 获取body中的数据 String body = new String(event.getBody(), StandardCharsets.UTF_8); // 如果日志中有以下关键字的不需要保存，过滤掉 if (body.contains(\"delete\") || body.contains(\"drop\") || body.contains(\"alert\")) { status = Status.BACKOFF; } else { // 存入Mysql SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); String date = df.format(new Date()); String sql = String.format(\"insert into %s (createTime, content) values (?, ?)\", tableName); PreparedStatement stmt = con.prepareStatement(sql); stmt.setString(1, date); stmt.setString(2, body); stmt.execute(); stmt.close(); status = Status.READY; } } else { status = Status.BACKOFF; } txn.commit(); } catch (Throwable t) { txn.rollback(); t.getCause().printStackTrace(); status = Status.BACKOFF; } finally { txn.close(); } return status; } } 上传jar # 自定义 sink 上传到 plugins.d/mysqlsource/lib scp hadoop-flume-example-1.0-SNAPSHOT.jar hadoop@node03:/bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/plugins.d/mysqlsource/lib # 其依赖 上传到 plugins.d/mysqlsource/libext scp mysql-connector-java-5.1.38.jar /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/plugins.d/mysqlsource/libext 配置 mysqlsink.conf a1.sources = r1 a1.sinks = k1 a1.channels = c1 # 配置source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/hadoop/flumedatas/taillog/access_log a1.sources.r1.channels = c1 # 配置channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # 配置sink a1.sinks.k1.channel = c1 # 自定义sink需要指定包名 a1.sinks.k1.type = com.sciatta.hadoop.flume.example.sink.MySQLSink a1.sinks.k1.table=flume2mysql 启动 flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f mysqlsink.conf -Dflume.root.logger=DEBUG,console 测试 echo \"hello\" >> /home/hadoop/flumedatas/taillog/access_log echo \"flume\" >> /home/hadoop/flumedatas/taillog/access_log Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/auxiliaryframework/sqoop/Sqoop安装部署.html":{"url":"src/bigdata/auxiliaryframework/sqoop/Sqoop安装部署.html","title":"Sqoop安装部署","keywords":"","body":"安装 安装在node03上 # 拷贝sqoop到node03 scp sqoop-1.4.6-cdh5.14.2.tar.gz hadoop@node03:/bigdata/soft # 解压 tar -zxf sqoop-1.4.6-cdh5.14.2.tar.gz -C ../install/ 修改配置文件 sqoop-env.sh cd /bigdata/install/sqoop-1.4.6-cdh5.14.2/conf cp sqoop-env-template.sh sqoop-env.sh vi sqoop-env.sh #Set path to where bin/hadoop is available export HADOOP_COMMON_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2 #Set path to where hadoop-*-core.jar is available export HADOOP_MAPRED_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2 #set the path to where bin/hbase is available export HBASE_HOME=/bigdata/install/hbase-1.2.0-cdh5.14.2 #Set the path to where bin/hive is available export HIVE_HOME=/bigdata/install/hive-1.1.0-cdh5.14.2 添加jar 支持MySQL scp java-json.jar mysql-connector-java-5.1.38.jar hadoop@node03:/bigdata/install/sqoop-1.4.6-cdh5.14.2/lib 支持Hive cd /bigdata/install/hive-1.1.0-cdh5.14.2/lib cp hive-exec-1.1.0-cdh5.14.2.jar ../../sqoop-1.4.6-cdh5.14.2/lib/ 验证 # 列出mysql的所有数据库 bin/sqoop list-databases --connect jdbc:mysql://node03:3306/ --username root --password root Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/auxiliaryframework/sqoop/Sqoop数据迁移.html":{"url":"src/bigdata/auxiliaryframework/sqoop/Sqoop数据迁移.html","title":"Sqoop数据迁移","keywords":"","body":"概述 Sqoop是apache旗下的一款“Hadoop和关系数据库之间传输数据”的工具。 导入数据：将MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统。 导出数据：从Hadoop的文件系统中导出数据到关系数据库。 将导入和导出的命令翻译成MapReduce程序实现，在翻译出的MapReduce中主要是对inputformat和outputformat进行定制。 Sqoop1和Sqoop2比较 比较 Sqoop1 Sqoop2 版本 1.4.x 1.99x 架构 仅使用一个sqoop客户端 引入了sqoop server，对connector实现了集中的管理 部署 简单，需要root权限，connector必须符合JDBC模型 复杂，配置部署更加繁琐 使用 命令方式容易出错，格式紧耦合；无法支持所有数据类型；安全机制不够完善，需要指定数据库用户和密码 多种交互方式，REST API、 JAVA API、 WEB UI以及CLI控制台方式进行访问；connector集中管理，所有连接完善权限管理，connector规范化，仅负责数据的读写 数据导入 准备mysql数据 CREATE DATABASE `userdb`; USE `userdb`; DROP TABLE IF EXISTS `emp`; CREATE TABLE `emp` ( `id` INT(11) DEFAULT NULL, `name` VARCHAR(100) DEFAULT NULL, `deg` VARCHAR(100) DEFAULT NULL, `salary` INT(11) DEFAULT NULL, `dept` VARCHAR(10) DEFAULT NULL, `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `is_delete` BIGINT(20) DEFAULT '1' ) ENGINE = INNODB DEFAULT CHARSET = latin1; INSERT INTO `emp`(`id`, `name`, `deg`, `salary`, `dept`) VALUES (1201, 'gopal', 'manager', 50000, 'TP'); INSERT INTO `emp`(`id`, `name`, `deg`, `salary`, `dept`) VALUES (1202, 'manisha', 'Proof reader', 50000, 'TP'); INSERT INTO `emp`(`id`, `name`, `deg`, `salary`, `dept`) VALUES (1203, 'khalil', 'php dev', 30000, 'AC'); INSERT INTO `emp`(`id`, `name`, `deg`, `salary`, `dept`) VALUES (1204, 'prasanth', 'php dev', 30000, 'AC'); INSERT INTO `emp`(`id`, `name`, `deg`, `salary`, `dept`) VALUES (1205, 'kranthi', 'admin', 20000, 'TP'); DROP TABLE IF EXISTS `emp_add`; CREATE TABLE `emp_add` ( `id` INT(11) DEFAULT NULL, `hno` VARCHAR(100) DEFAULT NULL, `street` VARCHAR(100) DEFAULT NULL, `city` VARCHAR(100) DEFAULT NULL, `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `is_delete` BIGINT(20) DEFAULT '1' ) ENGINE = INNODB DEFAULT CHARSET = latin1; INSERT INTO `emp_add`(`id`, `hno`, `street`, `city`) VALUES (1201, '288A', 'vgiri', 'jublee'), (1202, '108I', 'aoc', 'sec-bad'), (1203, '144Z', 'pgutta', 'hyd'), (1204, '78B', 'old city', 'sec-bad'), (1205, '720X', 'hitec', 'sec-bad'); DROP TABLE IF EXISTS `emp_conn`; CREATE TABLE `emp_conn` ( `id` INT(100) DEFAULT NULL, `phno` VARCHAR(100) DEFAULT NULL, `email` VARCHAR(100) DEFAULT NULL, `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `is_delete` BIGINT(20) DEFAULT '1' ) ENGINE = INNODB DEFAULT CHARSET = latin1; INSERT INTO `emp_conn`(`id`, `phno`, `email`) VALUES (1201, '2356742', 'gopal@tp.com'), (1202, '1661663', 'manisha@tp.com'), (1203, '8887776', 'khalil@ac.com'), (1204, '9988774', 'prasanth@ac.com'), (1205, '1231231', 'kranthi@tp.com'); 导入表数据到HDFS 导入到默认目录 导入数据默认在HDFS的当前用户下，如 /user/hadoop/emp/ bin/sqoop import --connect jdbc:mysql://node03:3306/userdb --password root --username root --table emp --m 1 导入到指定目录 # --delete-target-dir 目标目录如果存在，则删除 # --target-dir 指定目标目录 bin/sqoop import --connect jdbc:mysql://node03:3306/userdb --username root --password root --delete-target-dir --table emp --target-dir /test/sqoop/userdb/emp --m 1 指定分隔符 # --fields-terminated-by 指定字段间分隔符；默认 , bin/sqoop import --connect jdbc:mysql://node03:3306/userdb --username root --password root --delete-target-dir --table emp --target-dir /test/sqoop/userdb/emp2 --m 1 --fields-terminated-by '\\t' 指定数据子集 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root --password root --table emp_add \\ --target-dir /test/sqoop/userdb/emp_add -m 1 --delete-target-dir \\ --where \"city = 'sec-bad'\" 指定SQL语句 # 必须包含 $CONDITIONS bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root --password root \\ --delete-target-dir -m 1 \\ --query 'select phno from emp_conn where 1=1 and $CONDITIONS' \\ --target-dir /test/sqoop/userdb/emp_conn 增量导入 使用参数 --incremental、--check-column、--last-value # 增量导入不可有 --delete-target-dir 参数 # 从1202的下一条记录开始导入 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --incremental append \\ --check-column id \\ --last-value 1202 \\ -m 1 \\ --target-dir /test/sqoop/userdb/emp_increment_ 使用参数 -- where # 参数 --check-column 必须存在 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --incremental append \\ --check-column id \\ --where \"id > 1202\" \\ -m 1 \\ --target-dir /test/sqoop/userdb/emp_increment2_ 导入表数据到Hive 导入到指定Hive表 创建Hive数据库和表 create database sqooptohive; use sqooptohive; create external table emp_hive(id int,name string,deg string,salary int ,dept string) row format delimited fields terminated by '\\001'; 导入 bin/sqoop import --connect jdbc:mysql://node03:3306/userdb --username root --password root --table emp --fields-terminated-by '\\001' --hive-import --hive-table sqooptohive.emp_hive --hive-overwrite --delete-target-dir -m 1 自动创建Hive表 bin/sqoop import --connect jdbc:mysql://node03:3306/userdb --username root --password root --table emp_conn --hive-import -m 1 --hive-database sqooptohive; 导入表数据到HBase bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --columns \"id,name,deg,salary,dept,create_time,update_time,is_delete\" \\ --column-family \"info\" \\ --hbase-create-table \\ --hbase-row-key \"id\" \\ --hbase-table \"emp\" \\ --num-mappers 1 \\ --split-by id 数据导出 导出HDFS到表数据 # 导出表必须存在 bin/sqoop export \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root --password root \\ --table emp \\ --export-dir /test/sqoop/userdb/emp \\ --input-fields-terminated-by \",\" 导出HBase到表数据 借助Hive表实现。 HBase表映射到Hive外部表 因为Hive外部表是临时表，被删除不会影响到HBase表。避免如果是Hive内部表意外删除导致相应的HBase表被删除。 ```mysql CREATE EXTERNAL TABLE sqooptohive.emp_ext (id int, name string, deg string, salary int, dept string, create_time string, update_time string, is_delete int) row format delimited fields terminated by ',' STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \"hbase.columns.mapping\" = \":key,info:name,info:deg,info:salary,info:dept,info:create_time,info:update_time,info:is_delete\" ) TBLPROPERTIES(\"hbase.table.name\" = \"emp\"); ``` ### 创建Hive内部表，将外部表数据导入到内部表 此步的作用是为了将HBase端的数据拉取到Hive端。 ```mysql # 创建内部表 create table sqooptohive.emp_in (id int, name string, deg string, salary int, dept string, create_time string, update_time string, is_delete int) row format delimited fields terminated by ','; # 导入数据 insert overwrite table sqooptohive.emp_in select * from sqooptohive.emp_ext; ``` ### Hive内部表数据导出到MySQL 清空表数据 `delete from emp;` ```shell bin/sqoop export \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root --password root \\ --table emp \\ --export-dir /user/hive/warehouse/sqooptohive.db/emp_in \\ --input-fields-terminated-by ',' \\ --input-null-string '\\\\N' --input-null-non-string '\\\\N' ``` # Sqoop作业 将事先定义好的数据导入导出任务按照指定流程运行。 ## 创建作业 ```shell # 注意 -- import 中间有一个空格 bin/sqoop job \\ --create myjob \\ -- import \\ --connect jdbc:mysql://node03:3306/userdb --username root --password root \\ --table emp \\ --delete-target-dir \\ --m 1 ``` ## 验证作业 ``` shell # Sqoop作业列表 bin/sqoop job --list # 检查或验证特定的工作及其详细信息 bin/sqoop job --show myjob ``` ## 执行作业 ```shell # 需要root用户口令 bin/sqoop job --exec myjob ``` # 常用命令 ## import 将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中，如果导入的是Hive，那么当Hive中没有对应表时，则自动创建。 - 导入数据 ```shell bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --hive-import \\ --hive-table sqooptohive.emp \\ -m 1 ``` - NULL值的使用 准备测试数据 ```mysql -- salary INT(11) 为 NULL -- dept VARCHAR(10) 为 NULL INSERT INTO `emp`(`id`, `name`, `deg`) VALUES (1201, 'gopal', 'manager'); ``` 测试 ```shell # 导入到hive表后 # salary是NULL，需要通过 is null 查询，在hdfs存储为null字符串 # dept是“null”，需要通过 dept='null' 查询，在hdfs存储为null字符串 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --hive-import \\ --hive-table sqooptohive.emp \\ -m 1 # --null-string '\\\\N' mysql字段是null的字符串类型，解释为hive的NULL # --null-non-string '\\\\N' mysql字段是null的非字符串类型，解释为hive的NULL # 使得语义相同，可以通过 is null 查询，在hdfs存储为\\N字符串 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --hive-import --hive-overwrite \\ --hive-table sqooptohive.emp \\ --null-string '\\\\N' --null-non-string '\\\\N' \\ -m 1 ``` - 增量导入 append 使用场景：顺序增加，比如日志数据 # 最后一条数据1202 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb --username root --password root --table emp \\ --num-mappers 1 \\ --fields-terminated-by \"\\t\" --target-dir /test/sqoop/userdb/emp # --last-value 是上一次导入的最后一条数据；如果不指定会出现数据重复 # 导入完成后显示 # Incremental import complete! To run another incremental import of all data following this import, supply the following arguments: # 20/04/22 11:26:42 INFO tool.ImportTool: --incremental append # 20/04/22 11:26:42 INFO tool.ImportTool: --check-column id # 20/04/22 11:26:42 INFO tool.ImportTool: --last-value 1205 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb --username root --password root --table emp \\ --num-mappers 1 \\ --fields-terminated-by \"\\t\" --target-dir /test/sqoop/userdb/emp \\ --check-column id --incremental append --last-value 1202 增量导入 lastmodified 使用场景：部分业务数据字段更新，更新新增和修改的数据 # --merge-key 合并修改后的数据 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb --username root --password root --table emp \\ --num-mappers 1 \\ --fields-terminated-by \"\\t\" --target-dir /test/sqoop/userdb/emp \\ --check-column update_time --incremental lastmodified --last-value \"2020-04-22 11:25:55\" \\ --merge-key id export 从HDFS（包括Hive和HBase）中将数据导出到关系型数据库中。 导出数据 全量导出，如果目标表没有主键或唯一索引，若目标表已有导出的数据则会出现重复数据；如果目标表有主键或唯一索引，若目标表已有导出的数据则会出现Duplicate异常，导出失败。 因此，要保证目标表是空表，或至少保证导出数据在目标表中不存在。 bin/sqoop export \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --export-dir /user/hive/warehouse/sqooptohive.db/emp/ \\ --num-mappers 1 \\ --input-fields-terminated-by '\\001' NULL值的使用 # 如果hive表中有字段值是\\N，导出时会出现解析数据异常：java.lang.NumberFormatException: For input string: \"\\N\" # --input-null-string '\\\\N' hive中的\\N，字段类型是字符串类型，解析为mysql的null # --input-null-non-string '\\\\N' hive中的\\N，字段类型是非字符串类型，解析为mysql的null bin/sqoop export \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --export-dir /user/hive/warehouse/sqooptohive.db/emp/ \\ --input-fields-terminated-by '\\001' \\ --input-null-string '\\\\N' --input-null-non-string '\\\\N' \\ --num-mappers 1 增量导出 updateonly # --columns 要导出的mysql数据列，但要同hive字段顺序保持一致；对于由系统控制字段如create_time和update_time，不要包括在内（在设计表时，注意将由系统控制字段放在表的最后） # --update-key 可以包含多个列，用逗号隔开，用于匹配mysql唯一数据；且指定字段(多个)必须出现在--columns中；不必是主键或唯一索引 # --update-mode updateonly 只对mysql已有数据更新 bin/sqoop export \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root --password root --table emp \\ --export-dir /user/hive/warehouse/sqooptohive.db/emp/ \\ -m 1 --fields-terminated-by '\\001' \\ --columns id,name,deg,salary,dept \\ --update-key id --update-mode updateonly 增量导出 allowinsert # --update-key 必须注意，指定的字段必须是主键或唯一索引，否则会出现重复数据。原理是，首先向mysql插入，如果出现重复唯一主键错误时，然后尝试更新 # --columns 是hive中的字段，可以没有id，对应mysql中的字段；而--update-key id 是mysql的字段，主键自增 # --update-mode allowinsert mysql已有数据则更新，没有数据则插入 # alter table emp add primary key(id); bin/sqoop export \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root --password root --table emp \\ --export-dir /user/hive/warehouse/sqooptohive.db/emp/ \\ -m 1 --fields-terminated-by '\\001' \\ --columns id,name,deg,salary,dept \\ --update-key id --update-mode allowinsert 临时表 由于Sqoop将导出过程分解为多个事务，因此失败的导出作业可能会导致将部分数据提交到数据库。这可能进一步导致后续作业由于某些情况下的插入冲突而失败，或导致其他作业中的重复数据。可以通过 --staging-table 选项指定临时表来解决此问题，该选项充当用于暂存导出数据的辅助表。分阶段数据最终在单个事务中移动到目标表。 创建临时表 -- 创建临时表 CREATE TABLE `emp_tmp` ( `id` INT(11) DEFAULT NULL, `name` VARCHAR(100) DEFAULT NULL, `deg` VARCHAR(100) DEFAULT NULL, `salary` INT(11) DEFAULT NULL, `dept` VARCHAR(10) DEFAULT NULL, `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `is_delete` BIGINT(20) DEFAULT '1' ) ENGINE = INNODB DEFAULT CHARSET = latin1; 导出数据 # --staging-table 在插入目标表之前将暂存数据的表 # --clear-staging-table 在数据处理之前删除临时表中所有数据 # 不可与 --update-key 更新模式共用，更新已存在数据（会出现重复数据或者主键重复） bin/sqoop export \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root --password root --table emp \\ --export-dir /user/hive/warehouse/sqooptohive.db/emp/ \\ -m 1 --fields-terminated-by '\\001' \\ --columns id,name,deg,salary,dept \\ --update-mode allowinsert \\ --staging-table emp_tmp --clear-staging-table codegen 将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段，以及相关方法。 # --bindir 本地目录 bin/sqoop codegen \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --bindir /home/hadoop/empjar \\ --class-name Emp \\ --fields-terminated-by \"\\t\" create-hive-table 生成与关系数据库表结构对应的hive表结构。 bin/sqoop create-hive-table \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --hive-table sqooptohive.emp_struc eval 可以快速的使用SQL语句对关系型数据库进行操作，经常用于在import数据之前，了解一下SQL语句是否正确，数据是否正常，并可以将结果显示在控制台。 bin/sqoop eval \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --query \"SELECT * FROM emp\" import-all-tables 可以将RDBMS中的所有表导入到HDFS中，每一个表都对应一个HDFS目录。 bin/sqoop import-all-tables \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --warehouse-dir /test/sqoop/userdb \\ -m 1 job # 创建job # 注意import-all-tables和它左边的--之间有一个空格 # 如果需要连接metastore，则--meta-connect jdbc:hsqldb:hsql://node03:16000/ bin/sqoop job \\ --create myjob \\ -- import-all-tables \\ --connect jdbc:mysql://node03:3306/userdb --username root --password root \\ -m 1 # 列出所有job bin/sqoop job --list # 执行job bin/sqoop job --exec myjob # 删除job bin/sqoop job --delete myjob # 列出job的参数，需要输入数据库的密码 bin/sqoop job --show myjob 在执行一个job时，如果需要手动输入数据库密码，可以做如下优化 sqoop.metastore.client.record.password true If true, allow saved passwords in the metastore. list-databases 列出所有数据库 bin/sqoop list-databases \\ --connect jdbc:mysql://node03:3306 \\ --username root \\ --password root list-tables 列出数据库的所有表 bin/sqoop list-tables \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root merge 准备数据 # 旧数据 # 1201 gopal manager null null 2020-04-24 16:28:05.0 2020-04-24 16:28:05.0 1 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb --username root --password root --table emp \\ --num-mappers 1 \\ --fields-terminated-by \"\\t\" --target-dir /test/sqoop/userdb/merge/old # 新数据 # 1201 gopal manager 50000 TP 2020-04-25 16:38:58.0 2020-04-25 16:38:58.0 1 # 1202 manisha Proof reader 50000 TP 2020-04-25 16:38:59.0 2020-04-25 16:38:59.0 1 bin/sqoop import \\ --connect jdbc:mysql://node03:3306/userdb --username root --password root --table emp \\ --num-mappers 1 \\ --fields-terminated-by \"\\t\" --target-dir /test/sqoop/userdb/merge/new 创建JavaBean # --fields-terminated-by \"\\t\" 必须存在，否则分隔字段数据时会出现解析问题 bin/sqoop codegen \\ --connect jdbc:mysql://node03:3306/userdb \\ --username root \\ --password root \\ --table emp \\ --bindir /home/hadoop/sqoopdatas/empjar \\ --class-name Emp \\ --fields-terminated-by \"\\t\" 合并 # 1201 gopal manager 50000 TP 2020-04-25 16:38:58.0 2020-04-25 16:38:58.0 1 # 1202 manisha Proof reader 50000 TP 2020-04-25 16:38:59.0 2020-04-25 16:38:59.0 1 # --new-data HDFS 待合并的数据目录，合并后在新的数据集中保留 # --onto HDFS合并后，重复的部分在新的数据集中被覆盖 # --target-dir 合并后的数据在HDFS里存放的目录 bin/sqoop merge \\ --new-data /test/sqoop/userdb/merge/new/ \\ --onto /test/sqoop/userdb/merge/old/ \\ --target-dir /test/sqoop/userdb/merge/result \\ --jar-file /home/hadoop/sqoopdatas/empjar/Emp.jar \\ --class-name Emp \\ --merge-key id metastore 记录了Sqoop job的元数据信息，如果不启动该服务，那么默认job元数据的存储目录为~/.sqoop，可在sqoop-site.xml中修改。 # 启动sqoop的metastore服务 bin/sqoop metastore # 关闭sqoop的metastore服务 bin/sqoop metastore --shutdown help 打印sqoop帮助信息 bin/sqoop help metastore version 打印sqoop版本信息 bin/sqoop version Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/auxiliaryframework/azkaban/Azkaban安装部署.html":{"url":"src/bigdata/auxiliaryframework/azkaban/Azkaban安装部署.html","title":"Azkaban安装部署","keywords":"","body":"编译 需要使用jdk1.8 sudo yum -y install wget sudo yum -y install git sudo yum -y install gcc-c++ cd /bigdata/soft/ wget https://github.com/azkaban/azkaban/archive/3.51.0.tar.gz mv 3.51.0.tar.gz azkaban-3.51.0.tar.gz tar -zxvf azkaban-3.51.0.tar.gz -C ../install/ cd /bigdata/install/azkaban-3.51.0 ./gradlew build installDist -x test 经过漫长的编译后安装文件列表如下 azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz /bigdata/install/azkaban-3.51.0/azkaban-exec-server/build/distributions azkaban-web-server-0.1.0-SNAPSHOT.tar.gz /bigdata/install/azkaban-3.51.0/azkaban-web-server/build/distributions azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz /bigdata/install/azkaban-3.51.0/azkaban-solo-server/build/distributions execute-as-user.c（two server mode 需要的C程序） /bigdata/install/azkaban-3.51.0/az-exec-util/src/main/c create-all-sql-0.1.0-SNAPSHOT.sql（数据库脚本） /bigdata/install/azkaban-3.51.0/azkaban-db/build/install/azkaban-db 服务安装 数据库准备 登录mysql客户端 mysql -uroot -proot 执行命令 CREATE DATABASE azkaban; -- % 可以在任意远程主机登录 -- SHOW VARIABLES LIKE 'validate_password%'; 密码有效期 -- SET GLOBAL validate_password_length = 7; 密码长度 -- SET GLOBAL validate_password_number_count = 0; 密码中数字个数 -- SET GLOBAL validate_password_mixed_case_count = 0; 混合大小写个数 -- SET GLOBAL validate_password_special_char_count = 0; 特殊字符个数 CREATE USER 'azkaban'@'%' IDENTIFIED BY 'azkaban'; GRANT all privileges ON azkaban.* to 'azkaban'@'%' identified by 'azkaban' WITH GRANT OPTION; flush privileges; use azkaban; source /bigdata/soft/azkaban/create-all-sql-0.1.0-SNAPSHOT.sql 解压安装包 tar -xzvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gz -C ../../install/ tar -xzvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz -C ../../install/ mv azkaban-web-server-0.1.0-SNAPSHOT/ azkaban-web-server-3.51.0 mv azkaban-exec-server-0.1.0-SNAPSHOT/ azkaban-exec-server-3.51.0 azkaban-web-server 安装 安装SSL安全认证 允许使用https的方式访问azkaban的web服务 cd /bigdata/install/azkaban-web-server-3.51.0 # 密码 azkaban keytool -keystore keystore -alias jetty -genkey -keyalg RSA 修改配置文件 azkaban.properties vi azkaban.properties azkaban.name=Azkaban azkaban.label=My Azkaban default.timezone.id=Asia/Shanghai jetty.use.ssl=true jetty.ssl.port=8443 jetty.keystore=/bigdata/install/azkaban-web-server-3.51.0/keystore jetty.password=azkaban jetty.keypassword=azkaban jetty.truststore=/bigdata/install/azkaban-web-server-3.51.0/keystore jetty.trustpassword=azkaban mysql.host=node03 # azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus azkaban.activeexecutor.refresh.milisecinterval=10000 azkaban.queueprocessing.enabled=true azkaban.activeexecutor.refresh.flowinterval=10 azkaban.executorinfo.refresh.maxThreads=10 azkaban-exec-server 安装 修改配置文件 azkaban.properties azkaban.name=Azkaban azkaban.label=My Azkaban default.timezone.id=Asia/Shanghai jetty.use.ssl=true jetty.keystore=/bigdata/install/azkaban-web-server-3.51.0/keystore jetty.password=azkaban jetty.keypassword=azkaban jetty.truststore=/bigdata/install/azkaban-web-server-3.51.0/keystore jetty.trustpassword=azkaban azkaban.webserver.url=https://node03:8443 mysql.host=node03 插件 添加插件 sudo yum -y install gcc-c++ cp execute-as-user.c /bigdata/install/azkaban-exec-server-3.51.0/plugins/jobtypes cd /bigdata/install/azkaban-exec-server-3.51.0/plugins/jobtypes gcc execute-as-user.c -o execute-as-user sudo chown root execute-as-user sudo chmod 6050 execute-as-user 修改配置文件 commonprivate.properties cd /bigdata/install/azkaban-exec-server-3.51.0/plugins/jobtypes vi commonprivate.properties memCheck.enabled=false azkaban.native.lib=/bigdata/install/azkaban-exec-server-3.51.0/plugins/jobtypes 启动服务 # AzkabanExecutorServer cd /bigdata/install/azkaban-exec-server-3.51.0 bin/start-exec.sh # 激活 # {\"status\":\"success\"} curl -G \"node03:$( 停止服务 # AzkabanExecutorServer cd /bigdata/install/azkaban-exec-server-3.51.0 bin/shutdown-exec.sh # AzkabanWebServer cd /bigdata/install/azkaban-web-server-3.51.0 bin/shutdown-web.sh Web访问 # AzkabanWebServer地址 # 用户名和密码：azkaban https://node03:8443 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/auxiliaryframework/azkaban/Azkaban工作流调度器.html":{"url":"src/bigdata/auxiliaryframework/azkaban/Azkaban工作流调度器.html","title":"Azkaban工作流调度器","keywords":"","body":"概述 Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件（properties）格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪工作流。 功能 兼容任何版本的Hadoop 在编译的时候，只需要指定Hadoop的版本 易于使用的Web UI 简单的web和http工作流上传 只需要预先将workflow定义好以后，就可以通过浏览器把我们需要的job的配置文件传到Azkaban的web server 项目工作区 不同的项目可以归属于不同的空间，而且不同的空间又可以设置不同的权限。多个项目之间是不会产生任何的影响与干扰 工作流调度 可以手工运行，也可以定时处理 模块化和可插拔的插件机制 认证和授权 跟踪用户的行为 发送失败和成功的电子邮件警报 SLA警告 失败作业的重试机制 基本架构 Azkaban Web Server 提供了Web UI，是azkaban的主要管理者，包括 project 的管理，认证，调度，对工作流执行过程的监控等。 Azkaban Executor Server 负责具体的工作流和任务的调度提交。 Mysql 用于保存项目、日志或者执行计划之类的信息。 运行模式 solo server mode web server 和 executor server运行在一个进程。最简单的模式，数据库内置的H2数据库，管理服务器和执行服务器都在一个进程中运行，任务量不大项目可以采用此模式。 two server mode web server 和 executor server运行在不同的进程。数据库为mysql，管理服务器和执行服务器在不同进程，这种模式下，管理服务器和执行服务器互不影响。 multiple executor mode web server 和 executor server运行在不同的进程，executor server有多个。该模式下，执行服务器和管理服务器在不同主机上，且执行服务器可以有多个。 示例 Azkaba内置的任务类型支持command、java。 Command类型单一job 创建job # 创建job文件 touch mycommand.job vi mycommand.job type=command command=echo 'hello azkaban' # 创建zip zip -r -q mycommand.zip * 创建 command project 上传 mycommand.zip（必须上传zip文件） 执行 Execute Flow Command类型多个job 创建job # 创建job文件 touch start.job touch task1.job touch task2.job touch stop.job vi start.job type=command command=echo '========start========' vi task1.job type=command dependencies=start command=echo '========task1========' vi task2.job type=command dependencies=start command=echo \"========task2========\" vi stop.job type=command dependencies=task1,task2 command=echo \"========stop========\" # 创建zip zip -r -q dependencies.zip * 创建 dependencies project 上传 dependencies.zip 执行 HDFS任务 创建job # 创建job文件 touch fs.job vi fs.job type=command command=echo \"start execute\" command.1=/bigdata/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -mkdir /test/azkaban command.2=/bigdata/install/hadoop-2.6.0-cdh5.14.2/bin/hdfs dfs -put /etc/profile /test/azkaban # 创建zip zip -r -q fs.zip * 创建 fs project 上传 fs.zip 执行 MapReduce任务 创建job # 创建job文件 touch mr.job vi mr.job type=command command=/bigdata/install/hadoop-2.6.0-cdh5.14.2/bin/hadoop jar /bigdata/install/hadoop-2.6.0-cdh5.14.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.14.2.jar pi 3 5 # 创建zip zip -rq mr.zip * 创建 mr project 上传 mr.zip 执行 Hive任务 创建job sql脚本hive.sql create database if not exists azhive; use azhive; create table if not exists aztest(id string,name string) row format delimited fields terminated by '\\t'; job文件hive.job type=command command=/bigdata/install/hive-1.1.0-cdh5.14.2/bin/hive -f 'hive.sql' zip文件 zip -rq hive.zip * 创建 hive project 上传 hive.zip 执行 定时任务 使用azkaban的scheduler功能可以实现对我们的作业任务进行定时调度功能。 Execute Flow Schedule 选项 | 字段 | 强制性 | 范围 | 特殊字符 | | ------------ | ------ | ------------------------------------ | ------------------------------------------------------------ | | Min | 可选 | 0-59 | 任意匹配值, 分隔- 范围值/ 单位间隔 | | Hours | 必须 | 0-23 | 任意匹配值, 分隔- 范围值/ 单位间隔 | | Day of Month | yes | 1-31 | 任意匹配值, 分隔- 范围值/ 单位间隔? 为空值，当两个相似概念的字段中，只有一个字段起作用，另一个不起作用，可以使用?，同Day of Week搭配使用 | | Month | yes | 1-12 | 任意匹配值, 分隔- 范围值/ 单位间隔 | | Day of Week | yes | 1-7SUN MON TUE WED THU FRI SAT | 任意匹配值, 分隔- 范围值/ 单位间隔? 为空值 | | Year | no | | 任意匹配值, 分隔- 范围值/ 单位间隔 | 举例 /1 ? 每分钟执行一次定时调度任务 0 1 ? 每天晚上凌晨一点执行调度任务 0 /2 ? * 每隔两个小时定时执行调度任务 30 21 ? 每天晚上九点半定时执行调度任务 WebUI参数传递 创建job vi param.job type=command # ${param}解析页面传递的参数 # parameter声明一个变量 myparam=${param} command=echo ${myparam} 创建 param project 上传 param.zip 执行 flow parameters，添加传入的参数 name=param，value=lucky coming... Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/kafka/Kafka集群安装部署.html":{"url":"src/bigdata/kafka/Kafka集群安装部署.html","title":"Kafka集群安装部署","keywords":"","body":"先决条件 启动ZooKeeper集群 安装 在node01执行 tar -xzvf kafka_2.11-1.0.1.tgz -C ../install/ 修改node01配置文件 server.properties # 每个broker唯一 broker.id=0 # 数据存放目录 log.dirs=/bigdata/install/kafka_2.11-1.0.1/kafka-logs # 指定zk地址 zookeeper.connect=node01:2181,node02:2181,node03:2181 # 指定是否可以删除topic，默认是false 表示不可以删除 delete.topic.enable=true # 指定broker主机名 host.name=node01 分发 分发到node02和node03 scp -r kafka_2.11-1.0.1 node02:/bigdata/install scp -r kafka_2.11-1.0.1 node03:/bigdata/install 修改node02配置文件 server.properties broker.id=1 host.name=node02 修改node03配置文件 server.properties broker.id=2 host.name=node03 启动 所有节点分别运行 # /bigdata/install/kafka_2.11-1.0.1/logs 是log4j应用日志 nohup bin/kafka-server-start.sh /bigdata/install/kafka_2.11-1.0.1/config/server.properties > /dev/null 2>&1 & 停止 bin/kafka-server-stop.sh Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/kafka/Kafka监控工具.html":{"url":"src/bigdata/kafka/Kafka监控工具.html","title":"Kafka监控工具","keywords":"","body":"Kafka Manager kafkaManager是由雅虎开源的可以监控整个kafka集群相关信息的一个工具。 可以管理几个不同的集群 监控集群的状态(topics, brokers, 副本分布, 分区分布) 创建topic、修改topic相关配置 安装 # 安装zip工具 sudo yum install -y zip sudo yum install -y unzip scp kafka-manager-1.3.0.4.zip hadoop@node01:/bigdata/soft unzip kafka-manager-1.3.0.4.zip -d ../install 配置 cd /bigdata/install/kafka-manager-1.3.0.4/conf vi application.conf kafka-manager.zkhosts=\"node01:2181,node02:2181,node03:2181\" 启动 启动ZooKeeper集群 启动kafka集群 启动 kafka manager 服务 注意：必须使用root用户执行命令启动 kafka manager 服务 cd /bigdata/install/kafka-manager-1.3.0.4 # 切换到root用户下执行命令 su root nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 & 停止 # 查看后台运行进程 jobs -l # 结束进程 kill -9 28754 使用 访问服务地址 http://node01:8080/ 添加Cluster Cluster | Add Cluster Cluster name: kafka Cluster ZooKeeper hosts: node01:2181,node02:2181,node03:2181 KafkaOffsetMonitor 该监控是基于一个jar包的形式运行，部署较为方便。只有监控功能，使用起来也较为安全。 消费者组列表 查看topic的历史消费信息 每个topic的所有parition列表 对consumer消费情况进行监控，并可列出每个consumer offset，滞后数据 安装 cd /bigdata/install/ mkdir kafka_offset_moitor scp KafkaOffsetMonitor-assembly-0.2.0.jar hadoop@node01:/bigdata/install/kafka_offset_moitor 在 kafka_offset_moitor 目录下新建 start_kafka_web.sh vi start_kafka_web.sh #!/bin/sh java -cp KafkaOffsetMonitor-assembly-0.2.0.jar com.quantifind.kafka.offsetapp.OffsetGetterWeb --zk node01:2181,node02:2181,node03:2181 --port 8089 --refresh 10.seconds --retain 1.days 启动 nohup sh start_kafka_web.sh & 停止 jps # OffsetGetterWeb kill -9 29966 使用 访问服务地址 http://node01:8089/ Kafka Eagle（推荐） 安装 scp kafka-eagle-bin-1.2.3.tar.gz hadoop@node01:/bigdata/soft/ tar -xzvf kafka-eagle-bin-1.2.3.tar.gz -C ../install/ cd /bigdata/install/kafka-eagle-bin-1.2.3 tar -zxvf kafka-eagle-web-1.2.3-bin.tar.gz 配置 cd /bigdata/install/kafka-eagle-bin-1.2.3/kafka-eagle-web-1.2.3/conf vi system-config.properties system-config.properties kafka.eagle.zk.cluster.alias=cluster1 cluster1.zk.list=node01:2181,node02:2181,node03:2181 kafka.eagle.sasl.client=/bigdata/install/kafka-eagle-bin-1.2.3/kafka-eagle-web-1.2.3/conf/kafka_client_jaas.conf # 数据库自动构建 kafka.eagle.driver=com.mysql.jdbc.Driver kafka.eagle.url=jdbc:mysql://node03:3306/ke?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull kafka.eagle.username=root kafka.eagle.password=root 环境变量 sudo vi /etc/profile export KE_HOME=/bigdata/install/kafka-eagle-bin-1.2.3/kafka-eagle-web-1.2.3 export PATH=$PATH:$KE_HOME/bin # 立即生效 source /etc/profile 启动 启动mysql服务 运行 Kafka Eagle cd /bigdata/install/kafka-eagle-bin-1.2.3/kafka-eagle-web-1.2.3/bin # Bootstrap java进程 sh ke.sh start 停止 cd /bigdata/install/kafka-eagle-bin-1.2.3/kafka-eagle-web-1.2.3/bin sh ke.sh stop 使用 访问服务地址 http://node01:8048/ke 默认用户名：admin 默认密码：123456 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/kafka/Kafka分布式消息流平台.html":{"url":"src/bigdata/kafka/Kafka分布式消息流平台.html","title":"Kafka分布式消息流平台","keywords":"","body":"概述 为什么要有消息系统 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。 灵活性 & 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性） 缓冲 有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 核心概念 Kafka最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。 kafka是一个分布式消息队列。具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。Kafka就是一种发布-订阅模式。将消息保存在磁盘中，以顺序读写方式访问磁盘，避免随机读写导致性能瓶颈。 特性 高吞吐、低延迟 kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒。 高伸缩性 每个主题（topic）包含多个分区（partition），主题中的分区可以分布在不同的主机（broker）中。 持久性、可靠性 Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失。 容错性 允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作。 高并发 支持数千个客户端同时读写。 集群架构 producer 消息生产者，发布消息到Kafka集群的终端或服务。 consumer 从Kafka集群中消费消息的终端或服务。 consumer group 每个 consumer 都属于一个 consumer group。 每条消息只能被 consumer group 中的一个 Consumer 消费，但可以被多个 consumer group 中的 consumer消费。 broker Kafka集群中包含的服务器，一个borker就表示kafka集群中的一个节点。 topic 每条发布到Kafka集群的消息属于的类别，即Kafka是面向 topic 的。更通俗的说Topic就像一个消息队列，生产者可以向其写入消息，消费者可以从中读取消息，一个Topic支持多个生产者或消费者同时订阅它，所以其扩展性很好。 每条消息都要指定一个topic。 partition 每个 topic 包含一个或多个partition。Kafka分配的单位是partition。 物理上的概念，每个topic包含一个或多个partition，一个partition对应一个文件夹，这个文件夹下存储partition的数据和索引文件，每个partition内部是有序的。 消费者组的不同消费者不允许消费同一个partition。 每条消息可被不同的消费者组消费，但只可被消费者组的一个消费者消费。 如果 消费者数 > partition ，则多余的空闲；如果 消费者数 ，则会有消费者消费多个partition。 replica partition的副本，保障 partition 的高可用。 leader 每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。producer 和 consumer 只跟 leader 交互。 follower Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。 controller 集群运行时，broker向ZooKeeper注册并选举出一个controller。 controller负责Leader Partition的选举、感知集群中的其他broker、管理集群中的元数据。 zookeeper Kafka 通过 zookeeper 来存储集群的meta元数据信息。 一旦controller所在broker宕机，此时临时节点消失，集群里其他broker会一直监听这个临时节点，发现临时节点消失了，就争抢再次创建临时节点，保证有一台新的broker会成为controller角色。 offset 消费者在对应分区上已经消费的消息数（位置），offset保存的地方跟kafka版本有关系。 kafka0.8 版本之前offset保存在zookeeper上。 kafka0.8 版本之后offset保存在kafka集群上。它是把消费者消费topic的位置保存在kafka集群内部有一个默认的topic，名称叫 __consumer_offsets，它默认有50个分区。 ISR replica机制可以保证Kafka集群的高可用，但无法保证数据的一致性（不丢失）。如leader宕机，此时还没有把数据同步到follower上，即使选举出follower作为新的leader，但未同步的数据却丢失了。 ISR（in-sync replica）就是同leader partition保持同步的follower partition的数量，只有处于ISR列表中的follower才可以在leader宕机之后被选举为新的leader，因为在这个ISR列表里代表他的数据同leader是同步的。 命令行的管理使用 创建topic bin/kafka-topics.sh --create --partitions 2 --replication-factor 2 --topic test --zookeeper node01:2181,node02:2181,node03:2181 查询topic bin/kafka-topics.sh --list --zookeeper node01:2181,node02:2181,node03:2181 查询topic的描述信息 bin/kafka-topics.sh --describe --topic test --zookeeper node01:2181,node02:2181,node03:2181 删除topic bin/kafka-topics.sh --delete --topic test --zookeeper node01:2181,node02:2181,node03:2181 模拟生产者写入数据到topic中 # ctrl+c 退出 bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test 模拟消费者拉取topic中的数据 可以查看push数据情况 # offset -> ZooKeeper bin/kafka-console-consumer.sh --zookeeper node01:2181,node02:2181,node03:2181 --topic test --from-beginning # offset -> kafka broker(推荐) bin/kafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --topic test --from-beginning 分区策略 kafka的分区策略决定了producer生产者产生的一条消息最后会写入到topic的哪一个分区中。 指定具体的分区号 producer.send(new ProducerRecord<>(UtilFactory.TOPIC_TEST, 0, String.valueOf(0), String.valueOf(0))); 按照key的hashcode（kafka规则）分区 // 相同key到同一个分区 // 分区=hashcode(key)%分区数 producer.send(new ProducerRecord<>(UtilFactory.TOPIC_TEST, \"key-1\", \"rain\")); producer.send(new ProducerRecord<>(UtilFactory.TOPIC_TEST, \"key-0\", \"yo2\")); producer.send(new ProducerRecord<>(UtilFactory.TOPIC_TEST, \"key-0\", \"lucky\")); producer.send(new ProducerRecord<>(UtilFactory.TOPIC_TEST, \"key-1\", \"big rice\")); 不指定key按照value轮循分区 for (int i = 0; i (UtilFactory.TOPIC_TEST, String.valueOf(i))); } 自定义分区 自定义分区实现 注意此处同 按照key的hashcode（kafka规则）分区 分区策略有区别，前一个是针对key直接hashcode求余分区数得到所属分区，后一个有自己的规则，但可以保证相同的key去到同一个分区。 public class CustomHashPartition implements Partitioner { @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { int size = cluster.partitionCountForTopic(UtilFactory.TOPIC_TEST); return Math.abs(key.hashCode() % size); } @Override public void close() { } @Override public void configure(Map configs) { } } 测试 Map props = new HashMap<>(); props.put(\"partitioner.class\", \"com.sciatta.hadoop.kafka.example.partition.CustomHashPartition\"); producer = UtilFactory.getKafkaProducer(props); for (int i = 0; i (UtilFactory.TOPIC_TEST, String.valueOf(i), String.valueOf(i))); } producer.close(); 文件存储机制 同一个topic下有多个不同的partition，每个partition为一个目录，partition命名的规则是topic的名称加上一个序号，序号从0开始，如：test-0。 每一个partition目录下的文件被平均切割成大小相等（默认一个文件是1G，可以手动去设置）的数据文件，每一个数据文件都被称为一个段（segment file），但每个段消息数量不一定相等，这种特性能够使得老的segment可以被快速清除。默认保留7天的数据。 每次满1G后，再写入到一个新的文件中。 每个partition只需要支持顺序读写就可以。如：00000000000000000000.log是最早产生的文件，该文件达到1G后又产生了新的00000000000002025849.log文件，新的数据会写入到这个新的文件里面。这个文件到达1G后，数据又会写入到下一个文件中。也就是说它只会往文件的末尾追加数据，这就是顺序写的过程，生产者只会对每一个partition做数据的追加（写操作）。 数据消费有序性问题 在分布式场景下，一个topic中包含多个partition，每一个partition分布在不同的broker上，只能做到每一个partition内部间隔有序，不能做到全局有序。如果只有一个partition，当然就可以实现全局有序，但与分布式，负载均衡的理念相违背。 segment文件 生产者生产的消息按照一定的分区策略被发送到topic中的不同partition，partition在磁盘上就是一个目录，该目录名是topic的名称加上一个序号。 在这个partition目录下，有两类文件，一类是以log为后缀的文件，一类是以index为后缀的文件，每一个log文件和一个index文件相对应，这一对文件就是一个segment file，也就是一个段。（segment file=log file+index file） 其中的log文件就是数据文件，里面存放的就是消息，而index文件是索引文件，索引文件记录了元数据信息。 log文件达到1G后滚动生成新的log文件。 segment文件命名的规则：partition全局的第一个segment从0（20个0）开始，后续的每一个segment文件名是上一个segment文件中最后一条消息的offset值。 这样设计的好处是方便定位消费某一条是offset的消息。也就是说，首先把所有的log文件名排序，然后通过二分查找法定位offset所在的log文件，然后在index文件中找到offset所对应数据在log文件中的物理偏移位置。 如何快速查询数据 Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。 通过索引信息可以快速定位message。 index文件内容是 x,y 形式，其中x是当前log的第x条数据（查找的offset - log的文件名），y是所在log文件的物理偏移位置。 在index文件中x不连续，稀疏存储，index文件保存在内存中，提高查询效率。 如果查找第z条数据，查找小于z的最大x值，然后通过x在log文件中的物理偏移位置，顺序扫描，找到第z条数据。 log文件中的一条消息数据的固定物理结构包括：offset（8 Bytes）、message size（4 Bytes）、crc32（4 Bytes）、magic（1 Byte）、attributes（1 Byte）、key length（4 Bytes）、key（K Bytes）、payload length（4 Bytes）、value bytes payload（K Bytes）。 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 高吞吐、低延时的原因 Kafka是大数据领域无处不在的消息中间件，目前广泛使用在企业内部的实时数据管道，并帮助企业构建自己的流计算应用程序。Kafka虽然是基于磁盘做的数据存储，但却具有高性能、高吞吐、低延时的特点，其吞吐量动辄几万、几十上百万，这其中的原由值得我们一探究竟。 顺序读写 磁盘顺序读写性能要高于内存的随机读写。 不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高，一般而言要高出磁盘随机读写三个数量级，一些情况下磁盘顺序读写性能甚至要高于内存随机读写。 磁盘的顺序读写是磁盘使用模式中最有规律的，并且操作系统也对这种模式做了大量优化，Kafka就是使用了磁盘顺序读写来提升的性能。Kafka的message是不断追加到本地磁盘文件末尾的，而不是随机的写入，这使得Kafka写入吞吐量得到了显著提升。 Page Cache（页缓存） 为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有： 避免Object消耗：如果是使用Java堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。 避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题。 零拷贝 零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。Kafka利用linux操作系统的 \"零拷贝（zero-copy）\" 机制在消费端做的优化。 传统方式，读取磁盘文件并进行网络发送，经过的四次数据copy是非常繁琐的（内核态和用户态切换）。实际IO读写，需要进行IO中断，需要CPU响应中断（上下文切换），尽管后来引入DMA（Direct Memory Access）来接管CPU的中断请求（DMA可以绕过CPU，由硬件自己去直接访问系统主内存），但四次copy是存在“不必要的拷贝”的。 读取磁盘文件后，不需要做其他处理，直接用网络发送出去。 分区分段 索引优化 批量读写 整合flume flume 配置 flume-kafka.conf a1.sources = r1 a1.channels = c1 a1.sinks = k1 a1.sources.r1.type = exec a1.sources.r1.command = tail -F /home/hadoop/flumedatas/taillogs/access_log a1.sources.r1.channels = c1 a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.sinks.k1.channel = c1 a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.topic = flumetest a1.sinks.k1.kafka.bootstrap.servers = node01:9092,node02:9092,node03:9092 a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 创建topic bin/kafka-topics.sh --create --topic flumetest --partitions 2 --replication-factor 2 --zookeeper node01:2181,node02:2181,node03:2181 启动flume flume-ng agent -n a1 -c /bigdata/install/apache-flume-1.6.0-cdh5.14.2-bin/conf -f flume-kafka.conf -Dflume.root.logger=info,console 向flume发送数据 echo \"hello\" >> /home/hadoop/flumedatas/taillogs/access_log echo \"flume to kafka\" >> /home/hadoop/flumedatas/taillogs/access_log 验证kafka数据写入成功 bin/kafka-console-consumer.sh --topic flumetest --bootstrap-server node01:9092,node02:9092,node03:9092 --from-beginning 内核原理 HW&LEO基本概念 Base Offset：是起始位移，该副本中第一条消息的offset，如下图，这里的起始位移是0，如果一个日志文件写满1G后（默认1G后会log rolling），这个起始位移就不是0开始了。 HW（high watermark）：副本的高水印值，replica中leader副本和follower副本都会有这个值，通过它可以得知副本中已提交或已备份消息的范围，leader副本中的HW，决定了消费者能消费的最新消息能到哪个offset。如下图所示，HW值为8，代表offset为 [0,8) 的8条消息都可以被消费到，它们是对消费者可见的，而[8,13) 这5条消息由于未提交，对消费者是不可见的。注意HW最多达到LEO值，这时消费的消息范围就是[0,13) 。 LEO（log end offset）：日志末端位移，代表日志文件中下一条待写入消息的offset，这个offset上实际是没有消息的。不管是leader副本还是follower副本，都有这个值。当leader副本收到生产者的一条消息，LEO通常会自增1，而follower副本需要从leader副本fetch到数据后，才会增加它的LEO，最后leader副本会比较自己的LEO以及满足条件的follower副本上的LEO，选取两者中较小值作为新的HW，来更新自己的HW值。 ISR（in-sync replica）：就是同leader partition保持同步的follower partition的数量，只有处于ISR列表中的follower才可以在leader宕机之后被选举为新的leader，因为在这个ISR列表里代表他的数据同leader是同步的。 HW&LEO更新流程 LEO和HW的更新，需要区分leader副本和follower副本。 LEO 包括leader副本和follower副本。 leader LEO：leader的LEO就保存在其所在的broker的缓存里，当leader副本log文件写入消息后，就会更新自己的LEO。 remote LEO和follower LEO：remote LEO是保存在leader副本上的follower副本的LEO，可以看出leader副本上保存所有副本的LEO，当然也包括自己的。follower LEO就是follower副本的LEO，因此follower相关的LEO需要考虑两种情况: 如果是remote LEO，更新前leader需要确认follower的fetch请求包含的offset，这个offset就是follower副本的LEO，根据它对remote LEO进行更新。如果未收到fetch请求，或者fetch请求在请求队列中排队，则不做更新。可以看出在leader副本给follower副本返回数据之前，remote LEO就先更新了。 如果是follower LEO，它的更新是在follower副本得到leader副本发送的数据并随后写入到log文件，就会更新自己的LEO。 HW 包括leader副本和follower副本。 leader HW：它的更新是有条件的： producer向leader写消息，会尝试更新。 leader处理follower的fetch请求，先读取log数据，然后尝试更新HW。 副本成为leader副本时，会尝试更新HW。 broker崩溃可能会波及leader副本，也需要尝试更新。 更新时会比较所有满足条件的副本的LEO，包括自己的LEO和remote LEO，选取最小值作为更新后的leader HW。这里的满足条件时： 处于ISR中 副本LEO落后于leader LEO的时长不大于replica.lag.time.max.ms参数值（默认值是10秒），或者落后Leader的条数不大于预定值replica.lag.max.messages（默认值是4000） follower HW：更新发生在follower副本更新LEO之后，一旦follower向log写完数据，它就会尝试更新HW值。比较自己的LEO值与fetch响应中leader副本的HW值，取最小者作为follower副本的HW值。可以看出，如果follower的LEO值超过了leader的HW值，那么follower HW值是不会超过leader HW值的。 producer消息发送流程 消息发送过程中，涉及两个线程协同工作。主线程首先将业务数据封装成ProducerRecord对象，之后调用send()方法将消息放入RecordAccumulator（消息收集器，也是主线程和sender线程共享的缓冲区）中暂存。Sender线程负责将消息信息构成请求，最终执行网络I/O的线程，它从RecordAccumulator中取出消息并批量发送出去。 ProducerInterceptors对消息进行拦截。 Serializer对消息的key和value进行序列化。 Partitioner为消息选择合适的Partition。 RecordAccumulator收集消息，实现批量发送。RecordAccumulator是一个缓冲区，可以缓存一批数据，把topic的每一个分区数据存在一个队列中，然后封装消息成一个一个的batch批次，最后实现数据分批次批量发送。 Sender从RecordAccumulator获取消息。 构造ClientRequest。 将ClientRequest交给NetworkClient准备发送。 NetworkClient将请求送入KafkaChannel的缓存。 执行网络I/O，发送请求到kafka集群。 收到响应，调用ClientRequest的回调函数。 调用RecordBatch的回调函数，最终调用每个消息上注册的回调函数。 consumer 消费原理 Coordinator Coordinator一般指的是运行在broker上的group Coordinator，用于管理Consumer Group中各个成员，每个KafkaServer都有一个GroupCoordinator实例，管理多个消费者组，主要用于offset位移管理和Consumer Rebalance。 Coordinator存储的信息 对于每个Consumer Group，Coordinator会存储以下信息： 对每个存在的topic，可以有多个消费组group订阅同一个topic（对应消息系统中的广播） 对每个Consumer Group，元数据如下： 订阅的topics列表 Consumer Group配置信息，包括session timeout等 组中每个Consumer的元数据。包括主机名，consumer id 每个正在消费的topic partition的当前offsets Partition的ownership元数据，包括consumer消费的partitions映射关系 如何确定consumer group的coordinator consumer group如何确定自己的coordinator是谁呢？ 简单来说分为两步： 确定consumer group位移信息写入 __consumer_offsets 这个topic的哪个分区。具体计算公式： __consumers_offsets-partition = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount) 注意：groupMetadataTopicPartitionCount由offsets.topic.num.partitions指定，默认是50个分区。 该分区leader所在的broker就是被选定的coordinator offset管理 老版本的位移是提交到zookeeper中的，目录结构是：/consumers//offsets//，但是zookeeper并不适合进行大批量的读写操作，尤其是写操作。 因此，kafka提供了另一种解决方案：增加 __consumer_offsets，将offset信息写入这个topic，摆脱对zookeeper的依赖。__consumer_offsets 中的消息保存了每个consumer group某一时刻提交的offset信息，key是group.id+topic+分区号，value是offset。__consumers_offsets 配置了compact策略，使得它总是能够保存最新的位移信息，既控制了该topic总体的日志容量，也能实现保存最新offset的目的。 Rebalance rebalance本质上是一种协议，规定了一个consumer group下的所有consumer如何达成一致来分配订阅topic的每个分区。比如某个group下有20个consumer，它订阅了一个具有100个分区的topic。正常情况下，Kafka平均会为每个consumer分配5个分区。这个分配的过程就叫rebalance。 触发条件 组成员发生变更。如：新consumer加入组、已有consumer主动离开组或已有consumer崩溃。 订阅主题数发生变更。 订阅主题的分区数发生变更。 协议（protocol） rebalance本质上是一组协议。group与coordinator共同使用它来完成group的rebalance。目前kafka提供了5个协议来处理与consumer group coordination相关的问题： Heartbeat请求：consumer需要定期给coordinator发送心跳来表明自己还活着 LeaveGroup请求：主动告诉coordinator我要离开consumer group SyncGroup请求：group leader把分配方案告诉组内所有成员 JoinGroup请求：成员请求加入组 DescribeGroup请求：显示组的所有信息，包括成员信息，协议名称，分配方案，订阅信息等。通常该请求是给管理员使用 liveness consumer如何向coordinator证明自己还活着？ 通过定时向coordinator发送Heartbeat请求。如果超过了设定的超时时间，那么coordinator就认为这个consumer已经挂了。一旦coordinator认为某个consumer挂了，那么它就会开启新一轮rebalance，并且在当前其他consumer的心跳response中添加“REBALANCE_IN_PROGRESS”，告诉其他consumer重新申请加入组。 Rebalance过程 rebalance的前提是coordinator已经确定。总体而言，rebalance分为2步：Join和Sync。 Join， 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求入组。一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。 Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。 Rebalance分配方案策略 分配方案策略 假设topic有12个分区分别是：p0，p1，p2，p3，p4，p5，p6，p7，p8，p9，p10，p11；三个消费者分别是：c1，c2，c3 RangeAssignor 范围策略（默认） c1：p0，p1，p2，p3 c2：p4，p5，p6，p7 c3：p8，p9，p10，p11 假设c1宕机： c1 c2：p0，p1，p2，p3，p4，p5 c3：p6，p7，p8，p9，p10，p11 RoundRobinAssignor 轮询策略 c1：p0，p3，p6，p9 c2：p1，p4，p7，p10 c3：p2，p5，p8，p11 假设c1宕机： c1 c2：p0，p2，p4，p6，p8，p10 c3：p1，p3，p5，p7，p9，p11 以上两种方案，假设c1宕机，触发Rebalance，都会出现无视历史分配方案的缺陷。 StickyAssignor 黏性策略 c1：p0，p1，p2，p3 c2：p4，p5，p6，p7 c3：p8，p9，p10，p11 假设c1宕机： c1 c2：p0，p1，p4，p5，p6，p7 c3：p2，p3，p8，p9，p10，p11 采用了”有黏性”的策略对所有consumer实例进行分配，可以规避极端情况下的数据倾斜并且在两次rebalance间最大限度地维持了之前的分配方案。 使用场景和配置 如果group下所有consumer实例的订阅是相同的，那么使用round-robin会带来更公平的分配方案，否则使用range策略的效果更好。 用户可以根据consumer参数 partition.assignment.strategy 来进行设置。 核心参数 Producer 常见异常处理 LeaderNotAvailableException Leader副本不可用，导致写入数据失败，等待Leader重新选举。 NotControllerException Controller不可用，等待Controller重新选举。 NetworkException 网络异常。 retries 重新发送数据的次数。默认为0，表示不重试。 retry.backoff.ms 两次重试之间的时间间隔。默认为100ms。 max.in.flight.requests.per.connection 每个网络连接已经发送但还没有收到服务端响应的请求个数最大值。 消息重试是可能导致消息乱序的（如果乱序的消息属于不同分区，则不会出现问题；但如果属于同一个分区，则会违反分区内间隔有序规范），可以使用 max.in.flight.requests.per.connection 参数设置为1，这样可以保证producer必须把一个请求发送的数据发送成功了再发送后面的请求。避免数据出现乱序。 提升消息吞吐量 buffer.memory 设置发送消息的缓冲区。默认值是33554432（32MB）。 如果发送消息出去的速度小于写入消息进去的速度，就会导致缓冲区写满，此时生产消息就会阻塞。 compression.type producer用于压缩数据的压缩类型。默认是none表示无压缩。可以指定gzip、snappy。 压缩最好用于批量处理，批量处理消息越多，压缩性能越好。 batch.size producer批处理消息记录数，以减少请求次数。改善client与server之间的性能。默认是16384Bytes，即16kB，也就是一个batch满了16kB就发送出去。 如果batch太小，会导致频繁网络请求，吞吐量下降；如果batch太大，会导致一条消息需要等待很久才能被发送出去，而且会让内存缓冲区有很大压力，过多数据缓冲在内存里。 linger.ms 消息等待发送时间。默认是0，消息被立即发送。 假设设置为100毫秒，消息进入一个batch，如果100毫秒内，这个batch满了16kB（默认size），自然就会发送出去；但是如果100毫秒内，batch没满，那么也必须把消息发送出去。即不能让消息的发送延迟时间太长，避免给内存造成过大压力。 请求超时 max.request.size 控制发送出去的消息的大小，默认是1048576字节（1MB）。 很多消息可能会超过1MB，一般企业设置为10MB。 request.timeout.ms 请求发送后的超时时间限制，默认是30秒。如果30秒收不到响应，那么就会抛出一个TimeoutException。 ACK参数 acks 0。生产者发数据，不需要等待Leader应答，数据丢失的风险最高，但吞吐量也是最高的。对于一些实时数据分析场景，对数据准确性要求不高的场景适用。 1。需要等待Leader应答。在Leader还没有同步Follower数据时宕机，也会存在丢失数据的可能。 -1 或 all。需要等待Leader应答，并且Leader已同步ISR列表中的所有副本。数据最安全，但性能最差。 min.insync.replicas ISR列表最小副本数。最小为2，才能保证数据不会丢失。 Broker server.properties配置文件核心参数 broker.id 每个broker都必须设置唯一id。 log.dirs kafka所有数据写入这个目录下的磁盘文件中，如果说机器上有多块物理硬盘，那么可以把多个目录挂载到不同的物理硬盘上，然后这里可以设置多个目录，这样kafka可以将数据分散到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。 zookeeper.connect 指向ZooKeeper集群。 listeners broker监听客户端发起请求的端口号，默认是9092。 unclean.leader.election.enable 默认是false（1.0版本之后），只能选举ISR列表的follower成为新的leader。 delete.topic.enable 默认true，允许删除topic。 log.retention.hours 保留数据时间（默认168小时，即7天）。 Consumer heartbeat.interval.ms 默认值：3000 consumer心跳时间，必须得保持心跳才能知道consumer是否故障了，然后如果故障之后，就会通过心跳下发rebalance的指令给其他的consumer通知他们进行rebalance的操作。 session.timeout.ms 默认值：10000kafka多长时间感知不到一个consumer就认为他故障了，默认是10秒。 max.poll.interval.ms 默认值：300000 如果在两次poll操作之间，超过了这个时间，那么就会认为这个consume处理能力不足，会被踢出消费组，分区分配给其他Consumer去消费。 fetch.max.bytes 默认值：1048576 获取一条消息最大的字节数，一般建议设置大一些。 max.poll.records 默认值：500条 一次poll返回消息的最大条数。 connections.max.idle.ms 默认值：540000 consumer跟broker的socket连接如果空闲超过了一定的时间，此时就会自动回收连接，但是下次消费就要重新建立socket连接，这个建议设置为-1，不去回收。 auto.offset.reset 默认值：latest earliest: 当各分区有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费。 latest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据。 none: topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常。 enable.auto.commit 默认值：true 设置为自动提交offset。 auto.commit.interval.ms 默认值：60 * 1000 自动提交偏移量的时间间隔。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/spark/sparkcore/SparkCore核心功能.html":{"url":"src/bigdata/spark/sparkcore/SparkCore核心功能.html","title":"SparkCore核心功能","keywords":"","body":"概述 Apache Spark™ 用于大规模数据处理的统一分析引擎。 速度 使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批处理和流数据的高性能处理。 相比MapReduce快的主要原因： 基于内存 涉及Job迭代，对于MapReduce需要将中间结果输出到HDFS，因此需要大量磁盘IO；而Spark将Job的输出结果可以保存在内存中，因此性能大大提高。需要注意的是，MapReduce和Spark都涉及到shuffle阶段，此阶段产生的数据都会涉及读写磁盘。 线程 MapReduce的任务，以进程的方式独立运行于Yarn集群中；而Spark的任务是以线程的方式运行于进程中。线程的运行开销远低于进程。 易用 可以快速通过 java、scala、python、R 和 SQL 等不同语言编写Spark程序。 通用 Spark生态系统包括Spark SQL（离线分析）、Spark Streaming（实时计算）、MLib（机器学习） 和 GraphX（图计算） 无缝构建应用程序。 兼容 Spark可以运行在 Hadoop（YARN集群）、Apache Mesos、Kubernetes、standalone（Spark集群） 或者 cloud上，同时可以访问各式数据源。其本质就是一个计算逻辑任务，可以提交到不同的可为其提供计算资源的平台上。 集群架构 术语 Application 构建在Spark上的用户程序，包含driver和集群上的executor。 Application jar 包含Spark用户程序的jar文件。注意jar文件不应包含依赖 Hadoop 或 Spark 的库，而是在运行时被添加。 Driver program 运行 main() 函数的进程，创建SparkContext，Spark程序执行的入口。 Cluster manager 获取集群资源的外部服务。如：Spark standalone、Mesos、YARN Deploy mode 以Dirver进程运行位置区分。“cluster”模式，在集群内部启动Driver；“Client”模式，在集群外部启动Driver。 Worker node 在集群中运行Application代码的任意一个节点。 Executor 在Worker节点运行Task的进程，将数据保存在内存或磁盘存储。每一个Application都有自己的Executor。 Task 发送到Executor的一个工作单元。以Executor进程内线程方式运行。 Job 由多个Task组成的一个并行计算，响应一个Spark action。也就是说一个action对于一个Job。 Stage 每一个Job划分为一组更小的Task集合称为Stage，Stage具有依赖关系。 程序运行 spark-submit 本地运行 bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master local[2] \\ --executor-memory 1G \\ --total-executor-cores 2 \\ examples/jars/spark-examples_2.11-2.3.3.jar \\ 10 集群普通模式提交 # --class：指定包含main方法的主类 # --master：指定spark集群master地址 # --executor-memory：指定任务在运行的时候需要的每一个executor内存大小 # --total-executor-cores： 指定任务在运行的时候需要总的cpu核数 bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://node01:7077 \\ --executor-memory 1G \\ --total-executor-cores 2 \\ examples/jars/spark-examples_2.11-2.3.3.jar \\ 10 集群高可用模式提交 # 轮询 --master 列表找到 alive master bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://node01:7077,node03:7077 \\ --executor-memory 1G \\ --total-executor-cores 2 \\ examples/jars/spark-examples_2.11-2.3.3.jar \\ 10 spark-shell Spark shell 相当于一个Application，交互式 shell ，一般用于测试。 本地运行 # 本地运行，2个线程参与计算 # 默认启动 SparkSubmit 进程 bin/spark-shell --master local[2] # 读取本地文件进行单词统计 sc.textFile(\"file:///home/hadoop/sparkdatas/wordcount\").flatMap(x=>x.split(\" \")).map(x=>(x,1)).reduceByKey((x,y)=>x+y).collect # 读取hdfs文件进行单词统计 # 可以直接统计目录下的所有文件 sc.textFile(\"/test/mr/wordcount/input/2\").flatMap(_.split(\",\")).map((_,1)).reduceByKey(_+_).collect 集群普通模式提交 # Spark shell 相当于一个 application # --total-executor-cores 不会超过集群的CPU最大值 bin/spark-shell --master spark://node01:7077 --executor-memory 1G --total-executor-cores 4 sc.textFile(\"/test/mr/wordcount/input\").flatMap(_.split(\",\")).map((_,1)).reduceByKey(_+_).collect 流程简述 启动Spark集群，启动相应的Master和Worker进程，标记为Mater和Worker节点 Worker节点启动后，会向Master注册 同时，Worker节点会定时向Master发送心跳，汇报资源使用情况 运行Driver程序，向Master注册并申请计算资源 Master收到资源请求后，会调用Worker节点分配资源，也就是启动与资源描述一致的Executor进程 Worker节点向Master节点反馈Executor启动成功 Executor向Driver反向注册自己，并申请执行Task请求 Driver运行用户main方法，构建SparkContext，SparkContext内部依次构建DAG Scheduler和Task Scheduler 按照RDD依赖关系构建DAG，然后将DAG发送给DAG Scheduler DAG Scheduler将DAG按照宽依赖划分为多个stage，每个stage包含多个并发执行的task，然后将task封装到TaskSet集合中，发送给Task Scheduler Task Scheduler收到TaskSet后，按照stage的依赖关系，按顺序将task发送给Executor进行执行 所有task运行完成后，Driver向Master发送注销资源请求；Master通知Worker节点释放资源，也就是关闭Executor进程。最后，整个Job执行完成 资源参数剖析 --executor-memory 每一个Executor进程所需的内存大小；如果设置过小，小于Executor内Task处理分区的大小，Executor会将一部分数据存储到磁盘，当需要的时候，再从内存和磁盘中获取数据，这样就涉及到磁盘IO。大量磁盘IO操作，会导致应用性能降低。在实际工作中，--executor-memory 可以设置为10G/20G/30G等。 --total-executor-cores 任务运行需要总的cpu核数，它决定了任务的并行度。在实际工作中，--total-executor-cores 可以设置为30个/50个/100个等。 Spark on YARN 可以把Spark程序提交到YARN中运行，此时Spark任务所需要的计算资源由YARN中的ResourceManager去分配。 环境要求 hadoop集群 任意一台机器安装Spark Spark应用程序中的Driver分布 yarn-client 模式 Driver运行在Client端，应用程序运行结果会在客户端显示 因为Driver要同Executor频繁通讯，如果有大量数据和线程时，可能会造成网络超时导致任务执行失败。所有任务的结果都会汇总到Driver端，可以在客户端直接显示任务的运行结果，因此一般用于开发测试 yarn-cluster 模式 Drive运行在YARN集群中，运行结果不会在客户端显示，客户端可以在启动应用程序后立即关闭 将运行结果最终保存在外部存储介质（如HDFS、Redis、Mysql）中，客户端显示的仅是作为YARN的job的运行情况 用于生产环境 yarn-cluster 程序运行 提交任务 spark-submit --class org.apache.spark.examples.SparkPi \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 1g \\ --executor-memory 1g \\ --executor-cores 1 \\ /bigdata/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar \\ 10 如果出现虚拟内存不足 vi yarn-site.xml yarn.nodemanager.pmem-check-enabled false yarn.nodemanager.vmem-check-enabled false 流程简述 Client提交Application到YARN集群的ResourceManager ResourceManager向集群中的任意一个NodeManager请求启动容器并在容器中运行ApplicationMaster。其中ApplicationMaster就是Driver，对其进行初始化。此处的ApplicationMaster不仅负责资源申请，还负责任务调度 ApplicationMaster向ResourceManager注册并申请计算资源 ApplicationMaster申请到计算资源后，按照相应的NodeManager启动容器，在容器中运行Executor Executor向Driver反向注册自己并申请执行Task Driver执行main方法，构建SparkContext，经过一系列转换后，分阶段将Task发送到Executor执行 Executor向Driver汇报Task运行状态和进度 当所有Task执行完毕后，ApplicationMaster向ResourceManager申请注销资源并关闭自己 yarn-client 程序运行 提交任务 spark-submit --class org.apache.spark.examples.SparkPi \\ --master yarn \\ --deploy-mode client \\ --driver-memory 1g \\ --executor-memory 1g \\ --executor-cores 1 \\ /bigdata/install/spark-2.3.3-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.3.jar \\ 10 流程简述 1. Client初始化Driver，提交Application到YARN集群的ResourceManager 2. ResourceManager向集群中的任意一个NodeManager请求启动容器并在容器中运行ApplicationMaster。此处的ApplicationMaster只负责资源申请，不负责任务调度 3. ApplicationMaster向ResourceManager注册并申请计算资源 4. ApplicationMaster申请到计算资源后，按照相应的NodeManager启动容器，在容器中运行Executor 5. Executor向Driver反向注册自己并申请执行Task 6. Driver执行main方法，构建SparkContext，经过一系列转换后，分阶段将Task发送到Executor执行 7. Executor向Driver汇报Task运行状态和进度 8. 当所有Task执行完毕后，Driver向ResourceManager申请注销资源并关闭自己 RDD 概述 RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、元素可并行计算的集合。 Dataset 就是一个集合，存储很多数据 Distributed 内部的元素进行了分布式存储，方便于进行分布式计算 Resilient 表示弹性，RDD的数据可以保存在内存或者是磁盘中 属性 A list of partitions 分区列表 每一个分区包含了RDD的部分数据 Spark中的任务是以线程方式运行，一个分区对应一个Task线程 用户可以在创建RDD时，指定分区个数；若没有指定，则采用默认值 RDD的分区数 = max(block数, defaultMinPartitions) A function for computing each split 一个计算每一个分区的函数 A list of dependencies on other RDDs RDD的依赖关系列表 Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) 一个用于 key-value RDD 分区函数 分区函数 HashPartitioner（默认） key.hashcode % 分区数 = 分区号 RangePartitioner 只有对于 key-value 的 RDD RDD[(String, Int)] 并且产生shuffle，才会有Partitioner；非 key-value 的RDD RDD[String] 的 Parititioner 的值是None Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) 用于计算每个partition所在优先位置的一个列表 Spark任务在调度的时候会优先考虑存有数据的节点开启计算任务，减少数据的网络传输，提升计算效率 流程 假设一个300M的文件，默认每一个block的大小是128M，共有3个block。 Spark的分区数是由文件block数量和defaultMinPartitions共同决定，取最大值。假设defaultMinPartitions是2，则分区数是3。 rdd1是MapPartitionsRDD，作用于rdd1的flatMap函数，它将计算3个分区；rdd1没有分区函数 rdd2是MapPartitionsRDD，作用于rdd2的map函数，它将计算3个分区；rdd2没有分区函数 rdd3是MapPartitionsRDD，作用于rdd3的reduceByKey函数，它将计算3个分区；rdd3没有分区函数 rdd4是ShuffledRDD，将数据写入到磁盘文件；rdd4有分区函数HashPartitioner。相当于ReduceTask，其数量取决于HashPartitioner的分区数，而默认情况下HashPartitioner的分区数就是上一步RDD的分区数 算子分类 transformation（转换） 根据已经存在的rdd转换生成一个新的rdd，它是延迟加载，不会立即执行 如 map / flatMap / reduceByKey 等 action（动作） 它会真正触发任务的运行，将rdd的计算的结果数据返回给Driver端，或者是保存结果数据到外部存储介质中 如 collect / saveAsTextFile 等 依赖关系 宽窄依赖 窄依赖（narrow dependency） 窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用 算子：map/flatMap/filter/union 窄依赖不会产生shuffle 宽依赖（wide dependency） 宽依赖指的是子RDD的多个Partition会依赖同一个父RDD的Partition 算子：reduceByKey/sortByKey/groupBy/groupByKey/join 宽依赖会产生shuffle join分为宽依赖和窄依赖，如果RDD有相同的partitioner，那么将不会引起shuffle，这种join是窄依赖，反之就是宽依赖 Lineage 血统 RDD只支持粗粒度转换，即只记录单个块上执行的单个操作 将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区 RDD的Lineage会记录RDD的元数据信息和转换行为，Lineage保存了RDD的依赖关系，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区 stage 什么是stage 一个Job会被拆分为多组Task，每组任务被称为一个stage stage表示不同的调度阶段，一个spark job会对应产生很多个stage stage类型 ShuffleMapStage shuffle之前的所有transformation的Stage叫ShuffleMapStage。它对应的task是shuffleMapTask。 ResultStage shuffle之后操作的Stage叫ResultStage。它对应的task是ResultTask。 为什么划分stage 根据RDD之间依赖关系的不同将DAG划分成不同的stage 对于窄依赖，partition的转换处理在一个stage中完成计算 对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算 由于划分完stage之后，在同一个stage中只有窄依赖，没有宽依赖，可以实现流水线计算，stage中的每一个分区对应一个task，在同一个stage中就有很多可以并行运行的task。 如何划分stage 根据rdd的算子操作顺序生成DAG有向无环图，接下里从最后一个rdd往前推，创建一个新的stage，把该rdd加入到该stage中，它是最后一个stage 在往前推的过程中运行遇到了窄依赖就把该rdd加入到本stage中，如果遇到了宽依赖，就从宽依赖切开，那么最后一个stage也就结束了 重新创建一个新的stage，按照第二个步骤继续往前推，一直到最开始的rdd，整个划分stage也就结束了 stage间关系 划分完stage之后，每一个stage中有很多可以并行运行的task，后续会把每一个stage中的task封装在一个taskSet集合中，最后把一个一个的taskSet集合提交到worker节点上的executor进程中运行 rdd与rdd之间存在依赖关系，stage与stage之间也存在依赖关系，前面stage中的task先运行，运行完成了再运行后面stage中的task，也就是说后面stage中的task输入数据是前面stage中task的输出结果数据 缓存机制 可以把一个rdd的数据缓存起来，后续有其他的job需要用到该rdd的结果数据，可以直接从缓存中获取得到，避免了重复计算。缓存加快后续对该数据的访问操作。 cache和persist 区别 对RDD设置缓存可以调用rdd的2个方法：cache 和 persist 。 调用上面2个方法都可以对rdd的数据设置缓存，但不是立即就触发缓存执行，后面需要有action才会触发缓存的执行。 cache 默认是把数据缓存在内存中，其本质就是调用persist方法 persist 可以把数据缓存在内存或者是磁盘，有丰富的缓存级别，这些缓存级别都被定义在StorageLevel这个object中 设置缓存时机 某个rdd的数据后续会被多次使用 默认情况下多次对一个rdd执行算子操作， rdd都会对这个rdd及之前的父rdd全部重新计算一次。 这种情况在实际开发代码的时候会经常遇到，但是我们一定要避免一个rdd重复计算多次，否则会导致性能急剧降低。因此，可以把多次使用到的rdd，也就是公共rdd进行持久化，避免后续需要时需要再次重新计算，提升效率。 为了获取得到一个rdd的结果数据，经过了大量的算子操作或者是计算逻辑比较复杂 清除缓存 自动清除 一个Application应用程序结束之后，对应的缓存数据也就自动清除 手动清除 调用rdd的unpersist方法 checkpoint机制 同cache和persist区别 cache是把数据缓存在内存中，访问速度很快，但是宕机或进程异常终止时会导致数据容易丢失；persist则可以把数据缓存在磁盘上，可以保证一定数据的安全有效，当由于用户的误操作或磁盘损坏，也会导致数据丢失。 checkpoint提供了一种相对而言更加可靠的数据持久化方式。它是把数据保存在分布式文件系统，如HDFS。这里就是利用了HDFS高可用性，高容错性（多副本）来最大程度保证数据的安全性。 对checkpoint在使用的时候进行优化，在调用checkpoint操作之前，可以先做一个cache操作，缓存对应rdd的结果数据，后续就可以直接从cache中获取到rdd的数据写入到指定checkpoint目录中 Application运行完成后对应的checkpoint数据不会消失 设置checkpoint // 设置checkpoint目录 sc.setCheckpointDir(\"hdfs://node01:8020/checkpoint\") // 调用rdd的checkpoint方法 rdd.checkpoint // 调用action触发 rdd.collect shuffle原理 在 Spark 1.2 以前，默认的shuffle计算引擎是HashShuffleManager。HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而大量的磁盘IO操作影响性能。 因此，在 Spark 1.2 以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的ResultTask拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 HashShuffle 优化之前 在Shuffle write阶段，ShuffleMapTask会先将数据写到bucket缓存，当缓存写满之后刷写到block磁盘文件。数据经过HashPartitioner操作，将相同key的数据写入到相同的bucket缓存中。每个ShuffleMapTask会生成多少个磁盘文件？主要是由下一个阶段的Task数量决定，假设下一个阶段是3个ResultTask，每个ShuffleMapTask生成3个block文件，共12个block文件（ShuffleMapTask * ResultTask）。 在Shuffle read阶段，ResultTask将上一阶段计算的结果中相同key的数据从各个节点通过网络将数据拉取到自己所在的节点，然后对key进行聚合等操作。每一个ResultTask都有一个buffer缓存，每次都拉取buffer大小的缓存，然后进行聚合等操作。 缺点： 在磁盘上会产生海量的小文件，建立通信和拉取数据的次数变多，此时会产生大量耗时低效的磁盘和网络IO 操作。 在数据处理过程中会产生大量的对象保存文件操作句柄和临时信息，会导致堆内存不足，频繁GC会导致卡顿无法响应，严重时会和内存泄露一样出现OOM（Out of memory ）问题。 优化之后 优化之后，同一个core同样会将数据经过HashPartitioner后，写入到ResultTask数量的buffer中。但后续的Task会共用同一份buffer。这样，假设下一个阶段是3个ResultTask，每一个core生成3个block文件，共6个block文件。（Cores * ResultTask）。 缺点： 如果Reducer端的并行任务或者数据分片过多，则Cores * ResultTask依旧过大，也会产生很多小文件。 SortShuffle 普通模式 在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一边写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值（5M），如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。 在溢写磁盘前，会根据PartitionID和key的hash值进行排序，排序过后的数据会分批（默认批次为10000条数据）写入到磁盘文件中。每次溢写都会产生一个磁盘文件，也就是说一个ShuffleMapTask会产生多个临时文件。 最后在每个ShuffleMapTask中，将所有的临时文件合并，此过程将所有临时文件读取出来，一次写入到最终文件。也就是说一个Task的所有数据都在这一个data文件中，同时单独生成一份index索引文件，标识下游各个Task的数据在文件中的索引范围。另外，这个SortShuffle同Executor核数没有关系，即同并发度没有关系，每一个ShuffleMapTask都会产生data文件和index文件。 假设第一个stage 50个task，那么无论下游有几个task，需要502=100个磁盘文件（`2 ShuffleMapTask`）。 优点： 小文件明显变少了，一个ShuffleMapTask只生成一个data和一个index文件。文件整体有序，加上索引文件的辅助，查找变快，虽然排序浪费一些性能，但是查找变快很多 减少Writer缓存所占用的内存大小，而且同时避免GC的风险和频率。 bypass模式 bypass运行机制的触发条件如下： ResultTask数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，默认值200。 不是聚合类的shuffle算子（比如reduceByKey）。 此时task会为每个reduce端的task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。 而该机制与普通SortShuffleManager运行机制的不同在于： 磁盘写机制不同； 不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 参数调优 spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 spark.shuffle.memoryFraction（Spark1.6） 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 spark.shuffle.manager 默认值：sort 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。Spark1.6以后把hash方式给移除了，tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果业务逻辑不需要对数据进行排序，那么建议参考几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 spark中数据倾斜的解决方案 方案一：使用Hive ETL预处理数据 方案适用场景 导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路 此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理 这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点 实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点 治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验 在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 项目实践经验 有一个交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。 方案二：过滤少数导致倾斜的key 方案适用场景 如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路 如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理 将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点 实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点 方案实践经验 在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 方案三：提高shuffle操作的并行度（效果差） 方案适用场景 如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路 在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理 增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。 方案优点 实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点 只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验 该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 方案四：两阶段聚合（局部聚合+全局聚合） 方案适用场景 对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 方案实现思路 这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello, 2)(hello, 2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理 将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。 方案优点 对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点 仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 方案五：将reduce join转为map join 方案适用场景 在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。 方案实现思路 不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理 普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。 方案优点 对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点 适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 方案六：采样倾斜key并分拆join操作 方案适用场景 两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用方案五，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。 方案实现思路 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来（作用是减少分析数据量，但又可以基本代表整个数据集），然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀（保证随机前缀的独立RDD可以和膨胀n倍的独立RDDjoin时，结果同原来相同；但却可以使原相同key的数据均匀分布在多个task中），不会导致倾斜的大部分key也形成另外一个RDD。 如果join右侧过滤倾斜key对应的数据比较大的时候，可以使用膨胀数据方法 如果比较小的时候，可以使用方案五Broadcast变量方法 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。 方案实现原理 对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。 方案优点 对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点 如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。 方案七：使用随机前缀和扩容RDD进行join 方案适用场景 如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用这一种方案来解决问题了。 方案实现思路 该方案的实现思路基本和方案六类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。 方案实现原理 将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与方案六的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，无法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 方案优点 对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点 该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 方案实践经验 曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/spark/sparkcore/Spark调优.html":{"url":"src/bigdata/spark/sparkcore/Spark调优.html","title":"Spark调优","keywords":"","body":"分配更多的资源 增加资源与性能提升成正比。在资源比较充足的情况下，尽可能的使用更多的计算资源，尽量去调节到最大的大小。 --driver-memory 1g 配置driver的内存，使得driver端可以存储更多的数据，避免出现OOM。如果RDD有大量数据，在做collect算子操作时，会把各个Executor的数据转换为数组，拉取到driver端，如果内存不足就会出现OOM异常。 --num-executors 3 配置Executor的数量，提升并行能力。配合 --executor-cores 参数可以提高每个批次最大并行的task数量。 --executor-cores 3 配置每一个Executor的cpu个数，提升并行能力。 --executor-memory 1g 配置每一个Executor的内存大小。 对RDD进行cache，可以缓存更多的数据，减少磁盘IO。 进行Shuffle操作，reduce端会拉取数据存放到缓存，缓存不够会刷写磁盘。减少磁盘IO。 task执行过程中会创建大量对象，如果内存设置比较小，会导致频繁GC。 提高并行度 官方推荐，task数量设置成 spark Application 总cpu core数量的2~3倍 。因为与理想情况不同，有的task运行快，有的运行慢，如果设置同CPU核数相同，则运行快的task对应的CPU就会空闲下来造成资源浪费。如果有后补task，则CPU就不会空闲，并且数据被分摊。 spark.defalut.parallelism 默认是没有值的，如果设置为10，它会在shuffle的过程起作用。如 val rdd2 = rdd1.reduceByKey(_+_) 此时rdd2的分区数就是10。 rdd.repartition 该方法会生成一个新的rdd，使其分区数变大。由于一个partition对应一个task，则partition越多，对应的task个数就越多，通过这种方式可以提高并行度。 spark.sql.shuffle.partitions 默认为200。可以适当增大，来提高并行度。 如 spark.sql.shuffle.partitions = 500 RDD的重用和持久化 默认情况下多次对一个rdd执行算子操作去获取不同的rdd，都会对这个rdd及之前的父rdd全部重新计算一次。 这种情况在实际开发的时候会经常遇到，但是我们一定要避免一个rdd重复计算多次，否则会导致性能急剧降低。可以把多次使用到的rdd，也就是公共rdd进行持久化，避免后续需要，再次重新计算，提升效率。 可以调用rdd的cache或者persist方法 广播变量的使用 当某一个stage的task需要一份共有数据，如果task数量非常大的话，不同的task线程会拉取这份数据，导致大量的网络传输开销，并且所有都会拥有这份数据，导致占用大量的内存空间。如果rdd需要持久化，势必内存不够用需要刷写磁盘，导致磁盘IO性能降低。同时，也会导致内存紧张，频繁GC影响spark作业的运行效率。 广播变量的运行机制 广播变量初始的时候，在Drvier上有一份副本。通过在Driver把共享数据转换成广播变量。 task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；如果本地没有，那么就从Driver远程拉取广播变量副本，并保存在本地的BlockManager中； 此后这个executor上的task，都会直接使用本地的BlockManager中的副本。那么这个时候所有该executor中的task都会使用这个广播变量的副本。也就是说一个executor只需要在第一个task启动时，获得一份广播变量数据，之后的task都从本节点的BlockManager中获取相关数据。 executor的BlockManager除了从driver上拉取变量，也可就近从其他节点的BlockManager上拉取变量副本。 注意事项 能不能将一个RDD使用广播变量广播出去？不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。 广播变量只能在Driver端定义，不能在Executor端定义。 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。 如果executor端用到了Driver的变量，如果不使用广播变量，在Executor有多少task就有多少Driver端的变量副本；如果使用广播变量，在每个Executor中只有一份Driver端的变量副本。 尽量避免使用shuffle类算子 spark中的shuffle涉及到网络传输，本质就是将满足一定条件的key拉取到相应的节点，然后进行聚合或join运算。spark作业中最耗时的地方就是shuffle操作，应尽量避免。 当join运算的某一个rdd数据较少时，可以通过将其设置为广播变量的形式，来代替shuffle 如果必须要使用shuffle操作，可以使用map-side预聚合方式来减少网络数据传输量，类似于MapReduce的combiner。如 reduceByKey 会在map端使用用户自定义的函数进行预聚合；而 groupByKey 不会进行预聚合，会将全量数据分发到reduce端，性能相对来说会比较差。 使用高性能的算子 使用reduceByKey/aggregateByKey替代groupByKey 利用预聚合特性 使用mapPartitions替代map mapPartitions每次函数调用遍历一个分区的数据，而map每次遍历一条数据 如果分区数据过大时，可能会出现OOM问题。因为一次函数调用需要处理一个分区的数据，导致内存不够用，GC又无法回收太多对象 使用foreachPartitions替代foreach foreachPartitions一次函数调用处理一个分区的数据，而foreach仅处理一条数据 使用filter之后进行coalesce操作 如果filter算子过滤掉rdd的部分数据后，建议使用coalesce减少rdd的分区数量。由于使用filter算子，原分区数据会减少，若仍用原来数量的task处理分区数据，会有一些资源浪费。此时task越多，反而运行越慢。 使用repartitionAndSortWithinPartitions替代repartition与sort repartitionAndSortWithinPartitions是Spark官网推荐的一个算子。官方建议如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能要高。 使用Kryo优化序列化性能 Spark在进行任务计算的时候，会涉及到数据跨进程的网络传输、数据的持久化，这个时候就需要对数据进行序列化。Spark默认采用Java的序列化器。 优点：简单，只需要实现Serializble接口 缺点：速度慢，占用内存空间大 Spark支持使用Kryo序列化机制。 优点：速度快，序列化后的数据要更小，大概是Java序列化机制的1/10 开启Kryo序列化机制 conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) Kryo序列化启用后生效的地方 算子函数中使用到的外部变量 持久化RDD时进行序列化 产生shuffle的地方，也就是宽依赖 使用fastutil优化数据格式 fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue，可以提供更小的内存占用，更快的存取速度。 使用场景 算子函数使用的外部变量 首先从源头上就减少内存的占用（fastutil），通过广播变量进一步减少内存占用，再通过Kryo序列化类库进一步减少内存占用。 算子函数里使用了比较大的集合Map/List 调节数据本地化等待时长 Spark的task分配算法，优先会希望每个task正好分配到它要计算的数据所在的节点（移动计算），这样的话就不用在网络间传输数据；但如果数据所在节点不具备分配资源的条件，可以设置让这个task先等待一段时间再分配到数据所在节点。如果超过了这个时间等待阈值，则就近分配资源运行task。 本地化级别 PROCESS_LOCAL（进程本地化） 数据在Executor的BlockManager中，性能最好 NODE_LOCAL（节点本地化） 数据在本节点作为hdfs的一个block存在，或者在其他Executor的BlockManager中，数据需要在不同进程中传输，性能其次 RACK_LOCAL（机架本地化） 数据在同一机架的不同节点上，需要网络传输数据。性能比较差 ANY（无限制） 数据在集群中的任意一个地方，且不再同一个机架上。性能最差 数据本地化等待时长 首先采用最佳的方式，等待3s后降级，还是无法运行，继续降级，最后只能采用最差级别。 spark.locality.wait（默认是3s） spark.locality.wait.process spark.locality.wait.node spark.locality.wait.rack 注意如果本地化等待时间设置过大，则会出现等待时间甚至高于不同节点通过网络拉取的时间，这样虽然实现task在数据本地运行，但反而会使得spark作业的运行时间增加。 基于Spark内存模型调优 静态内存模型无法借用其他空闲内存区域； 而统一内存模型，Storage和Executor内存区域是可以相互借用的，但是Executor内存使用会优先于Storage，因为Executor需要马上申请内存，而Storage并不是很急迫，因此可以暂时缓存到磁盘。 Executor申请内存，而Executor不够，则借用Storage内存，如果Storage内存已被使用，则驱逐Storage写到磁盘 Executor使用内存不多，Storage借用Executor内存，如果此时Executor需要申请内存但已被Storage借用，则驱逐Storage借用的内存写到磁盘 在spark1.6版本以前，spark的executor使用的静态内存模型，但是从spark1.6开始，多增加了一个统一内存模型。通过 spark.memory.useLegacyMode 参数配置，默认false，表示用的是新的统一内存模型。 内存模型 静态内存模型 Storage内存区域，由 spark.storage.memoryFraction 控制（默认是0.6，占总内存的60%） Reserved 预留，防止OOM（60% * 10% = 6%） 用于缓存RDD数据和广播变量数据，由 spark.storage.saftyFraction 控制（60% * 90% = 54%） 用于unroll，缓存iterator形式的block数据，由 spark.storage.unrollFraction 控制（60% 90% 20% = 10.8%） Executor内存区域，由 spark.shuffle.memoryFraction 控制（默认是0.2，占总内存的20%） Reserved 预留，防止OOM（20% * 20% = 4%） 用于缓存shuffle过程中的数据，由 spark.shuffle.saftyFraction 控制（20% * 80% =16%） 其他内存区域，由以上两部分内存大小决定（默认是0.2，占总内存的20%），用户定义的数据结构或spark内部元数据 统一内存模型 可用内存（usable）= 总内存 - 预留内存 统一内存（Storage + Executor）由 spark.memory.fraction 控制（2.x 默认0.6，占可用内存的60%；1.6默认0.75） Storage内存区域，用于缓存RDD数据和广播变量数据，由 spark.memory.storageFraction 控制（默认是0.5，usable 60% 50%） Executor内存区域，用于缓存shuffle过程中的数据，由 1- spark.memory.storageFraction 控制（默认是0.5，usable 60% 50%） 其他内存区域，用户定义的数据结构或spark内部元数据 （占可用内存的40%） 预留内存 300M，作用同其他内存区域相同，防止OOM Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/spark/sparksql/SparkSQL核心功能.html":{"url":"src/bigdata/spark/sparksql/SparkSQL核心功能.html","title":"SparkSQL核心功能","keywords":"","body":"概述 SparkSQL是Apache Spark用来处理结构化数据的一个模块。 易集成 将SQL查询与Spark程序无缝混合 可以使用不同的语言进行代码开发，如Java，Scala，Python 和 R 统一数据访问 以相同的方式连接到任何数据源 兼容Hive 运行 SQL 或者 HiveQL 查询已存在的数据仓库 支持标准的数据库连接 DataFrame 概述 在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库的二维表格 DataFrame带有Schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型，但底层做了更多的优化 DataFrame可以从很多数据源构建。如：已经存在的RDD、结构化文件、外部数据库、Hive表 RDD可以把内部元素看作是java对象；而DataFrame可以把内部元素看作是Row对象，表示一行一行的数据 DataFrame和RDD的优缺点 RDD 优点 编译时类型安全 具有面向对象的编程风格 缺点 构建大量的java对象占用了大量heap堆空间，导致频繁的GC（程序在进行垃圾回收的过程中，所有的任务都是暂停，影响程序执行的效率） 数据的序列化和反序列化性能开销很大（包括对象内容和结构） DataFrame DataFrame引入了schema元信息和off-heap 优点 DataFrame引入off-heap，大量的对象构建直接使用操作系统层面上的内存，不在使用heap堆中的内存，这样一来heap堆中的内存空间就比较充足，不会导致频繁GC，程序的运行效率比较高，它是解决了RDD构建大量的java对象占用了大量heap堆空间，导致频繁的GC这个缺点 DataFrame引入schema元信息（数据结构的描述信息），spark程序中的大量对象在进行网络传输的时候，只需要把数据的内容本身进行序列化就可以，数据结构信息可以省略掉。这样一来数据网络传输的数据量是有所减少，数据的序列化和反序列性能开销就不是很大了。它是解决了RDD数据的序列化和反序列性能开销很大这个缺点 缺点 编译时类型不安全 不再具有面向对象的编程风格 常用操作 DSL（domain-specific language）风格语法 // Michael, 29 // Andy, 30 // Justin, 19 case class People(name:String,age:Int) // RDD[String] val rdd1 = sc.textFile(\"/test/spark/people.txt\") // RDD[People] val rdd2 = rdd1.map(_.split(\", \")).map(x=>People(x(0),x(1).toInt)) // DataFrame val df = rdd2.toDF // 打印schema df.printSchema // 展示数据 df.show df.select(\"name\") // $ 是一种方法调用，$\"name\" 相当于 new ColumnName(\"name\") 的缩写 // 查询某一列 df.select($\"name\") // 按年龄分组 df.groupBy(\"age\").count.show // 按年龄分组，统计结果排序 df.groupBy(\"age\").count.sort($\"count\".desc).show SQL（Standard Query Language）风格语法 // 创建临时表 df.createTempView(\"people\") // Spark session 调用 sql 方法 spark.sql(\"select * from people\").show spark.sql(\"select age, count(*) as count from people group by age\").show DataSet 概述 DataSet是分布式的数据集合，DataSet提供了强类型支持，也是在RDD的每行数据加了类型约束 DataSet是在Spark1.6中添加的新的接口。它集中了RDD的优点（强类型和可以用强大lambda函数）以及使用了Spark SQL优化的执行引擎 DataSet包含了DataFrame的功能，Spark2.0中两者统一，DataFrame表示为DataSet[Row]，即DataSet的子集 DataSet可以在编译时检查类型 面向对象的编程接口 常用操作 val ds = spark.createDataset(sc.textFile(\"/test/spark/people.txt\")) // 展示数据 ds.show 系统集成 Hive集成 本地支持HiveSQL Maven依赖 org.apache.spark spark-hive_2.11 2.3.3 自定义设置derby.log、数据仓库数据、元数据位置 object Config { def getLocalConfig(appName: String): SparkConf = { // 自定义derby.log位置 System.setProperty(\"derby.system.home\", \"/Users/yangxiaoyu/work/test/sparkdatas/hivelocal\") val sparkConf = new SparkConf() sparkConf .setMaster(\"local[2]\") .setAppName(appName) // 自定义数据仓库数据位置 .set(\"spark.sql.warehouse.dir\", \"/Users/yangxiaoyu/work/test/sparkdatas/hivelocal/spark-warehouse\") // 自定义元数据位置 .set(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:;databaseName=/Users/yangxiaoyu/work/test/sparkdatas/hivelocal/metastore_db;create=true\") sparkConf } def getServerConfig(appName: String): SparkConf = { val sparkConf = new SparkConf() sparkConf.setAppName(appName) sparkConf } } object HiveLocalSupport { def main(args: Array[String]): Unit = { val config = Config.getLocalConfig(getClass.getName) val sparkSession = SparkSession.builder().config(config).enableHiveSupport().getOrCreate() sparkSession.sql(\"create database if not exists test\") sparkSession.sql(\"use test\") sparkSession.sql(\"create table if not exists people(id string, name string, age int) row format delimited fields terminated by ' '\") sparkSession.sql(\"load data local inpath '/Users/yangxiaoyu/work/test/sparkdatas/people' into table people\") sparkSession.sql(\"select * from people\").show() sparkSession.stop() } } 联动Hive 初始化环境 配置读取Hive元数据 node03执行 将hive的hive-site.xml分发到所有spark的conf目录下（mysql连接信息） cp hive-site.xml /bigdata/install/spark-2.3.3-bin-hadoop2.7/conf scp hive-site.xml node02:$PWD scp hive-site.xml node01:$PWD node01执行 将mysql驱动分发到所有spark的jars目录下 cp mysql-connector-java-5.1.38.jar /bigdata/install/spark-2.3.3-bin-hadoop2.7/jars scp mysql-connector-java-5.1.38.jar node02:$PWD scp mysql-connector-java-5.1.38.jar node03:$PWD 启动交互式环境 配置数据仓库位置 `spark.sql.warehouse.dir` spark-sql \\ --master spark://node01:7077 \\ --executor-memory 1g \\ --total-executor-cores 2 \\ --conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse 运行Hive语句 select * from myhive.score; 提交脚本 spark-sql \\ --master spark://node01:7077 \\ --executor-memory 1g \\ --total-executor-cores 2 \\ --conf spark.sql.warehouse.dir=hdfs://node01:8020/user/hive/warehouse \\ -e 'select * from myhive.score;' MySQL集成 Maven依赖 MySQL JDBC 驱动 mysql mysql-connector-java 5.1.38 runtime 登录Mysql创建测试数据 启动mysql服务 # 启动服务 systemctl start mysqld.service # 登录 mysql -uroot -proot 创建mysql测试数据 -- 创建数据库 create database spark; -- 创建表 use spark; create table user(id int, name varchar(40), age int); -- 插入数据 insert into user(id, name, age) values(1,'yoyo',37); insert into user(id, name, age) values(2,'lucky',3); insert into user(id, name, age) values(2,'rain',38); SparkSQL通过JDBC加载MySQL数据库表数据到DataFrame，利用Spark分布式计算框架的优势，对数据计算之后将结果写回到MySQL数据库表中 object ReadWrite { def main(args: Array[String]): Unit = { val url = \"jdbc:mysql://node03:3306/spark\" val readTableName = \"user\" val writeTableName = \"newuser\" val properties = new Properties() properties.setProperty(\"user\", \"root\") properties.setProperty(\"password\", \"root\") // val config = Config.getLocalConfig(getClass.getName) val config = Config.getServerConfig(getClass.getName) val sparkSession = SparkSession.builder().config(config).getOrCreate() // 读取数据 val readDF = sparkSession.read.jdbc(url, readTableName, properties) // readDF.printSchema() // readDF.show() // 创建临时表 readDF.createTempView(\"user\") // 处理数据 val writeDF = sparkSession.sql(\"select id, name, age, age-10 as newhope from user\") // 写入数据 // mode // overwrite 表示覆盖，如果表不存在，创建 // append 表示追加，如果表不存在，创建 // ignore 表示忽略，如果表存在，不进行任何操作 // error 如果表存在，报错（默认选项） writeDF.write.mode(\"overwrite\").jdbc(url, writeTableName, properties) sparkSession.stop() } } 集群运行 # --driver-class-path 指定 Driver 端所需要的jar # --jars 指定 Executor 端所需要的jar spark-submit --master spark://node01:7077 \\ --executor-memory 1g --total-executor-cores 2 \\ --class com.sciatta.hadoop.spark.example.sql.mysql.ReadWrite \\ --driver-class-path /bigdata/soft/mysql-connector-java-5.1.38.jar \\ --jars /bigdata/soft/mysql-connector-java-5.1.38.jar \\ hadoop-spark-example-1.0-SNAPSHOT.jar Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/bigdata/spark/Spark集群安装部署.html":{"url":"src/bigdata/spark/Spark集群安装部署.html","title":"Spark集群安装部署","keywords":"","body":"先决条件 安装ZooKeeper集群并启动 服务规划 服务器 Master Worker node01 √ alive node02 √ node03 √ standby √ 安装 scp spark-2.3.3-bin-hadoop2.7.tgz hadoop@node01:/bigdata/soft tar -zxvf spark-2.3.3-bin-hadoop2.7.tgz -C /bigdata/install/ 修改配置 spark-env.sh cp spark-env.sh.template spark-env.sh vi spark-env.sh # 配置java的环境变量 export JAVA_HOME=/bigdata/install/jdk1.8.0_141 # 配置zk相关信息 export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181 -Dspark.deploy.zookeeper.dir=/spark\" # 指向hadoop配置文件路径 export HADOOP_CONF_DIR=/bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop slaves cp slaves.template slaves # 指定spark集群的worker节点 node02 node03 分发 scp -r /bigdata/install/spark-2.3.3-bin-hadoop2.7 node02:/bigdata/install scp -r /bigdata/install/spark-2.3.3-bin-hadoop2.7 node03:/bigdata/install 配置环境变量 三台机器均需修改 sudo vi /etc/profile export SPARK_HOME=/bigdata/install/spark-2.3.3-bin-hadoop2.7 export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin # 立即生效 source /etc/profile 启动 node01执行 # 切换到sbin下运行，会有同名文件冲突 cd /bigdata/install/spark-2.3.3-bin-hadoop2.7/sbin # 启动master和worker # 注意启动的机器就是master，而worker由slave决定 start-all.sh node03执行 # 高可用 start-master.sh 停止 node01执行 cd /bigdata/install/spark-2.3.3-bin-hadoop2.7/sbin stop-all.sh node03执行 stop-master.sh 验证 访问页面 alive：http://node01:8080/ standby：http://node03:8080/ Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/database/mysql/CentOS7安装MySQL5.7.html":{"url":"src/database/mysql/CentOS7安装MySQL5.7.html","title":"CentOS7安装MySQL5.7","keywords":"","body":"安装MySQL CentOS 7中切换到root用户，安装mysql CentOS 7中默认安装有MariaDB，这个是MySQL的分支；但还是要安装MySQL，安装完成之后会直接覆盖掉MariaDB 安装在node03上 # 切换到root用户 su root # 安装wget cd /bigdata/soft/ yum -y install wget # 使用wget命令下载mysql的rpm包 # -i 指定输入文件 # -c 表示断点续传 wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm # 安装mysql yum -y install mysql57-community-release-el7-10.noarch.rpm # 安装mysql server yum -y install mysql-community-server 设置MySQL 启动服务 # 启动MySQL服务 systemctl start mysqld.service # 查看mysql启动状态 # active（running）表示mysql服务已启动 systemctl status mysqld.service 用临时密码登录 # 找出临时密码 # G;sZ/.i(G7Gt grep \"password\" /var/log/mysqld.log # 使用临时密码，登陆mysql客户端 mysql -uroot -p 修改密码 -- 设置密码策略为LOW，此策略只检查密码的长度 set global validate_password_policy=LOW; -- 设置密码最小长度 set global validate_password_length=4; -- 修改mysql的root用户，本地登陆的密码为root ALTER USER 'root'@'localhost' IDENTIFIED BY 'root'; -- 开启mysql的远程连接权限 grant all privileges on *.* to 'root'@'%' identified by 'root' with grant option; -- 即时生效 flush privileges; -- 退出 exit 卸载MySQL 使用root用户卸载mysql # 停止mysql服务 systemctl stop mysqld.service # 列出已安装的mysql相关的包 # 卸载完成后，用这两个命令再次检查 yum list installed mysql* # 或 rpm -qa | grep -i mysql # 卸载，命令后边依次添加上一步列出的包名，包名之间用空格分隔 rpm -e --nodeps 删除mysql残留文件 # 查看mysql相关目录 find / -name mysql # 删除上一步列出的目录 rm -rf # 删除文件 rm -rf /root/.mysql_history rm -f /var/log/mysqld.log Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/database/mysql/SQL经典50题.html":{"url":"src/database/mysql/SQL经典50题.html","title":"SQL经典50题","keywords":"","body":"SQL语句的执行顺序 语法 ? select SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [STRAIGHT_JOIN] [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr] ... [into_option] [FROM table_references [PARTITION partition_list]] [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ... [WITH ROLLUP]] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [PROCEDURE procedure_name(argument_list)] [into_option] [FOR UPDATE | LOCK IN SHARE MODE] into_option: { INTO OUTFILE 'file_name' [CHARACTER SET charset_name] export_options | INTO DUMPFILE 'file_name' | INTO var_name [, var_name] ... } 执行顺序 from on 先on过滤条件之后，才会join生成临时表 join where 临时表生成之后，根据限制条件从临时表中筛选 在分组（聚集函数）之前筛选数据 group by 分组之后，执行聚集函数 having 聚合函数执行之后对分组数据进一步筛选，同group by一起使用，不可单独使用 select 如果有group by使用的话，select查询的字段可能是group by后跟的分组字段，也有可能是对字段聚合函数计算的结果 distinct order by group by和orderby可以实现组内排序，即 group by A,B order by A,B limit 初始化数据 create database school charset=utf8; use school; DROP TABLE IF EXISTS `student`; CREATE TABLE IF NOT EXISTS `student`( `s_id` VARCHAR(20), `s_name` VARCHAR(20) NOT NULL DEFAULT '', `s_birth` VARCHAR(20) NOT NULL DEFAULT '', `s_sex` VARCHAR(10) NOT NULL DEFAULT '', PRIMARY KEY(`s_id`) ); insert into student values('01' , '赵雷' , '1990-01-01' , '男'); insert into student values('02' , '钱电' , '1990-12-21' , '男'); insert into student values('03' , '孙风' , '1990-05-20' , '男'); insert into student values('04' , '李云' , '1990-08-06' , '男'); insert into student values('05' , '周梅' , '1991-12-01' , '女'); insert into student values('06' , '吴兰' , '1992-03-01' , '女'); insert into student values('07' , '郑竹' , '1989-07-01' , '女'); insert into student values('08' , '王菊' , '1990-01-20' , '女'); DROP TABLE IF EXISTS `teacher`; CREATE TABLE IF NOT EXISTS `teacher`( `t_id` VARCHAR(20), `t_name` VARCHAR(20) NOT NULL DEFAULT '', PRIMARY KEY(`t_id`) ); insert into teacher values('01' , '张三'); insert into teacher values('02' , '李四'); insert into teacher values('03' , '王五'); DROP TABLE IF EXISTS `course`; CREATE TABLE IF NOT EXISTS `course`( `c_id` VARCHAR(20), `c_name` VARCHAR(20) NOT NULL DEFAULT '', `t_id` VARCHAR(20) NOT NULL, PRIMARY KEY(`c_id`) ); insert into course values('01' , '语文' , '02'); insert into course values('02' , '数学' , '01'); insert into course values('03' , '英语' , '03'); DROP TABLE IF EXISTS `score`; CREATE TABLE IF NOT EXISTS `score`( `s_id` VARCHAR(20), `c_id` VARCHAR(20), `s_score` INT(3), PRIMARY KEY(`s_id`,`c_id`) ); insert into score values('01' , '01' , 80); insert into score values('01' , '02' , 90); insert into score values('01' , '03' , 99); insert into score values('02' , '01' , 70); insert into score values('02' , '02' , 60); insert into score values('02' , '03' , 80); insert into score values('03' , '01' , 80); insert into score values('03' , '02' , 80); insert into score values('03' , '03' , 80); insert into score values('04' , '01' , 50); insert into score values('04' , '02' , 30); insert into score values('04' , '03' , 20); insert into score values('05' , '01' , 76); insert into score values('05' , '02' , 87); insert into score values('06' , '01' , 31); insert into score values('06' , '03' , 34); insert into score values('07' , '02' , 89); insert into score values('07' , '03' , 98); 解题 一 查询\"01\"课程比\"02\"课程成绩高的学生的信息及课程分数 select s1.s_id as s_id,s1.s_score as c1,s2.s_score as c2,student.s_name,student.s_birth,student.s_sex from (select * from score where c_id='01') s1 inner join (select * from score where c_id='02') s2 on s1.s_id=s2.s_id and s1.s_score>s2.s_score inner join student on s1.s_id=student.s_id; +------+------+------+--------+------------+-------+ | s_id | c1 | c2 | s_name | s_birth | s_sex | +------+------+------+--------+------------+-------+ | 02 | 70 | 60 | 钱电 | 1990-12-21 | 男 | | 04 | 50 | 30 | 李云 | 1990-08-06 | 男 | +------+------+------+--------+------------+-------+ 在比较01课程大于02课程时，可以用on过滤，再去join；也可以join生成临时表之后，再用where过滤。很显然，on优于where。 二 查询\"01\"课程比\"02\"课程成绩低的学生的信息及课程分数 select student.*,s1.s_score c1,s2.s_score c2 from student inner join (select * from score where c_id='01') s1 on student.s_id=s1.s_id inner join (select * from score where c_id='02') s2 on s1.s_id=s2.s_id where s1.s_score 三 查询平均成绩大于等于60分的同学的学生编号和学生姓名和平均成绩 select student.s_id,student.s_name,avgs from (select s_id,avg(s_score) as avgs from score group by s_id having avgs>=60) s1 inner join student on s1.s_id=student.s_id; +------+--------+---------+ | s_id | s_name | avgs | +------+--------+---------+ | 01 | 赵雷 | 89.6667 | | 02 | 钱电 | 70.0000 | | 03 | 孙风 | 80.0000 | | 05 | 周梅 | 81.5000 | | 07 | 郑竹 | 93.5000 | +------+--------+---------+ 四 查询平均成绩小于60分的同学的学生编号和学生姓名和平均成绩 select student.s_id,student.s_name,avgs from (select s_id,avg(s_score) as avgs from score group by s_id having avgs 包括没有考试的学生，其平均成绩是0。 五 查询所有同学的学生编号、学生姓名、选课总数、所有课程的总成绩 select student.s_id,student.s_name, case when s.sumcourse is null then 0 else s.sumcourse end as sumcourse, case when s.sumscore is null then 0 else s.sumscore end as sumscore from student left join (select s_id, count(*) as sumcourse, sum(s_score) as sumscore from score group by s_id) s on student.s_id=s.s_id; +------+--------+-----------+----------+ | s_id | s_name | sumcourse | sumscore | +------+--------+-----------+----------+ | 01 | 赵雷 | 3 | 269 | | 02 | 钱电 | 3 | 210 | | 03 | 孙风 | 3 | 240 | | 04 | 李云 | 3 | 100 | | 05 | 周梅 | 2 | 163 | | 06 | 吴兰 | 2 | 65 | | 07 | 郑竹 | 2 | 187 | | 08 | 王菊 | 0 | 0 | +------+--------+-----------+----------+ 流程控制 case when -- 表示等值比较（value和compare-value）； -- 如果没有匹配，则返回ELSE后的结果； -- 如果没有ELSE部分，则返回NULL CASE value WHEN [compare-value] THEN result [WHEN [compare-value] THEN result ...] [ELSE result] END -- 条件是否返回true（不为0和NULL），成立则执行后边的语句，然后退出； -- 对于多个when条件同时成立，只会执行第一个满足条件的when -- 如果都不为true，则返回ELSE后的结果； -- 如果没有ELSE部分，则返回NULL CASE WHEN [condition] THEN result [WHEN [condition] THEN result ...] [ELSE result] END if -- 如果expr1为true（不为 0 和 NULL），则IF()的返回值为expr2； -- 否则，返回值为expr3 IF(expr1, expr2, expr3) 六 查询\"李\"姓老师的数量 select count(*) from teacher where t_name like '李%'; +----------+ | count(*) | +----------+ | 1 | +----------+ 七 查询学过\"张三\"老师授课的同学的信息 select * from student where s_id in ( select s.s_id from (select c_id from teacher t left join course c on t.t_id=c.t_id where t_name='张三') a left join score s on a.c_id=s.c_id ); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 01 | 赵雷 | 1990-01-01 | 男 | | 02 | 钱电 | 1990-12-21 | 男 | | 03 | 孙风 | 1990-05-20 | 男 | | 04 | 李云 | 1990-08-06 | 男 | | 05 | 周梅 | 1991-12-01 | 女 | | 07 | 郑竹 | 1989-07-01 | 女 | +------+--------+------------+-------+ 八 查询没学过\"张三\"老师授课的同学的信息 select * from student where s_id not in ( select s.s_id from (select c_id from teacher t left join course c on t.t_id=c.t_id where t_name='张三') a left join score s on a.c_id=s.c_id ); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 06 | 吴兰 | 1992-03-01 | 女 | | 08 | 王菊 | 1990-01-20 | 女 | +------+--------+------------+-------+ 九 查询学过编号为\"01\"并且也学过编号为\"02\"的课程的同学的信息 select * from student where s_id in ( select a.s_id from (select s_id from score where c_id='01') a inner join (select s_id from score where c_id='02') b on a.s_id =b.s_id ); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 01 | 赵雷 | 1990-01-01 | 男 | | 02 | 钱电 | 1990-12-21 | 男 | | 03 | 孙风 | 1990-05-20 | 男 | | 04 | 李云 | 1990-08-06 | 男 | | 05 | 周梅 | 1991-12-01 | 女 | +------+--------+------------+-------+ 十 查询学过编号为\"01\"但是没有学过编号为\"02\"的课程的同学的信息 select * from student where s_id in ( select a.s_id from (select s_id from score where c_id='01') a inner join (select s_id from student where s_id not in (select s_id from score where c_id='02')) b on a.s_id =b.s_id ); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 06 | 吴兰 | 1992-03-01 | 女 | +------+--------+------------+-------+ 十一 查询没有学全所有课程的同学的信息 select * from student where s_id not in ( select s_id from ( select s_id, count(*) cs from score group by s_id having cs=(select count(*) from course) ) a ); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 05 | 周梅 | 1991-12-01 | 女 | | 06 | 吴兰 | 1992-03-01 | 女 | | 07 | 郑竹 | 1989-07-01 | 女 | | 08 | 王菊 | 1990-01-20 | 女 | +------+--------+------------+-------+ 十二 查询至少有一门课与学号为\"01\"的同学所学相同的同学的信息 select * from student where s_id in ( select distinct b.s_id from (select c_id from score where s_id='01') a left join (select s_id,c_id from score where s_id!='01') b on a.c_id = b.c_id ); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 02 | 钱电 | 1990-12-21 | 男 | | 03 | 孙风 | 1990-05-20 | 男 | | 04 | 李云 | 1990-08-06 | 男 | | 05 | 周梅 | 1991-12-01 | 女 | | 06 | 吴兰 | 1992-03-01 | 女 | | 07 | 郑竹 | 1989-07-01 | 女 | +------+--------+------------+-------+ 十三 查询和\"01\"号的同学学习的课程完全相同的其他同学的信息 select * from student where s_id in ( select b.s_id from (select s_id,count(*) as mycourse from score group by s_id) a left join ( select b.s_id,count(*) as samecourse from (select c_id from score where s_id='01') a inner join score b on a.c_id = b.c_id group by s_id ) b on a.s_id=b.s_id where samecourse=(select count(*) from score where s_id='01') and samecourse=mycourse ) and s_id <> '01'; +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 02 | 钱电 | 1990-12-21 | 男 | | 03 | 孙风 | 1990-05-20 | 男 | | 04 | 李云 | 1990-08-06 | 男 | +------+--------+------------+-------+ 当两个同学学习课程的数目和名称相同时，为\"完全相同\" \"01\"号同学学习的课程是1、2和3，有A同学学习的课程是1、2和3，则\"01\"号同学和A同学学习的课程完全相同。 \"01\"号同学学习的课程是1和2，有A同学学习的课程是1、2和3，虽然\"01\"号同学学习的课程全被A同学学习，但A同学和\"01\"号同学学习的课程数量不同。 十四 查询没学过\"张三\"老师讲授的任一门课程的学生姓名 select s_name from student where s_id not in ( select distinct s_id from score where c_id in ( select c_id from course where t_id = (select t_id from teacher where t_name='张三') ) ); +--------+ | s_name | +--------+ | 吴兰 | | 王菊 | +--------+ 十五 查询两门及其以上不及格课程的同学的学号，姓名及其平均成绩 select a.s_id,a.s_name,avgscore from ( select s_id,s_name from student where s_id in ( select s_id from score where s_score=2 ) ) a left join (select s_id,avg(s_score) as avgscore from score group by s_id) b on a.s_id=b.s_id; +------+--------+----------+ | s_id | s_name | avgscore | +------+--------+----------+ | 04 | 李云 | 33.3333 | | 06 | 吴兰 | 32.5000 | +------+--------+----------+ 十六 检索\"01\"课程分数小于60，按分数降序排列的学生信息 select * from student where s_id in ( select s_id from score where c_id='01' and s_score 十七 按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩 select student.s_id, if(yw is null,0,yw) as '语文', if(sx is null,0,sx) as '数学', if(yy is null,0,yy) as '英语', if(pjcj is null,0,pjcj) as '平均成绩' from student left join (select s_id,s_score as yw from score where c_id='01') a on student.s_id=a.s_id left join (select s_id,s_score as sx from score where c_id='02') b on student.s_id=b.s_id left join (select s_id,s_score as yy from score where c_id='03') c on student.s_id=c.s_id left join (select s_id,avg(s_score) as pjcj from score group by s_id) d on student.s_id=d.s_id order by pjcj desc; +------+--------+--------+--------+--------------+ | s_id | 语文 | 数学 | 英语 | 平均成绩 | +------+--------+--------+--------+--------------+ | 07 | 0 | 89 | 98 | 93.5000 | | 01 | 80 | 90 | 99 | 89.6667 | | 05 | 76 | 87 | 0 | 81.5000 | | 03 | 80 | 80 | 80 | 80.0000 | | 02 | 70 | 60 | 80 | 70.0000 | | 04 | 50 | 30 | 20 | 33.3333 | | 06 | 31 | 0 | 34 | 32.5000 | | 08 | 0 | 0 | 0 | 0.0000 | +------+--------+--------+--------+--------------+ 十八 查询各科成绩最高分、最低分和平均分：以如下形式显示：课程ID，课程name ，最高分，最低分，平均分，及格率，中等率，优良率，优秀率 select course.c_id, course.c_name, if(maxscore is null,0,maxscore) as '最高分', if(minscore is null,0,minscore) as '最低分', if(avgscore is null,0,avgscore) as '平均分', if((jg/totalcourse)*100 is null,0,(jg/totalcourse)*100) as '及格率', if((zd/totalcourse)*100 is null,0,(zd/totalcourse)*100) as '中等率', if((yl/totalcourse)*100 is null,0,(yl/totalcourse)*100) as '优良率', if((yx/totalcourse)*100 is null,0,(yx/totalcourse)*100) as '优秀率' from course left join (select c_id,max(s_score) as maxscore from score group by c_id ) a on course.c_id=a.c_id left join (select c_id,min(s_score) as minscore from score group by c_id ) b on course.c_id=b.c_id left join (select c_id,avg(s_score) as avgscore,count(s_score) as totalcourse from score group by c_id ) c on course.c_id=c.c_id left join (select c_id, count(s_score) as jg from score where s_score>=60 group by c_id) d on course.c_id=d.c_id left join (select c_id, count(s_score) as zd from score where s_score>=70 and s_score=80 and s_score=90 group by c_id) g on course.c_id=g.c_id order by course.c_id; +------+--------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | c_id | c_name | 最高分 | 最低分 | 平均分 | 及格率 | 中等率 | 优良率 | 优秀率 | +------+--------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | 01 | 语文 | 80 | 31 | 64.5000 | 66.6667 | 33.3333 | 33.3333 | 0.0000 | | 02 | 数学 | 90 | 30 | 72.6667 | 83.3333 | 0.0000 | 50.0000 | 16.6667 | | 03 | 英语 | 99 | 20 | 68.5000 | 66.6667 | 0.0000 | 33.3333 | 33.3333 | +------+--------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ 及格为>=60，中等为：70-80，优良为：80-90，优秀为：>=90 十九 按各科成绩进行排序，并显示排名 select c_id,s_id,s_score,row_num,rank,dense_rank from ( select a.*, if (@precid!=c_id,@rownum:=1,@rownum:=@rownum+1) as row_num, (case when @rownum=1 then @rank:=1 when @prescore=s_score then @rank else @rank:=@rownum end) as rank, (case when @rownum=1 then @denserank:=1 when @prescore=s_score then @denserank else @denserank:=@denserank+1 end) as dense_rank, @precid:=c_id, @prescore:=s_score from (select * from score order by c_id,s_score desc) a, (select @precid:=null,@rownum:=0,@prescore:=null,@rank:=0,@denserank:=0) b ) a; +------+------+---------+---------+------+------------+ | c_id | s_id | s_score | row_num | rank | dense_rank | +------+------+---------+---------+------+------------+ | 01 | 01 | 80 | 1 | 1 | 1 | | 01 | 03 | 80 | 2 | 1 | 1 | | 01 | 05 | 76 | 3 | 3 | 2 | | 01 | 02 | 70 | 4 | 4 | 3 | | 01 | 04 | 50 | 5 | 5 | 4 | | 01 | 06 | 31 | 6 | 6 | 5 | | 02 | 01 | 90 | 1 | 1 | 1 | | 02 | 07 | 89 | 2 | 2 | 2 | | 02 | 05 | 87 | 3 | 3 | 3 | | 02 | 03 | 80 | 4 | 4 | 4 | | 02 | 02 | 60 | 5 | 5 | 5 | | 02 | 04 | 30 | 6 | 6 | 6 | | 03 | 01 | 99 | 1 | 1 | 1 | | 03 | 07 | 98 | 2 | 2 | 2 | | 03 | 03 | 80 | 3 | 3 | 3 | | 03 | 02 | 80 | 4 | 3 | 3 | | 03 | 06 | 34 | 5 | 5 | 4 | | 03 | 04 | 20 | 6 | 6 | 5 | +------+------+---------+---------+------+------------+ row_num 连续排名，即使相同的值，依旧按照连续数字进行排名。 rank 并列跳跃排名，并列即相同的值，相同的值保留重复名次，遇到下一个不同值时，跳跃到总共的排名。 dense_rank 并列连续排序，并列即相同的值，相同的值保留重复名次，遇到下一个不同值时，依然按照连续数字排名。 二十 查询学生的总成绩并进行排名 select s_id,sumscore,row_num,rank from ( select s_id, sumscore, if(@presumscore is null,@rownum:=1,@rownum:=@rownum+1) as row_num, (case when @rownum=1 then @rank:=1 when @presumscore=sumscore then @rank else @rank:=@rownum end) as rank, @presumscore:=sumscore from (select s_id,sum(s_score) as sumscore from score group by s_id order by sumscore desc) a, (select @rownum:=0,@presumscore:=null,@rank:=0) b ) a; +------+----------+---------+------+ | s_id | sumscore | row_num | rank | +------+----------+---------+------+ | 01 | 269 | 1 | 1 | | 03 | 240 | 2 | 2 | | 02 | 210 | 3 | 3 | | 07 | 187 | 4 | 4 | | 05 | 163 | 5 | 5 | | 04 | 100 | 6 | 6 | | 06 | 65 | 7 | 7 | +------+----------+---------+------+ 二十一 查询不同老师所教不同课程平均分从高到低显示 select t.t_id,t.t_name,s.c_id,avg(s.s_score) as avgscore from teacher t left join course c on t.t_id=c.t_id left join score s on c.c_id=s.c_id group by s.c_id,t.t_id,t.t_name order by avgscore desc; +------+--------+------+----------+ | t_id | t_name | c_id | avgscore | +------+--------+------+----------+ | 01 | 张三 | 02 | 72.6667 | | 03 | 王五 | 03 | 68.5000 | | 02 | 李四 | 01 | 64.5000 | +------+--------+------+----------+ 二十二 查询所有课程的成绩第2名到第3名的学生信息及该课程成绩 select s.*,a.s_score,a.c_id,a.dense_rank from ( select c_id,s_id,s_score,row_num,rank,dense_rank from ( select a.*, if (@precid!=c_id,@rownum:=1,@rownum:=@rownum+1) as row_num, (case when @rownum=1 then @rank:=1 when @prescore=s_score then @rank else @rank:=@rownum end) as rank, (case when @rownum=1 then @denserank:=1 when @prescore=s_score then @denserank else @denserank:=@denserank+1 end) as dense_rank, @precid:=c_id, @prescore:=s_score from (select * from score order by c_id,s_score desc) a, (select @precid:=null,@rownum:=0,@prescore:=null,@rank:=0,@denserank:=0) b ) a where dense_rank in (2,3) ) a left join student s on a.s_id=s.s_id; +------+--------+------------+-------+---------+------+------------+ | s_id | s_name | s_birth | s_sex | s_score | c_id | dense_rank | +------+--------+------------+-------+---------+------+------------+ | 05 | 周梅 | 1991-12-01 | 女 | 76 | 01 | 2 | | 02 | 钱电 | 1990-12-21 | 男 | 70 | 01 | 3 | | 07 | 郑竹 | 1989-07-01 | 女 | 89 | 02 | 2 | | 05 | 周梅 | 1991-12-01 | 女 | 87 | 02 | 3 | | 07 | 郑竹 | 1989-07-01 | 女 | 98 | 03 | 2 | | 03 | 孙风 | 1990-05-20 | 男 | 80 | 03 | 3 | | 02 | 钱电 | 1990-12-21 | 男 | 80 | 03 | 3 | +------+--------+------------+-------+---------+------+------------+ 二十三 统计各科成绩各分数段人数：课程编号,课程名称,[100-85],[85-70],[70-60],[0-60]及所占百分比 select a.c_id, c.c_name, sum(l1) as `[0-60]`, sum(l1)/count(*)*100 as `[0-60]%`, sum(l2) as `[70-60]`, sum(l2)/count(*)*100 as `[70-60]%`, sum(l3) as `[85-70]`, sum(l3)/count(*)*100 as `[85-70]%`, sum(l4) as `[100-85]`, sum(l4)/count(*)*100 as `[100-85]%` from ( select score.*, (case when s_score>=0 and s_score60 and s_score70 and s_score85 and s_score 二十四 查询学生平均成绩及其名次 select s_id,avgscore,row_num,rank from ( select s_id, avgscore, if(@preavgscore is null,@rownum:=1,@rownum:=@rownum+1) as row_num, (case when @rownum=1 then @rank:=1 when @preavgscore=avgscore then @rank else @rank:=@rownum end) as rank, @preavgscore:=avgscore from (select s_id,avg(s_score) as avgscore from score group by s_id order by avgscore desc) a, (select @rownum:=0,@preavgscore:=null,@rank:=0) b ) a; +------+----------+---------+------+ | s_id | avgscore | row_num | rank | +------+----------+---------+------+ | 07 | 93.5000 | 1 | 1 | | 01 | 89.6667 | 2 | 2 | | 05 | 81.5000 | 3 | 3 | | 03 | 80.0000 | 4 | 4 | | 02 | 70.0000 | 5 | 5 | | 04 | 33.3333 | 6 | 6 | | 06 | 32.5000 | 7 | 7 | +------+----------+---------+------+ 二十五 查询各科成绩前三名的记录 select s.*,a.s_score,a.c_id,a.dense_rank from ( select c_id,s_id,s_score,row_num,rank,dense_rank from ( select a.*, if (@precid!=c_id,@rownum:=1,@rownum:=@rownum+1) as row_num, (case when @rownum=1 then @rank:=1 when @prescore=s_score then @rank else @rank:=@rownum end) as rank, (case when @rownum=1 then @denserank:=1 when @prescore=s_score then @denserank else @denserank:=@denserank+1 end) as dense_rank, @precid:=c_id, @prescore:=s_score from (select * from score order by c_id,s_score desc) a, (select @precid:=null,@rownum:=0,@prescore:=null,@rank:=0,@denserank:=0) b ) a where dense_rank in (1,2,3) ) a left join student s on a.s_id=s.s_id; +------+--------+------------+-------+---------+------+------------+ | s_id | s_name | s_birth | s_sex | s_score | c_id | dense_rank | +------+--------+------------+-------+---------+------+------------+ | 01 | 赵雷 | 1990-01-01 | 男 | 80 | 01 | 1 | | 03 | 孙风 | 1990-05-20 | 男 | 80 | 01 | 1 | | 05 | 周梅 | 1991-12-01 | 女 | 76 | 01 | 2 | | 02 | 钱电 | 1990-12-21 | 男 | 70 | 01 | 3 | | 01 | 赵雷 | 1990-01-01 | 男 | 90 | 02 | 1 | | 07 | 郑竹 | 1989-07-01 | 女 | 89 | 02 | 2 | | 05 | 周梅 | 1991-12-01 | 女 | 87 | 02 | 3 | | 01 | 赵雷 | 1990-01-01 | 男 | 99 | 03 | 1 | | 07 | 郑竹 | 1989-07-01 | 女 | 98 | 03 | 2 | | 03 | 孙风 | 1990-05-20 | 男 | 80 | 03 | 3 | | 02 | 钱电 | 1990-12-21 | 男 | 80 | 03 | 3 | +------+--------+------------+-------+---------+------+------------+ 二十六 查询每门课程被选修的学生数 select c_id,count(*) as sn from score group by c_id; +------+----+ | c_id | sn | +------+----+ | 01 | 6 | | 02 | 6 | | 03 | 6 | +------+----+ 二十七 查询出只有两门课程的全部学生的学号和姓名 select a.s_id,b.s_name from (select s_id from score group by s_id having count(c_id)=2) a left join student b on a.s_id=b.s_id; +------+--------+ | s_id | s_name | +------+--------+ | 05 | 周梅 | | 06 | 吴兰 | | 07 | 郑竹 | +------+--------+ 二十八 查询男生、女生人数 select s_sex,count(s_id) from student group by s_sex; +-------+-------------+ | s_sex | count(s_id) | +-------+-------------+ | 女 | 4 | | 男 | 4 | +-------+-------------+ 二十九 查询名字中含有\"风\"字的学生信息 select * from student where s_name like '%风%'; +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 03 | 孙风 | 1990-05-20 | 男 | +------+--------+------------+-------+ 三十 查询同名同姓学生名单，并统计同名人数 select s_name,number-1 as `同名人数` from (select s_name,count(*) number from student group by s_name) a; +--------+--------------+ | s_name | 同名人数 | +--------+--------------+ | 吴兰 | 0 | | 周梅 | 0 | | 孙风 | 0 | | 李云 | 0 | | 王菊 | 0 | | 赵雷 | 0 | | 郑竹 | 0 | | 钱电 | 0 | +--------+--------------+ 三十一 查询1990年出生的学生名单 select * from student where year(s_birth)='1990'; +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 01 | 赵雷 | 1990-01-01 | 男 | | 02 | 钱电 | 1990-12-21 | 男 | | 03 | 孙风 | 1990-05-20 | 男 | | 04 | 李云 | 1990-08-06 | 男 | | 08 | 王菊 | 1990-01-20 | 女 | +------+--------+------------+-------+ 三十二 查询每门课程的平均成绩，结果按平均成绩降序排列，平均成绩相同时，按课程编号升序排列 select c_id,avg(s_score) avgscore from score group by c_id order by avgscore desc,c_id; +------+----------+ | c_id | avgscore | +------+----------+ | 02 | 72.6667 | | 03 | 68.5000 | | 01 | 64.5000 | +------+----------+ 三十三 查询平均成绩大于等于85的所有学生的学号、姓名和平均成绩 select a.s_id,b.s_name,avgscore from (select s_id,avg(s_score) avgscore from score group by s_id having avgscore>=85) a left join student b on a.s_id=b.s_id; +------+--------+----------+ | s_id | s_name | avgscore | +------+--------+----------+ | 01 | 赵雷 | 89.6667 | | 07 | 郑竹 | 93.5000 | +------+--------+----------+ 三十四 查询课程名称为\"数学\"，且分数低于60的学生姓名和分数 select student.s_name,score.s_score from course left join score on course.c_id=score.c_id left join student on score.s_id=student.s_id where course.c_name='数学' and score.s_score 三十五 查询所有学生的课程及分数情况 select student.s_id, if(yw is null,0,yw) as '语文', if(sx is null,0,sx) as '数学', if(yy is null,0,yy) as '英语' from student left join (select s_id,s_score as yw from score where c_id='01') a on student.s_id=a.s_id left join (select s_id,s_score as sx from score where c_id='02') b on student.s_id=b.s_id left join (select s_id,s_score as yy from score where c_id='03') c on student.s_id=c.s_id; +------+--------+--------+--------+ | s_id | 语文 | 数学 | 英语 | +------+--------+--------+--------+ | 01 | 80 | 90 | 99 | | 02 | 70 | 60 | 80 | | 03 | 80 | 80 | 80 | | 04 | 50 | 30 | 20 | | 05 | 76 | 87 | 0 | | 06 | 31 | 0 | 34 | | 07 | 0 | 89 | 98 | | 08 | 0 | 0 | 0 | +------+--------+--------+--------+ 三十六 查询任何一门课程成绩在70分以上的姓名、课程名称和分数 select b.s_name,c.c_name,a.s_score from (select s_id,c_id,s_score from score where s_score>70) a left join student b on a.s_id=b.s_id left join course c on a.c_id=c.c_id; +--------+--------+---------+ | s_name | c_name | s_score | +--------+--------+---------+ | 赵雷 | 语文 | 80 | | 孙风 | 语文 | 80 | | 周梅 | 语文 | 76 | | 赵雷 | 数学 | 90 | | 孙风 | 数学 | 80 | | 周梅 | 数学 | 87 | | 郑竹 | 数学 | 89 | | 赵雷 | 英语 | 99 | | 钱电 | 英语 | 80 | | 孙风 | 英语 | 80 | | 郑竹 | 英语 | 98 | +--------+--------+---------+ 三十七 查询不及格的课程 select a.s_id,a.c_id,b.c_name,a.s_score from (select s_id,c_id,s_score from score where s_score 三十八 查询课程编号为01且课程成绩在80分以上的学生的学号和姓名 select a.s_id,b.s_name from (select s_id from score where c_id='01' and s_score>80) a left join student b on a.s_id=b.s_id; Empty set 三十九 求每门课程的学生人数 select c_id, count(*) from score group by c_id; +------+----------+ | c_id | count(*) | +------+----------+ | 01 | 6 | | 02 | 6 | | 03 | 6 | +------+----------+ 四十 查询选修\"张三\"老师所授课程的学生中，成绩最高的学生信息及其成绩 select b.*,a.c_id,a.s_score from (select s_id,c_id,s_score from score where c_id in ( select c_id from course where t_id in (select t_id from teacher where t_name='张三')) order by s_score desc limit 0,1) a left join student b on a.s_id=b.s_id; +------+--------+------------+-------+------+---------+ | s_id | s_name | s_birth | s_sex | c_id | s_score | +------+--------+------------+-------+------+---------+ | 01 | 赵雷 | 1990-01-01 | 男 | 02 | 90 | +------+--------+------------+-------+------+---------+ 四十一 查询不同课程成绩相同的学生的学生编号、课程编号、学生成绩 select distinct a.s_id,a.c_id,a.s_score from score a,score b where a.s_id=b.s_id and a.s_score=b.s_score and a.c_id!=b.c_id; +------+------+---------+ | s_id | c_id | s_score | +------+------+---------+ | 03 | 01 | 80 | | 03 | 02 | 80 | | 03 | 03 | 80 | +------+------+---------+ from A, B where A.id=B.id 等价于 from A inner join B on A.id=B.id 四十二 查询每门课程成绩最好的前两名 select c_id,s_id,s_score from ( select a.*, if (@precid!=c_id,@rownum:=1,@rownum:=@rownum+1) as row_num, (case when @rownum=1 then @rank:=1 when @prescore=s_score then @rank else @rank:=@rownum end) as rank, (case when @rownum=1 then @denserank:=1 when @prescore=s_score then @denserank else @denserank:=@denserank+1 end) as dense_rank, @precid:=c_id, @prescore:=s_score from (select * from score order by c_id,s_score desc) a, (select @precid:=null,@rownum:=0,@prescore:=null,@rank:=0,@denserank:=0) b ) a where row_num in (1,2); +------+------+---------+ | c_id | s_id | s_score | +------+------+---------+ | 01 | 01 | 80 | | 01 | 03 | 80 | | 02 | 01 | 90 | | 02 | 07 | 89 | | 03 | 01 | 99 | | 03 | 07 | 98 | +------+------+---------+ 四十三 统计每门课程的学生选修人数（超过5人的课程才统计）。要求输出课程号和选修人数，查询结果按人数降序排列，若人数相同，按课程号升序排列 select c_id,count(c_id) as number from score group by c_id having number>5 order by number desc,c_id asc; +------+--------+ | c_id | number | +------+--------+ | 01 | 6 | | 02 | 6 | | 03 | 6 | +------+--------+ 四十四 检索至少选修两门课程的学生学号 select s_id from score group by s_id having count(*)>=2; +------+ | s_id | +------+ | 01 | | 02 | | 03 | | 04 | | 05 | | 06 | | 07 | +------+ 四十五 查询选修了全部课程的学生信息 select * from student where s_id in ( select s_id from score group by s_id having count(*)=(select count(*) from course) ); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 01 | 赵雷 | 1990-01-01 | 男 | | 02 | 钱电 | 1990-12-21 | 男 | | 03 | 孙风 | 1990-05-20 | 男 | | 04 | 李云 | 1990-08-06 | 男 | +------+--------+------------+-------+ 四十六 查询各学生的年龄 select s_id, s_name, s_birth, YEAR(CURRENT_DATE())- YEAR(s_birth)- IF(DATE_FORMAT(CURRENT_DATE(),'%m-%d')>=DATE_FORMAT(s_birth,'%m-%d'),0,1) as age from student; +------+--------+------------+------+ | s_id | s_name | s_birth | age | +------+--------+------------+------+ | 01 | 赵雷 | 1990-01-01 | 30 | | 02 | 钱电 | 1990-12-21 | 29 | | 03 | 孙风 | 1990-05-20 | 30 | | 04 | 李云 | 1990-08-06 | 29 | | 05 | 周梅 | 1991-12-01 | 28 | | 06 | 吴兰 | 1992-03-01 | 28 | | 07 | 郑竹 | 1989-07-01 | 30 | | 08 | 王菊 | 1990-01-20 | 30 | +------+--------+------------+------+ 考虑到了本年是否已经过了生日的问题 四十七 查询本周过生日的学生 # CURRENT_DATE() 2020-06-27 select * from student where DATE_FORMAT(s_birth,'%m-%d') between DATE_FORMAT(DATE_ADD(CURRENT_DATE(), INTERVAL ( 7 - (WEEKDAY(CURRENT_DATE())+1) + 1 - 7 ) DAY),'%m-%d') and DATE_FORMAT(DATE_ADD(CURRENT_DATE(), INTERVAL ( 7 - (WEEKDAY(CURRENT_DATE())+1) ) DAY),'%m-%d'); Empty set 四十八 查询下周过生日的学生 # CURRENT_DATE() 2020-06-27 select * from student where DATE_FORMAT(s_birth,'%m-%d') between DATE_FORMAT(DATE_ADD(CURRENT_DATE(), INTERVAL ( 7 - (WEEKDAY(CURRENT_DATE())+1) + 1 - 7 + 7 ) DAY),'%m-%d') and DATE_FORMAT(DATE_ADD(CURRENT_DATE(), INTERVAL ( 7 - (WEEKDAY(CURRENT_DATE())+1) + 7 ) DAY),'%m-%d'); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 07 | 郑竹 | 1989-07-01 | 女 | +------+--------+------------+-------+ 四十九 查询本月过生日的学生 # CURRENT_DATE() 2020-06-27 select * from student where DATE_FORMAT(s_birth,'%m')=DATE_FORMAT(CURRENT_DATE(),'%m'); Empty set 五十 查询下月过生日的学生 # CURRENT_DATE() 2020-06-27 select * from student where MONTH(s_birth)=(MONTH(NOW())+1); +------+--------+------------+-------+ | s_id | s_name | s_birth | s_sex | +------+--------+------------+-------+ | 07 | 郑竹 | 1989-07-01 | 女 | +------+--------+------------+-------+ Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/database/redis/Redis核心功能.html":{"url":"src/database/redis/Redis核心功能.html","title":"Redis核心功能","keywords":"","body":"缓存 数据分类和使用频率 数据分类 静态数据：一般不变，类似于字典表 准静态数据：变化频率很低，部门结构设置，全国行政区划数据等 中间状态数据：一些计算的可复用中间数据，变量副本，配置中心的本地副本 这些数据适合于使用缓存的方式访问 热数据：使用频率高 读写比较大：读的频率 >> 写的频率 缓存加载时机 启动全量加载 全局有效，使用简单 懒加载 同步使用加载 先看缓存是否有数据，没有的话从数据库读取 读取的数据，先放到内存，然后返回给调用方 延迟异步加载 从缓存获取数据，不管是否为空直接返回 如果为空，则发起一个异步加载的线程，负责加载数据 异步线程负责维护缓存的数据，定期或根据条件触发更新 缓存的有效性 读写比 对数据的写操作导致数据变动，意味着维护成本。N : 1 命中率 命中缓存意味着缓存数据被使用，意味着有价值。90%+ 缓存使用不当导致的问题 系统预热导致启动慢 试想一下，一个系统启动需要预热半个小时。 导致系统不能做到快速应对故障宕机等问题。 系统内存资源耗尽 只加入数据，不能清理旧数据。 旧数据处理不及时，或者不能有效识别无用数据。 缓存常见问题 缓存穿透 大量并发查询不存在的key，导致直接将压力透传到数据库 解决办法 缓存空值的key Bloom过滤或RoaringBitmap判断key是否存在 完全以缓存为准，使用延迟异步加载策略 缓存击穿 某个key失效的时候，正好有大量并发请求访问这个key 解决办法 key的更新操作添加全局互斥锁 完全以缓存为准，使用延迟异步加载策略 缓存雪崩 当某一时刻发生大规模缓存失效的情况，会有大量的请求直接打到数据库，导致数据库压力过大甚至宕机。 解决办法 更新策略在时间上做到比较均匀 使用的热数据尽量分散到不同的机器上 多台机器做主从复制或多副本，实现高可用 实现熔断限流机制，对系统进行负载能力控制 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-29 17:43:05 "},"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html":{"url":"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html","title":"GitBook&GitHub&Typora最佳实践","keywords":"","body":"概述 目前有众多的知识管理软件，对于比较私密的文档使用印象笔记，不仅支持Markdown这种轻量级的标记语言，也支持复杂的富文本语言，但是一些比较有用的功能则需要收费，比如查看历史版本、离线访问、容量限制。最近，使用了一下阿里的语雀，书写体验、排版和易操作方面确实属于不错的一款产品，可是，某一天断网了，不支持离线访问是硬伤。因此，想要寻找如下一款产品满足如下需求： 沉浸式的书写体验，不需要丰富的社交场景，也不需要过于酷炫的功能，只想安安静静的码字。 离线编辑，想什么时候访问自己做主。同时，同步是必不可少的。 支持版本管理。 不收费，也没有容量限制。想写多少写多少。 支持私有笔记。 支持同其他同事、同学沟通，因为大部分都是程序员，所以可以查看Markdown文件是必不可少的。 对笔记中的目录结构没有限制。 对笔记中关联的图像，本地和远程都可以访问，而且没有容量限制。 可以方便的导出其他格式，比如html，PDF等。 通过调研，使用GitHub&GitBook&Typora方案，可以完美的解决以上问题。 搭建环境 GitHub GitHub创建develop-stack项目，其中.gitignore选择gitbook 回到本地创建本地目录 mkdir note && cd note 下载项目到本地 git clone https://github.com/sciatta/develop-stack.git GitBook 安装nodejs brew install node 安装GitBook命令行工具 npm install -g gitbook-cli 在develop-stack目录执行 gitbook init 初始化Gitbook目录环境，生成两个重要文件README.md和SUMMARY.md Typora 编辑SUMMARY.md描述项目的目录和文件结构 在develop-stack目录执行 gitbook init 。GitBook会查找SUMMARY.md文件中描述的目录和文件，如果没有则会将其创建。注意，如果删除SUMMARY.md描述项目的目录和文件结构，执行 gitbook init 命令不会删除相应的目录或文件，需要手动维护。 GitBook 本地运行 gitbook serve --port 8088 开启GitBook服务。通过 http://localhost:8080 访问本地服务。在执行命令的同时会执行构建命令 gitbook build 生成 _book 目录。注意， _book 目录是临时目录，每次构建时全部重建。如果退出服务的话，执行 control + c 即可 本地构建 gitbook build ./ docs 。 GitHub 执行 git add 命令 git add -A 将文件提交到暂存区 执行 git commit 命令 git commit -m “test” 将文件提交到本地仓库 执行 git push 命令 git push origin master 将文件提交到远程仓库 GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Source选择 master branch/docs folder 。成功后显示 Your site is ready to be published at https://sciatta.github.io/develop-stack/ ，稍后即可访问GitHub Pages网站。 GitBook插件 在develop-stack目录创建book.json文件，配置插件后，运行 gitbook install 命令自动安装插件。 hide-element 隐藏默认gitbook左侧提示：Published with GitBook { \"plugins\": [ \"hide-element\" ], \"pluginsConfig\": { \"hide-element\": { \"elements\": [\".gitbook-link\"] } } } expandable-chapters gitbook默认目录没有折叠效果。 { \"plugins\": [ \"expandable-chapters\" ] } code 在代码区域的右上角添加一个复制按钮，点击一键复制代码。 { \"plugins\" : [ \"code\" ] } splitter 左侧目录和右侧文章可以拖动调节宽度。 { \"plugins\": [ \"splitter\" ] } search-pro 支持中英文。 { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ] } pageview-count 记录每个文章页面被访问的次数。本质是访问 https://hitcounter.pythonanywhere.com/ { \"plugins\": [ \"pageview-count\"] } tbfed-pagefooter 在每个文章下面标注版权信息和文章时间。 { \"plugins\": [ \"tbfed-pagefooter\" ], \"pluginsConfig\": { \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy sciatta.com 2020\", \"modify_label\": \"修订时间: \", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" } } } popup 点击可以在新窗口展示图片。 { \"plugins\": [ \"popup\" ] } auto-scroll-table 为避免表格过宽，增加滚动条。 { \"plugins\": [\"auto-scroll-table\"] } -sharing 去掉 GitBook 默认的分享功能。由于默认的一些推特，脸书都需要翻墙，所以将分享功能全部关闭。 \"plugins\": [ \"-sharing\" ] github-buttons 给 GitBook 添加 GitHub 的图标来显示 star 和 follow。 { \"plugins\": [ \"github-buttons\" ], \"pluginsConfig\": { \"github-buttons\": { \"buttons\": [{ \"user\": \"sciatta\", \"repo\": \"develop-stack\", \"type\": \"star\", \"count\": true, \"size\": \"small\" }] } } } github 在右上角显示 github 仓库的图标链接 { \"plugins\": [ \"github\" ], \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/sciatta\" } } } Google统计 添加Google统计 Google统计 https://analytics.google.com/ 在一个平台上可以全面分析业务数据，进而做出更明智的决策。在网站注册，获取跟踪ID { \"plugins\": [\"ga\"], \"pluginsConfig\": { \"ga\": { \"token\": \"UA-156491400-1\" } } } BaiDu 添加BaiDu统计 百度统计 https://tongji.baidu.com/ 在网站注册，获取跟踪ID { \"plugin\": [\"baidu-v3\"], \"pluginsConfig\": { \"baidu\": { \"token\": \"c6612709c010da681bbd4b785968a638\" } } } Donate Gitbook 捐赠打赏插件 { \"plugins\": [\"donate\"], \"pluginsConfig\": { \"donate\": { \"wechat\": \"/assets/wechat.jpg\", \"alipay\": \"/assets/alipay.jpg\", \"title\": \"\", \"button\": \"捐赠\", \"alipayText\": \" \", \"wechatText\": \" \" } } } anchors 标题带有 github 样式的锚点 { \"plugins\" : [ \"anchors\" ] } anchor-navigation-ex 页面内导航，一键回到顶部。 { \"plugins\": [\"anchor-navigation-ex\"], \"pluginsConfig\": { \"anchor-navigation-ex\": { \"showLevel\": true, \"associatedWithSummary\": false, \"printLog\": false, \"multipleH1\": true, \"mode\": \"float\", \"showGoTop\": true, \"float\": { \"floatIcon\": \"fa fa-navicon\", \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" }, \"pageTop\": { \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" } } } } sitemap 生成站点地图，便于爬虫抓取页面。可以通过http://www.sciatta.com/sitemap.xml 访问。 { \"plugins\": [\"sitemap\"], \"pluginsConfig\": { \"sitemap\": { \"hostname\": \"http://www.sciatta.com/\" } } } Typora设置 偏好设置 | 编辑器 | 图片插入 复制图片到 ./${filename}.assets 文件夹 优先使用相对路径 偏好设置 | 通用 | 启动选项 | 打开指定目录 选择工作目录 偏好设置 | 通用 | 侧边栏 侧边栏的大纲视图允许折叠和展开 绑定域名 获取GitHub Pages的IP地址 ping -c 3 sciatta.github.io 注意，ping的时候不需要加仓库的名称。 配置阿里云 进入阿里云解析列表，添加记录： 记录类型 主机记录 记录值 A @ 185.199.111.153 A www 185.199.111.153 配置GitHub Pages GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Custom domain：www.sciatta.com Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-29 21:41:07 "},"src/versioncontrol/git/Git命令.html":{"url":"src/versioncontrol/git/Git命令.html","title":"Git命令","keywords":"","body":"Git配置 Git 提供 git config 工具，专门用来配置或读取相应的工作环境变量。 /etc/gitconfig 对所有用户有效，git config --system 读取此文件 ~/.gitconfig 对当前用户有效，git config --global 读取此文件 工作目录中的 .git/config 对当前项目有效，git config 读取此文件 用户信息 git config --global user.name \"rain\" git config --global user.email 15058553030@163.com 文本编辑器 git config --global core.editor emacs 差异分析工具 git config --global merge.tool vimdiff 查看配置信息 # 查看全部 git config --list # 查看某一个 git config user.name 仓库管理 git init 在执行完成 git init 命令后，Git 仓库会生成一个 .git 目录，该目录包含了资源的所有元数据，其他的项目目录保持不变。默认创建master分支。 # 在当前目录生成.git目录 git init # 指定目录作为仓库 git init newrepo git clone # 会在当前目录创建hadoop-main目录 git clone https://github.com/sciatta/hadoop-main.git # 指定本地目录 git clone https://github.com/sciatta/hadoop-main.git hadoop git remote git本地仓库同github仓库映射有两种方式： 先建立本地仓库，然后再同github仓库关联；适用于将已有项目托管到github。 # 创建本地仓库 git init # 关联本地仓库和远程仓库 git remote add origin https://github.com/sciatta/hadoop-main.git # 推送更新 git push origin master 先建立github仓库，然后再同本地仓库关联；适用于在开发之前，先在github托管项目。 # 创建本地仓库，同时关联 git clone https://github.com/sciatta/hadoop-main.git # 查看远程仓库 git remote # 别名对应的实际链接地址 # 如： # origin https://github.com/sciatta/hadoop-main.git (fetch) # origin https://github.com/sciatta/hadoop-main.git (push) git remote -v # 关联远程仓库，别名同实际链接地址做映射 git remote add github https://github.com/sciatta/hadoop-main.git # 取消关联 git remote rm github git fetch fetch和pull的区别 head指向本地分支master；remotes指向远程分支orgin/master fetch只更新remotes；需要手动merge pull更新remotes和head # 从远程仓库下载最新分支与数据 # 此命令执行完后需要执行 git merge origin/master 远程分支到你所在的 master 分支 git fetch origin master git pull # 拉取远程仓库origin的master分支到本地 git pull orgin master git push # 将master分支推送到远程仓库origin的master分支 git push origin master 快照操作 git add 将文件添加到缓存区 # 将README文件添加到缓存 git add README # 添加当前目录的所有文件 git add -A git status # 查看项目的当前状态 git status # 显示简要状态 git status -s git diff 查看执行 git status 的结果的详细信息 # 未缓存的改动 git diff # 已缓存的改动 git diff --cached # 查看已缓存的与未缓存的所有改动 git diff HEAD # 显示摘要 git diff --stat git commit 将缓存区内容添加到仓库中 # 添加到本地仓库，-m 后跟注释 # 如果没有 -m 注释的话，git会打开一个文本编辑器 git commit -m '第一次版本提交' # 跳过 add git commit -a # 修改上一次提交，可用于修改误写注释 git commit --amend git reset HEAD 取消已缓存的内容 git reset HEAD README git rm 从已跟踪文件清单中移除 # 从已跟踪文件清单中移除，同时清除工作目录文件 git rm README # 从已跟踪文件清单中移除，工作目录文件仍然存在 git rm --cached README # 递归删除 git rm –r * git mv # 移动或重命名一个文件、目录、软连接 git mv README README.md git log # 回顾提交历史 git log # 简洁 git log --oneline # 开启拓扑图选项 git log --graph # 逆向显示所有日志 git log --reverse 分支管理 git branch # 创建分支 git branch abc # 没有参数时，列出当前的本地分支 git branch # 列出本地和远程分支 git branch -a # 删除分支 git branch -d abc git checkout # 切换分支 git checkout abc # 创建分支，之后立即切换到此分支 git checkout -b abc git merge # 合并分支 # 假设当前分支是abc，则将test分支内容合并到当前abc分支 git merge test # 合并时发生冲突 >>>>>> test # 修改后 git add 通知 git 冲突解决完毕 标签管理 如果达到一个重要的阶段，并希望永远记住那个特别的提交快照，可以使用 git tag 给它打上标签。 git tag # 创建带注释的标签 git tag -a v1.0 # 为历史提交版本创建标签 git tag -a v0.9 85fc7e7 # 查看所有标签 git tag Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-25 09:22:54 "},"src/versioncontrol/git/GitHub日常使用.html":{"url":"src/versioncontrol/git/GitHub日常使用.html","title":"GitHub日常使用","keywords":"","body":"GitHub 开发规范 Master 稳定分支 长存 只从Hotfix和Release分支合并，不会直接commit 经过测试的分支，但不是最新的 Develop 开发分支 长存 未经过测试的分支，保持最新 Feature 特性分支 开发特定需求且时间较长时建立，开发完成向Develop分支合并后删除 Release 发布分支 测试时建立，测试完成合并后删除 从Develop分支，是准备释出的版本，只修改版本号与Bug，完成后合并回Develop和Master分支，并在Master上标出版本号的tag Hotfix 缺陷分支 修复生产Bug时建立，修复完成合并后删除 从Master分支，处理已释出版本需要立即修改的错误，完成后合并回Develop和Master分支，并在Master上标出版本号的tag create develop branch git branch develop # 本地develop上传到远程分支（如果没有则创建），然后再关联upstream git push -u origin develop Start new feature # 以develop为基准创建feature分支，并切换到feature分支 git checkout -b some-feature develop # 可选提交到远程 git push -u origin some-feature git status git add some-file git commit finish the feature # 合并前先更新，保证提交不冲突 git pull origin develop git checkout develop # 关闭fast-forward模式，保留分支commit信息 git merge --no-ff some-feature git push origin develop # 删除本地feature分支 git branch -d some-feature # 删除远程feature分支，如有 git push origin --delete some-feature Start new release git checkout -b release-0.1.0 develop Finish the release git pull origin master git checkout master git merge --no-ff release-0.1.0 git push git pull origin develop git checkout develop git merge --no-ff release-0.1.0 git push git branch -d release-0.1.0 git push origin --delete release-0.1.0 # 给当前master版本打上tag git tag -a v0.1.0 master # 提交到远程 git push --tags Start new hotfix git checkout -b hotfix-0.1.1 master Finish the hotfix git pull origin master git checkout master git merge --no-ff hotfix-0.1.1 git push git pull origin develop git checkout develop git merge --no-ff hotfix-0.1.1 git push git branch -d hotfix-0.1.1 git tag -a v0.1.1 master git push --tags 向开源项目提交 Pull request 将开源项目fork到远程origin仓库 https://github.com/sciatta/shardingsphere 将远程origin仓库clone到本地仓库 将远程origin仓库clone到本地仓库 git clone git@github.com:sciatta/shardingsphere.git 通过 git status 查看当前所在分支 On branch master 通过 git remote -v 查看远程仓库映射关系 origin git@github.com:sciatta/shardingsphere.git (fetch) origin git@github.com:sciatta/shardingsphere.git (push) 与上游upstream仓库建立映射关系 建立映射 git remote add upstream https://github.com/apache/shardingsphere.git 通过 git remote -v 查看远程仓库映射关系 origin git@github.com:sciatta/shardingsphere.git (fetch) origin git@github.com:sciatta/shardingsphere.git (push) upstream https://github.com/apache/shardingsphere.git (fetch) upstream https://github.com/apache/shardingsphere.git (push) 创建本地仓库develop分支开发 创建develop分支 git checkout -b develop 在此分支上进行开发 git add -A 提交本地仓库 git commit -m 提交远程origin仓库 本地仓库拉取远程upstream仓库的最新内容 git fetch upstream 切换到本地master分支 git checkout master 本地仓库和远程upstream仓库的master分支同步 git rebase upstream/master rebase 合并为一条时间轴。如在master中执行git rebase develop，找到master和develop的公共祖先，祖先先合并develop的新增提交，然后在后面追加master的新增提交，即变更起始点。注意必须没有待提交的文件 优点：得到更简洁的项目历史，去掉了merge commit 缺点：如果合并出现代码问题不容易定位，因为re-write了history merge 合并路径为分叉时间轴。如在master中执行git merge develop，找到master和develop的公共祖先，然后由公共祖先、master最新提交和develop最新提交，三方合并产生一个新的提交 优点：记录了真实的commit情况，包括每个分支的详情 缺点：因为每次merge会自动产生一个merge commit，所以在使用一些git 的GUI tools，特别是commit比较频繁时，看到分支很杂乱。 切换到本地develop分支 git checkout develop 不要在master分区rebase，这样会将最新修改放在最前面 合并最新master分支 git rebase master 远程master分支位置不变，其他人修改位置不变。但最新起始点已经变成develop的位置，本地master位置改变 提交到远程，其他人更新前fetch最新版本再提交，不会冲突 将本地develop分支提交到origin仓库 git push origin develop:develop 提交 Pull Request 在远程origin仓库 https://github.com/sciatta/shardingsphere 提交 Pull request upstream/main orgin/develop 合并upstream最新版本 同步本地master git fetch upstream git checkout master git rebase upstream/master 同步本地develop git checkout develop git rebase master 更新远程 git push origin master git push origin develop Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-25 17:06:12 "},"src/virtualization/vmware/VMware系统安装.html":{"url":"src/virtualization/vmware/VMware系统安装.html","title":"VMware系统安装","keywords":"","body":"Mac安装VMware 安装VMware_Fusion_Pro_11.5 注册码 7HYY8-Z8WWY-F1MAN-ECKNY-LUXYX 设置网络 偏好设置 | 网络 | 新建vmnet2自定义网络连接 选择：使用NAT 选择：将MAC主机连接到该网络 取消选择：通过DHCP在该网络上提供地址 修改子网 # 空格需用\\转义 cd /Library/Preferences/VMware\\ Fusion sudo vi networking answer VNET_2_HOSTONLY_SUBNET 192.168.2.0 修改网关 cd vmnet2 sudo vi nat.conf # NAT gateway address ip = 192.168.2.2 偏好设置 | 网络 | vmnet2 取消选择：将MAC主机连接到该网络 | 应用 选择：将MAC主机连接到该网络 | 应用 目的是为了使配置生效。 创建自定义虚拟机 选择操作系统 Linux | Centos 7 64 创建完成后设置 内存：2048 MB 硬盘：40 GB 网络适配器：vmnet2 启动磁盘：CD/DVD（即设置BIOS） 安装Centos7 设置中选择 CD/DVD（IDE） 选中连接CD/DVD驱动器 选择镜像CentOS-7-x86_64-DVD-1810.iso 启动 install Centos 7 language：english date & time：Asia/shanghai installation destination：automatic partitioning selected network & hostname：ens33 | on root password：root（太短，双击确认即可） Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/virtualization/docker/Docker架构.html":{"url":"src/virtualization/docker/Docker架构.html","title":"Docker架构","keywords":"","body":"Docker架构 基础 什么是容器 ​ 作为容器，包含了开发、共享、部署的一切，包括代码，运行时，系统库，系统工具和相关的设置。因此，它可以快速运行，并可以在多台不同环境的计算机间无缝运行，避免解决各种因软件和硬件环境差异带来的各种开发和系统问题。 安装 ​ 登录DockerHub ​ 下载Docker.dmg，并安装 卸载 ​ 在【应用程序】卸载docker ​ 无法使用docker命令 架构 Docker使用的是client-server架构。Docker client同Docker daemon（守护进程）通信，Docker daemon负责构建、运行和分发Docker容器。Docker client和Docker daemon可以运行在同一个系统内，Docker client也可以连接到一个远程的Docker daemon。 ​ The Docker daemon ​ Docker daemon（dockerd）监听Docker API请求，管理Docker对象（镜像，容器，网络和卷）。一个daemon也可以同其他daemons通信来管理Docker服务。 ​ The Docker client ​ Docker client（docker）是Docker用户与Docker交互的主要方式。当您使用docker run等命令时，client会将这些命令发送给dockerd，dockerd会执行这些命令。docker命令使用docker API。Docker client可以与多个daemon通信。 ​ Docker registries ​ Docker registry存储Docker镜像。Docker Hub是一个公共的注册中心，任何人均可使用，Docker默认从Docker Hub中获取镜像。 ​ Docker objects ​ IMAGES 镜像 ​ 创建Docker容器的只读模板。为了创建自己的镜像，你需要创建一个Dockerfile，定义一些语法步骤，来创建和运行一个镜像。在Dockerfile中的每一个指令创建镜像中的一层。当你改变Dockerfile，然后重建镜像，仅仅改变的那些层被重建。 ​ CONTAINERS 容器 ​ 一个容器是一个镜像的运行实例。一个容器与其他容器，主机隔离。 ​ SERVICES 服务 ​ 服务允许跨多个Docker daemon来扩展容器，多个管理者和工作者作为一个集群工作。一个集群的每一个成员都是一个Docker daemon，这些daemon使用Docker API来通信。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/virtualization/docker/Docker命令.html":{"url":"src/virtualization/docker/Docker命令.html","title":"Docker命令","keywords":"","body":"Docker命令 管理命令 docker builder docker checkpoint docker config docker container docker container run docker container run --publish 8000:8080 --detach --name tc ti:1.0 --detach: 在后台运行容器，打印容器ID --name: 给容器指定一个名称 --publish: 发布容器的端口到主机，要求Docker将主机8000端口的流量转发到容器的8080端口 在一个新容器中运行命令。其中，容器名称必须唯一。 docker container ls docker container ls -a -a: 显示所有容器，默认仅显示正在运行的容器 列举所有容器 docker container rm docker container rm --force tc --force: 强制删除正在运行的容器 删除一个或多个容器。 docker context docker image docker image build docker image build -t ti:1.0 . -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签 执行Dockerfile文件命令，构建镜像。成功后显示 Successfully tagged testubuntu:1.0。 docker image ls docker image ls 列举所有镜像 docker image rm docker image rm ti 删除本地镜像文件，rm后可以是镜像名称，也可以是镜像 ID。 docker image tag docker image tag ti:1.0 sciatta/ti:1.0 为镜像创建标签，创建的新标签指向的是原镜像。 在Docker Hub共享镜像，必须命名为/:的样式。 docker image pull docker image push docker image push sciatta/ti:1.0 上传镜像到仓库。 docker network docker node docker plugin docker secret docker service docker stack docker swarm docker system docker trust docker volume 命令 docker attach docker build docker commit docker cp docker create docker deploy docker diff docker events docker exec docker exec -it tc ps aux 在一个正在运行的容器内执行命令。 docker export docker history docker images docker import docker info docker inspect docker kill docker load docker login docker login 输入用户名和密码，登录到docker仓库。默认是Docker Hub。如果没有Docker ID（用户名），需要在https://hub.docker.com注册。 登录成功后显示Login Succeeded。 docker logout docker logs docker pause docker port docker ps docker pull docker push docker rename docker restart docker rm docker rmi docker run docker run -i -t sciatta/ti /bin/bash -i: 以交互模式运行容器 -t: 为容器重新分配一个伪输入终端 如果本机没有sciatta/ti镜像，从配置仓库pull镜像，同运行命令docker pull。 创建一个新的容器，同运行命令docker container create。 Docker为容器创建一个可读写的文件系统作为最后一层。允许一个正在运行的容器，创建和修改本机文件系统的文件、目录。 在没有指定网络选项时，Docker创建一个网络接口，连接容器到默认的网络，包括为容器分配一个IP地址。默认，容器可以使用主机的网络连接来连接到外部网络。 Docker启动容器，执行/bin/bash。因为容器以交互式运行，并且已经附加到终端 ，所以当日志输出到你的终端时，可以使用键盘输入。 当输入exit退出/bin/bash命令后，容器停止运行，但容器没有被删除。可以重新启动或者删除容器。 docker save docker search docker start docker stats docker stop docker tag docker top docker unpause docker update docker version docker wait Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/virtualization/docker/Dockerfile参考.html":{"url":"src/virtualization/docker/Dockerfile参考.html","title":"Dockerfile参考","keywords":"","body":"Dockerfile参考 概述 Docker从Dockerfile文件中读取指令，自动构建镜像。 docker build 从Dockerfile和上下文构建镜像。构建上下文是在指定位置 PATH 或 URL 下的文件集合。其中，PATH是本地文件系统的目录，URL是Git仓库的位置。上下文的处理是递归的。 docker build . 以当前目录作为上下文。 构建过程是运行在 Docker daemon，而并不是CLI。构建过程的第一件事是发送整个上下文（递归）到daemon。因此，最好是把Dockerfile放置在一个空目录作为上下文，仅增加对构建需要的文件。为了使用上下文下的文件，可以使用COPY指令。为了提高构建性能，可以在上下文目录增加一个.dockerignore文件来排除文件和目录。 注释 指令不是大小写敏感的，但按照惯例，把指令全部写成大写来与参数作区分。Docker按顺序运行Dockerfile内的指令。一个Dockerfile必须以FROM指令开始（可在解析器指令、注释、全局范围参数之后）。 Docker会将以＃开头的行视为注释，除非该行是有效的解析器指令。一行中其他任何地方的＃标记均被视为参数。注释中不支持换行符。 # Comment RUN echo 'we are running some # of cool things' 第一个 # 为注释；第二个 # 被视为参数，正常输出 we are running some # of cool things 。 解析器指令 解析器指令是可选的，影响Dockerfile后续行的执行。解析器指令不会增加层，也不会显示为一个构件步骤。解析器指令被写成一个特殊的注释# directive=value。处理完注释，空行或构件指令后，Docker不再寻找解析器指令。 而是将格式化为解析器指令的任何内容都视为注释，并且不会尝试验证它是否可能是解析器指令。 因此，所有解析器指令必须位于Dockerfile的最顶部。 解析器指令不是大小写敏感的，但按照惯例，全部写成小写。约定还应在任何解析器指令之后包含一个空白行。 解析器指令不支持换行符。而非换行符的空格字符是支持的。 syntax # syntax=[remote image reference] escape # escape=\\ (backslash) 转义指令设置Dockerfile中的转义字符，如果没有指定，默认是 \\ 。 .dockerignore 文件 docker CLI 发送上下文到 docker daemon之前，会首先在上下文的root目录查找.dockerignore文件。如果存在，CLI会修改上下文，排除同模式匹配的文件和目录。这样可以避免发送不必要的大或敏感文件和目录到daemon。如果需要这些文件或目录的话，可以使用 ADD 或 COPY 指令。 CLI将.dockerignore文件解释为以换行符分隔的模式列表，类似于Unix shell的文件组。为了匹配，上下文的根被认为是工作目录和根目录。例如，模式 /foo/bar 和 foo/bar 都在PATH或位于URL的git仓库根目录的foo子目录中排除名为bar的文件或目录。 如果.dockerignore文件中第一行以 # 开始 ，则被看做是注释，在CLI解释之前被忽略。 # 注释 # comment注释 * 匹配任意字符 /temp匹配在root的直接子目录中，以temp为前缀的文件或目录 //temp* 匹配在root的二级子目录中，以temp为前缀的文件或目录 ** 匹配任意数量的目录，包括0个 */.go排除所有目录以.go结尾的文件，包括root目录 ! 排除例外 !README.md包含文件README.md ? 匹配一个字符 temp?匹配在root中以一个字符扩展temp的所有文件或目录 FROM FROM [AS ] FROM [:] [AS ] FROM [@] [AS ] 一个有效的Dockerfile文件必须包含一个 FROM 指令。 ARG 是唯一一个可以先于FROM的指令。 FROM可以在单个Dockerfile中多次出现，以创建多个映像或将一个构建阶段用作对另一个构建阶段的依赖。 只需在每个新的FROM指令之前记录一次提交输出的最后一个镜像ID。 每个FROM指令清除由先前指令创建的任何状态。 可以增加 AS 到 FROM 指令，为一个新的构建阶段提供一个可选的名称。名称可以用于后续 FROM 和 COPY --from= 指令，来在这个阶段引用镜像。 tag 和 digest 值是可选的。如果省略，构建器默认latest tag。如果没有找到tag，则返回一个错误。 ARG VERSION=latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION VERSION定义先于FROM指令，因此在FROM指令之后，ARG不可以用于其他指令。在构建阶段如果需要使用ARG的默认值，则使用ARG指令不赋予值即可。 RUN RUN 以shell方式运行，默认 /bin/sh -c； 如：RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME' RUN [\"executable\", \"param1\", \"param2\"] 以exec方式。注意双引号，JSON格式; 如：RUN [\"/bin/bash\", \"-c\", \"echo hello\"] 在当前镜像的顶层创建一个新层来执行命令，然后提交结果。 不像shell方式，以exec方式方式并不会调用命令shell。也就是说shell处理将不会发生。例如 RUN [ \"echo\", \"$HOME\" ]在 $HOME上将不会进行变量替换。如果需要进行shell处理，则以shell形式，或直接执行shell，如：RUN [ \"sh\", \"-c\", \"echo $HOME\" ]。会进行环境变量扩展，而不是docker。 CMD CMD [\"executable\",\"param1\",\"param2\"] exec方式 CMD [\"param1\",\"param2\"] ENTRYPOINT的默认参数 CMD command param1 param2 shell方式 在Dockerfile中仅可以有一条 CMD 指令。如果有多条，仅有最后一条有效。 主要目的为正在执行容器提供默认执行方式。当运行镜像时，CMD指令被执行。 运行 docker run 指定参数，将覆盖 CMD 中的默认相同参数。 在构建阶段，RUN 执行一个命令，然后提交结果；而 CMD 并不在构建阶段执行，针对镜像的预期指定命令，即在容器执行时执行。 LABEL LABEL = = = ... 为镜像增加元数据。一个LABEL是一个 key-value 对。为了在LABEL的value中包括空字符，需要使用引号和反斜杠。 一个镜像可以有多个LABEL；可以指定多个LABEL在一行。 LABEL可以包括在父镜像中，被用于继承。如果LABEL重复，后者覆盖前者。使用 docker inspect 查看LABEL。 LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" MAINTAINER (deprecated) MAINTAINER 生成镜像的作者。LABEL比MAINTAINER更灵活，建议使用LABEL替换。 例如 LABEL maintainer=\"SvenDowideit@home.org.au\" EXPOSE EXPOSE [/...] 在运行时，容器监听的指定网络端口。可以指定端口监听 TCP 或是 UDP，如果协议没有指定，则TCP是默认协议。 EXPOSE 指令实际并没有发布端口，它的主要功能是在构建镜像者和运行容器者之间作为一种规约，也就是预期要发布的端口。当运行容器时，实际要发布端口的话，使用 -p 标志发布并映射一个或多个端口，或 -P 发布所有 EXPOSE 的端口并映射到高阶端口。 EXPOSE 80/udp 默认TCP协议，可以指定协议为UDP。 不管 EXPOSE 如何设置，都可以在运行时使用 -p 标志覆盖。 ENV ENV ENV = ... 在构建阶段，环境变量可以像使用变量一样用于后续的指令中，被Dockerfile所解析。环境变量可以在Dockerfile中以$variable_name 或 ${variable_name} 方式来引用。 如果在变量前面加上转义字符 \\ ，则按字面字符解释。如 $foo 或者 ${foo}，被分别解释为 $foo 和 ${foo} 。 环境变量在运行时持续存在。可以使用docker inspect 查看。 可以在一个command中设置一个环境变量，使用 RUN = 。 ADD ADD [--chown=:] ... ADD [--chown=:] [\"\",... \"\"] 1、路径包括空字符 2、--chown 只支持UNIX系统 从复制文件，目录或者远程 URL，到镜像文件系统。 可以指定多个资源，如果它们是文件或目录，则将其路径解释为相对于构建上下文的路径。 ADD hom* /mydir/ ADD hom?.txt /mydir/ 可以是一个绝对路径，也可以是一个相对 WORKDIR 的相对路径。 ADD test relativeDir/ # adds \"test\" to `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # adds \"test\" to /absoluteDir/ 路径必须在构建上下文内部；不可使用../something /something，因为docker构建的第一步（FROM）已经把上下文目录发送到docker daemon。 如果是一个URL，并且没有以斜杠结束，则文件从URL下载并复制到。 如果是一个URL，并且以斜杠结束，则文件名按照URL进行推导，文件下载为/。如 ADD http://example.com/foobar / 将创建文件 /foobar 。 如果是一个目录，则目录的所有内容被复制，包括文件系统的元数据。注意，目录本身不会被复制，仅是内容。 如果是一个本地tar压缩格式，则被解压缩为一个目录。来自远程URL的资源不被解压缩。 如果多个资源被指定，或者是目录，或者是通配符形式，则必须是一个目录，必须以 反斜杠结尾。 如果不存在，路径所有缺失的目录被创建。 COPY COPY [--chown=:] ... COPY [--chown=:] [\"\",... \"\"] 从复制文件，目录，到容器文件系统。 ENTRYPOINT ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec form, preferred) ENTRYPOINT command param1 param2 (shell form) 配置容器，作为可执行方式运行。 以exec方式运行，命令行参数会追加到 docker run ENTRYPOINT之后，并覆盖CMD指定的元素。可以搭配 CMD 命令使用，一般是变参会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参。可以使用ENTRYPOINT的exec方式，作为稳定默认命令和参数。然后使用两种形式的CMD设置更可能更改的其他默认值。 FROM nginx ENTRYPOINT [\"nginx\", \"-c\"] # 定参 CMD [\"/etc/nginx/nginx.conf\"] # 变参 docker run nginx:test 不传参运行，启动主进程 nginx -c /etc/nginx/nginx.conf docker run nginx:test -c /etc/nginx/new.conf 传参运行，容器内会默认运行 nginx -c /etc/nginx/new.conf 允许向ENTRYPOINT传参。如 docker run -d ，将 -d 传入到ENTRYPOINT。可以使用 docker run --entrypoint 覆盖 ENTRYPOINT指令。 以Shell方式运行，可防止使用任何 CMD 或 run 命令行参数，但缺点是ENTRYPOINT将作为/bin/sh -c 的子命令启动，该子命令不传递信号。这意味着executable将不是容器的PID 1，并且不会接收Unix信号，因此executable将不会从 docker stop 接收到信号。 多个ENTRYPOINT指令，只有最后一个有效。 ENTRYPOINT和CMD交互 No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUME VOLUME [\"/var/log/\"] VOLUME /var/log or VOLUME /var/log /var/db 创建一个指定名称的挂载点，可以将主机或其他容器的目录挂载到容器中。当容器删除时，数据被持久化而不会丢失。 主机目录是在容器运行时声明的：主机目录（挂载点）从本质上说是依赖于主机的。 这是为了保证镜像的可移植性，因为不能保证给定的主机目录在所有主机上都可用。 因此，您无法从Dockerfile中挂载主机目录。VOLUME 指令不支持指定host-dir参数。 当创建或运行容器时，必须指定挂载点。 USER USER [:] USER [:] USER指令设置运行镜像时要使用的用户名（或UID）以及可选的用户组（或GID），以及Dockerfile中跟随该镜像的所有RUN，CMD和ENTRYPOINT指令。 如果用户没有组，则该镜像（或接下来指令）将用root组运行。 WORKDIR WORKDIR /path/to/workdir WORKDIR指令为Dockerfile中跟在其后的所有RUN，CMD，ENTRYPOINT，COPY和ADD指令设置工作目录。 如果WORKDIR不存在，即使后续的Dockerfile指令中未使用，WORKDIR也将被创建。 WORKDIR指令可在Dockerfile中多次使用。 如果提供了相对路径，则它将相对于上一个WORKDIR指令的路径。 WORKDIR /a WORKDIR b WORKDIR c RUN pwd 最后输出的WORKDIR是 /a/b/c 。 ARG ARG [=] ARG指令定义的变量，用户可以在构建阶段，通过 docker build 指令使用 --build-arg = 覆盖已定义的变量。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/os/linux/Linux命令.html":{"url":"src/os/linux/Linux命令.html","title":"Linux命令","keywords":"","body":"文件管理 cat # 1、将file1内容和file2内容输出到file3 # 2、“命令 > 文件”将标准输出重定向到一个文件中（清空原有文件的数据） # 3、“命令 >> 文件”将标准输出重定向到一个文件中（追加到原有内容的后面） cat file1 file2 > file3 # 由1开始对所有输出的行编号，包括空行 cat -n file1 file2 > file3 # 由1开始对所有输出的行编号，不包括空行 cat -b file1 file2 > file3 # 拷贝标准输入到标准输出 cat # 拷贝标准输入到file cat > file # 拷贝file1到标准输出 cat file1 # 拷贝file1和file2到标准输出 cat file1 file2 # 1、清空file1的内容 # 2、dev/null：在类Unix系统中，/dev/null称空设备，它丢弃一切写入其中的数据（但报告写入操作成功）， # 读取它则会立即得到一个EOF cat /dev/null > file1 # 不会得到任何信息，因为将本来该通过标准输出显示的文件信息重定向到了/dev/null中 cat file1 > /dev/null chown # 修改文件的用户及用户组，需要root权限 chown hadoop:test file # -R 递归设置当前目录和其下子目录、文件 chown -R hadoop:hadoop dir chgrp # 修改文件或目录所属的用户组 # -v 显示指令执行过程 chgrp -v hadoop file chmod Linux/Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他 设置权限 字串设定 [ugoa...][[+-=][rwxX]...][,...] u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群组，o 表示其他以外的人，a 表示这三者皆是 +表示增加权限、- 表示取消权限、= 表示唯一设定权限 r 表示可读取，w 表示可写入，x 表示可执行 数字设定 chmod abc file a，b，c各为一个数字，分别表示User，Group，及Other的权限 r=4，w=2，x=1 # 所有人增加写权限 chmod a+w file # 文件拥有者和群组增加写权限，其他减少写权限 chmod ug+w,o-w file # -R 递归设置 chmod -R u+rwx,go-rwx dir # 所有人增加读写执行权限 chmod 777 file find # 查找当前目录和子目录下所有匹配样式file*的文件或目录，输出路径 find . -name \"file*\" # 查找当前目录名称为.lastUpdated后缀的所有文件或目录，然后删除 # xargs传递上一个指令的输出作为参数，一般和 | 联用，使得一些不支持 | 管道符的命令（rm）也可以接受到上一个命令的输出作为参数 find . -name \"*.lastUpdated\" | xargs rm -rfv cut # 截取每一行的第二个字节 cut -b 2 file # 截取每一行的第二个字符，汉子可以显示正确 cut -c 2 file # 截取每一行的第二~三个字符 cut -c 2-3 file # 默认分隔符tab，若没有分隔符，则输出整行 # -f 指定输出区域，默认从1开始 cut -f 2 file # -s 没有分割符不打印 cut -s -f 2 file # -d 指定分隔符为空格 cut -d ' ' -f 2 file ln 建立链接（link），不必重复的占用磁盘空间 软链接 以路径的形式存在。类似于Windows操作系统中的快捷方式 可以跨文件系统，硬链接不可以 可以对一个不存在的文件名进行链接 可以对目录进行链接 硬链接 以文件副本的形式存在。但不占用实际空间 不允许给目录创建硬链接 硬链接只有在同一个文件系统中才能创建 # -s 建立软链接 ln -s file linkf less less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动 less 在查看之前不会加载整个文件 # ps查看进程信息并通过less分页显示 ps -ef | less # 查看命令历史使用记录并通过less分页显示 history | less # 1、浏览多个文件 # 2、输入 # :n 切换到file2 # :p 切换到file1 less file1 file2 # ****************************************************** # 移动到最后一行 G # 移动到第一行 g # 向下搜索“字符串” /字符串 # 向上搜索“字符串”的功能 ?字符串 # 重复前一个搜索（与 / 或 ? 有关） n # 反向重复前一个搜索（与 / 或 ? 有关） N # 默认向下移动一行，输入数字回车则向下移动j行 （j）Enter # 向上移动一行 k # 向下翻一页 空格键（d） # 向上翻一页 b # 退出less命令 q（ZZ） # ****************************************************** more # 如有连续两行以上空白行则以一行空白行显示 more -s file # 从第5行开始显示 more +5 file # ****************************************************** # 向下翻一页 空格键 # 向上翻一页 b # ****************************************************** mv # 移动并改名 mv file1 file # ok和no都是目录。当no不存在时，则将ok改名为no；当no存在时，将ok目录转移到no目录 mv ok/ no # ok和no都是目录。当no不存在时，失败；当no存在时，将ok目录所有内容转移到no目录 mv ok/* no rm # rm: remove regular file ‘services’? n # 删除文件需要询问 rm services # -r 递归删除目录 rm -r test # -f 强制删除所有文件和目录（即使只读），不需要询问 rm -rf test touch # -rw-r--r--. 1 root root 670293 Dec 31 17:10 services # -rw-r--r--. 1 root root 670293 Dec 31 21:07 services # 文件存在，更新修改时间为当前时间 touch services # 文件不存在，创建空文件 touch file1 # 创建一个名称为 --name 的文件(同 -name) # 错误：touch --name 无法识别的选项；-和--后面接option # 修正：touch -- --name -- 后面不会再包含option，被认为是 filenames 和 arguments touch -- --name # 错误：cat --name 无法识别的选项 cat -- --name which # which指令会在环境变量$PATH设置的目录里查找符合条件的文件 which bash # 打印所有匹配的路径（不仅只有第一个） which -a bash cp # -r 递归复制test目录下的内容到test1 cp -r test test1 scp scp 是 secure copy 的缩写，scp 是 linux 系统下基于 ssh 登陆进行安全的远程文件拷贝命令 scp 是加密的，rcp 是不加密的，scp 是 rcp 的加强版 # 拷贝 本地文件 -> 远程目录 scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.100:/bigdata/soft # 拷贝 本地目录 -> 远程目录 # -r 递归拷贝目录 scp -r hadoop-2.6.0-cdh5.14.2/ node02:$PWD # 拷贝 远程文件 -> 本地目录 scp hadoop@192.168.2.100:/bigdata/soft/jdk-8u141-linux-x64.tar.gz /bigdata/soft awk 格式化文本 AWK 取了三位创始人 Alfred Aho，Peter Weinberger 和 Brian Kernighan 的 Family Name 的首字符 BEGIN{ } 行处理前执行语句 {} 每行处理执行的语句 END{} 行处理后执行语句 # ****************************************************** # 测试数据 log.txt # ****************************************************** 2 this is a test 3 Are you like awk This's a test 10 There are orange,apple,mongo # ****************************************************** # 行匹配语句 awk '' 只能用单引号 # 每行按空格或TAB分割，输出文本中的1、4项 awk '{print $1,$4}' log.txt # 格式化输出 awk '{printf \"%-8s %-10s\\n\",$1,$4}' log.txt # -F 指定分隔符 awk -F, '{print $1,$2}' log.txt # 使用多个分隔符。先使用空格分割，然后对分割结果再使用,分割 awk -F '[ ,]' '{print $1,$2,$5}' log.txt # -v设置变量 # 数字值为+1，字符串值为1 awk -va=1 '{print $1,$1+a}' log.txt # 设置两个变量 awk -va=1 -vb=s '{print $1,$1+a,$1b}' log.txt # 过滤第一列大于2的行 awk '$1>2' log.txt # 过滤第一列等于2的行 awk '$1==2 {print $1,$3}' log.txt # 过滤第一列大于2并且第二列等于'Are'的行 awk '$1>2 && $2==\"Are\" {print $1,$2,$3}' log.txt # 内建变量 # $n 当前记录的第n个字段，字段间由FS分隔 # $0 完整的输入记录 # ARGC 命令行参数的数目 # ARGIND 命令行中当前文件的位置(从0开始算) # ARGV 包含命令行参数的数组 # CONVFMT 数字转换格式(默认值为%.6g)ENVIRON环境变量关联数组 # ERRNO 最后一个系统错误的描述 # FIELDWIDTHS 字段宽度列表(用空格键分隔) # FILENAME 当前文件名 # FNR 各文件分别计数的行号 # FS 字段分隔符(默认是任何空格)（Field Separator） # IGNORECASE 如果为真，则进行忽略大小写的匹配 # NF 一条记录的字段的数目（Number for Field） # NR 已经读出的记录数，就是行号，从1开始（Number of Record） # OFMT 数字的输出格式(默认值是%.6g) # OFS 输出记录分隔符（输出换行符），输出时用指定的符号代替换行符（Out of Field Separator） # ORS 输出记录分隔符(默认值是一个换行符)（Output Record Separate） # RLENGTH 由match函数所匹配的字符串的长度 # RS 记录分隔符(默认是一个换行符)（Record Separator） # RSTART 由match函数所匹配的字符串的第一个位置 # SUBSEP 数组下标分隔符(默认值是/034) awk 'BEGIN{printf \"%4s %4s %4s %4s %4s %4s %4s %4s %4s\\n\",\"FILENAME\",\"ARGC\",\"FNR\",\"FS\",\"NF\",\"NR\",\"OFS\",\"ORS\",\"RS\";printf \"---------------------------------------------\\n\"}{printf \"%4s %4s %4s %4s %4s %4s %4s %4s %4s\\n\",FILENAME,ARGC,FNR,FS,NF,NR,OFS,ORS,RS}' log.txt # FS awk 'BEGIN{FS=\",\"} {print $1,$2}' log.txt # 指定分隔符 awk -F\\' 'BEGIN{printf \"%4s %4s %4s %4s %4s %4s %4s %4s %4s\\n\",\"FILENAME\",\"ARGC\",\"FNR\",\"FS\",\"NF\",\"NR\",\"OFS\",\"ORS\",\"RS\";printf \"---------------------------------------------\\n\"} {printf \"%4s %4s %4s %4s %4s %4s %4s %4s %4s\\n\",FILENAME,ARGC,FNR,FS,NF,NR,OFS,ORS,RS}' log.txt # 指定输出分隔符 awk '{print $1,$2,$5}' OFS=\" $ \" log.txt # 使用正则表达式 # ~ 表示模式开始。// 中是模式。 awk '$2 ~ /th/ {print $2,$4}' log.txt # 忽略大小写 awk 'BEGIN{IGNORECASE=1} /this/' log.txt # 模式取反 awk '$2 !~ /th/ {print $2,$4}' log.txt tail # -n 查看文件内容，显示后n行内容；默认后10行 tail -n 10 start-dfs.sh # -F 监控文件末尾改变情况，实时输出 tail -F start-dfs.sh -n 2 # 显示从第100行到末尾 tail -n +100 start-dfs.sh dirname # 获取当前路径的父路径 # 只有文件名 . dirname a # /a dirname /a/b basename # 获取当前路径的文件名 # b /a/b 文档编辑 grep 查找或匹配文本 # 在当前目录文件名为file前缀的文件中，查找包含hadoop文本的文件，并输出行内容 grep hadoop file* # -r 递归方式查找当前目录和其子目录 # -n 打印行号 grep -rn hadoop . # 通过上一个命令的结果作为输入进行查找 ls -al | grep file sed 编辑匹配到的文本 # -e 执行脚本命令 # a append，在第三行下面追加\\后面的内容，追加后打印到控制台，不影响原来的文本 sed -e 3a\\hive file1 # i insert，在第二前面插入内容 sed -e 2i\\hive file1 # d delete nl /etc/passwd | sed '2,5d' # c replace nl /etc/passwd | sed '2,5c No 2-5 number' # p print 配合-n使用，仅显示script处理后的结果 nl /etc/passwd | sed -n '5,7p' # 搜索显示 nl /etc/passwd | sed -n '/root/p' # 搜寻并替换 # s/要被取代的字串/新的字串/g # s表示正则表达式替换，g表示每一行匹配全部替换，不加的话表示每一行替换第一个匹配 ifconfig ens33 | grep 'inet ' | sed 's/^.*inet //g' # -i 直接修改源文件 sed -i 's/.*doop$/hive/g' file1 # $表示最后一行，a表示追加 sed -i '$a zookeeper' file1 sort # 以默认的方式将文本文件的第一列以ASCII码的次序排列，并将结果输出到标准输出 sort file1 uniq # -c 统计重复行，但必须相邻，所以先用sort将第一列排序 sort file3 | uniq -c # -d 仅显示重复出现的行，即次数大于1 sort file3 | uniq -cd wc # 统计 行数、单词数、字节数 wc file1 file2 file3 let # 执行表达式，变量不需要加$ let a=5+4 vi/vim vi 是老式的字处理器。vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计 # ****************************************************** # 【命令模式】 # ****************************************************** # 切换到【输入模式】 i # 切换到【底线命令模式】 : # 向后删除光标所在字符 # shift+x 向前删除 # nx 连续删除 x # 删除游标所在行 dd # 复制游标所在行 # nyy 向下复制n行 yy # 复制在游标之后 p # 撤销前一个动作 u # 重做前一个动作 control+r # 移动 # 光标向左移动一个字符 h 或 向左箭头键(←) # 光标向下移动一个字符 # 向下移动30行，可以使用 \"30j\" 或 \"30↓\" 的组合按键 # n 向下移动n个字符 j 或 向下箭头键(↓) # 光标向上移动一个字符 k 或 向上箭头键(↑) # 光标向右移动一个字符 # n 向右移动n个字符 l 或 向右箭头键(→) # 屏幕『向下』移动一页，相当于 [Page Down] [Ctrl] + [f] # 屏幕『向上』移动一页，相当于 [Page Up] [Ctrl] + [b] # 移动到行首，相当于 [Home] 0 # 移动到行尾，相当于[ End ] $ # 移动到当前屏幕上方的第一个字符 shift+h # 移动到当前屏幕中央的第一个字符 shift+m # 移动到当前屏幕下方的第一个字符 shift+l # 移动到文件最后一行的第一个字符 # n+shift+g 移动到文件的第n行 shift+g # 移动到文件第一行的第一个字符，相当于 1+shift+g gg # 搜索 # 从光标向下搜索 /word # 从光标向上搜索 ?word # 重复上一次搜索 n # 反向重复上一次搜索 shift+n # 替换 # n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 :n1,n2s/word1/word2/g # 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 # :%s/word1/word2/g 等价 :1,$s/word1/word2/g # 替换前确认 # :%s/word1/word2/gc :1,$s/word1/word2/gc # 在 10 - 20 行添加 // 注释 # 因为要换中有/，所以分隔符用#代替 :10,20s#^#//#g # 在 10 - 20 行删除 // 注释 :10,20s#^//##g # 查找关键字出现次数 :%s/word//gn # ****************************************************** # 【输入模式】 # ****************************************************** # 换行 enter # 退格键，删除光标前一个字符 back space / delete # 切换到【命令模式】 esc # ****************************************************** # 【底线命令模式】 # ****************************************************** # 退出程序 # q! 不保存退出 q # 保存文件 w # 切换到【命令模式】 esc # 保存退出 wq # 设置行号 # set nonu 取消行号 set nu # 取消高亮显示 # set hls 高亮显示 set nohls 文件传输 ftp # 该服务器是Linux内核的官方服务器 ftp ftp.kernel.org bye # 中断与ftp服务器连接并退出程序 bye 磁盘管理 cd # 切换当前工作目录为目标目录 cd a/b # 若目录名称省略，则变换至使用者的 home 目录 cd df disk free # 显示文件系统的磁盘使用情况统计，显示文件系统、挂载点等信息 # -T 显示文件系统类型 df -Th du disk usage # 只显示当前目录和子目录的大小 du -h # 显示当前目录的文件大小，子目录，以及递归子目录的大小（子目录下文件不显示大小） du -h * # 仅显示当前目录的子目录和文件的总计大小 du -sh * mkdir # -p 如果父目录不存在，则创建 mkdir -p a/b pwd print work directory # 输出当前工作路径 pwd mount # 挂载 # 文件系统（分区） -> 目录（挂载点） mount /dev/hda1 /mnt # 只读模式挂载 mount -o ro /dev/hda1 /mnt # 挂载ios文件 mount -o loop /tmp/image.iso /mnt/cdrom umount # 通过文件系统卸载 # -v 执行时显示详细的信息 umount -v /dev/sda1 # 通过挂载点卸载 umount -v /mnt/mymount/ rmdir # 删除目录 rmdir dir # 删除目录dir2，如果此时dir1为空目录，则一并删除 rmdir -p dir1/dir2 stat # 显示inode的内容 stat log.txt ls # 默认当前目录，显示所有文件和目录，包括隐藏档 ls -al iostat # -m 每秒兆字节 # -x 显示扩展信息 # 2 每个2秒刷新一次 iostat -mx 2 # 安装iostat sudo yum install -y sysstat 磁盘维护 lsblk # 列出所有可用块设备的信息，并显示他们之间的依赖关系 # -p 打印完整设备路径 lsblk -p # NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT # /dev/sda 8:0 0 40G 0 disk # ├─/dev/sda1 8:1 0 1G 0 part /boot # └─/dev/sda2 8:2 0 39G 0 part # ├─/dev/mapper/centos-root 253:0 0 37G 0 lvm / # └─/dev/mapper/centos-swap 253:1 0 2G 0 lvm [SWAP] # /dev/sr0 11:0 1 1024M 0 rom parted # 显示分区表 # MBR(msdos) -> fdisk 分区 # 支持4个主分区 或 3个主分区+1个扩展分区（逻辑分区无限制） # 不支持大于2T的分区 # GPT -> gdisk 分区 无限制 parted /dev/sda print fdisk Partition table manipulator for Linux Linux fdisk是一个创建和维护分区表的程序，它兼容DOS类型的分区表、BSD或者SUN类型的磁盘列表 容量 硬盘总容量 = 主分区 + 扩展分区 扩展分区容量 = 逻辑分区总容量 # 列出所有磁盘分区 fdisk -l # 对磁盘分区 # m 显示菜单和帮助信息 # n 增加一个新分区 # p 主分区（e 扩展分区）输入分区数（1） # w 保存结果 fdisk /dev/sda gdisk # 查看磁盘信息 gdisk -l /dev/sda # 对磁盘分区 # ? 显示菜单和帮助信息 # n 增加一个新分区 # p 查看分区表 # w 保存结果 gdisk /dev/sda # ****************************************************** # LVM Logical Volume Manager # ****************************************************** # partition -> pv -> vg -pe-> lv -> mkfs -> mount # ****************************************************** # 分区 # gdisk -l /dev/vda 建立文件系统（code 8E00） # pv阶段 # 1）pvscan 查看有无pv # 2）pvcreate /dev/vda{5,6,7,8} 建立pv # 3）pvdisplay /dev/vda5 显示更详细pv信息 # vg阶段 # 1）vgcreate -s 16M vbirdvg /dev/vda{5,6,7} -s指定pe大小 # 2）vgscan 查看vg # 3）pvscan 查看pv # 4）vgdisplay vbirdvg 显示更详细vg信息 # ---- # vgextend vbirdvg /dev/vda8 可以给现有vg扩充pv # vgreduce 在 VG 內移除 PV # vgremove 刪除一個 VG # ---- # lv阶段 # 1）lvcreate -L 2G -n vbirdlv vbirdvg -L容量 -n名称 # 2）lvscan 查看lv # 3）lvdisplay /dev/vbirdvg/vbirdlv 显示更详细lv信息 # ---- # lvextend 在 LV 裡面增加容量 # lvextend /dev/vbirdvg/vbirdlv /dev/vda8（vda8必须属于vbirdvg，这个pv全部给vbirdlv） # lvreduce 在 LV 裡面減少容量 # lvremove 刪除一個 LV # lvresize 對 LV 進行容量大小的調整 # ---- # 格式化 # 1）mkfs.xfs /dev/vbirdvg/vbirdlv # 2）mkdir /srv/lvm 目录 # 3）mount /dev/vbirdvg/vbirdlv /srv/lvm 文件系统挂载到目录 # 4）df -Th /srv/lvm 检查 # ---- # xfs_info /srv/lvm 查看文件系统信息 # xfs_growfs /srv/lvm lv增大后，要对 目录/文件系统 操作 xfs_growfs ，空间才会相应变化 # ---- partprobe # 查看磁盘分区是否有变化 partprobe /dev/sda # 不重启使分区生效 partprobe -s mkfs # 分区格式化为xfs # 等价于 mkfs -t xfs mkfs.xfs /dev/sda1 网络通讯 telnet # 登录IP为 192.168.0.5 的远程主机 telnet 192.168.0.5 ifconfig # 显示网络设备信息 ifconfig # 给eth0网卡配置IP地址，子网掩码，广播地址 ifconfig eth0 192.168.1.56 netmask 255.255.255.0 broadcast 192.168.1.255 netstat # -a 列出所有的连接状态，包括 tcp/udp/unix socket 等 # -u udp only # -t tcp only # -l 仅列出有在 Listen 的服务网络状态 # -n 不使用主机名称和服务名称，使用 IP 和 port number # -P 列出pid netstat -tulnp # 安装netstat yum install -y net-tools ping # 检测是否与主机连通 # -c 指定接收包的次数 ping -c 2 www.baidu.com ssh # 1、登录远程服务器，接着输入密码 # 2、iTerm远程连接centos服务器，显示命令帮助为中文，而centos服务器内部可以显示英文 # centos语言为US，iTerm语言为CN，因此需要修改iTerm： # 1）vim ~/.zshrc # 2）末尾添加 # export LC_ALL=en_US.UTF-8 # export LANG=en_US.UTF-8 # 3）source ~/.zshrc ssh root@172.16.92.132 系统管理 useradd /etc/passwd 账号信息 # 添加用户并指定组 useradd -g hadoop test usermod # 修改用户所属组 usermod -g test test userdel # -r 删除用户登入目录以及目录中所有文件 # 如组中只有一个用户，则删除此用户时，组一并删除 userdel -r test groupadd /etc/group 组账户信息 # 添加组 groupadd test groupmod # 修改组信息 groupmod -n test1 test groupdel # 删除组 # 若组中存在用户，则不允许删除 groupdel test1 date # 显示当前日期时间 date # + 设定格式标记 # %c 直接显示日期与时间 date '+%c' # 指定日期加100天 date -d\"20200801 +100 days\" +\"%Y%m%d\" exit # 退出shell exit sleep # number 时间长度，后面可接 s、m、h 或 d，其中 s 为秒，m 为 分钟，h 为小时，d 为日数 date;sleep 30s;date kill # 显示信号 kill -l # 彻底杀死进程 kill -9 pid # 正常停止一个进程 kill -15 pid ps # -e 显示所有进程 # -f 全格式列表 # 显示所有命令，连带命令行 ps -ef # -u 指定用户的进程 ps -u hadoop ps -u hadoop -f nice # 降低优先级，设置优先级为19 # 范围是[-20, 19]，默认值是10。nice值越大优先级越低，优先级高的会抢占优先级低的进程的时间片 nice -n 19 top # 实时显示进程信息 top # 输入h显示帮助信息 h # 显示多核CPU使用率 1 shutdown # 立即关机 shutdown -h now # 重新启动 shutdown -r now sudo # 以系统管理者的身份执行指令 sudo ls # 以指定用户身份执行指令 sudo -u hadoop ls -l uname # 显示系统信息 uname -a # 显示计算机名 uname -n who # 显示当前登录系统的用户 # -H 显示标题栏 who -H whoami # 显示用户名 whoami su # 切换到root用户 su root # 切换用户，改变环境变量 su - root cpuinfo # 物理CPU个数 cat /proc/cpuinfo | grep \"physical id\" | sort | uniq | wc -l # 每个物理cpu中core的个数(即核数) cat /proc/cpuinfo | grep \"cpu cores\" | uniq free # 显示内存使用情况 free -h locale # 显示目前所支持的语系 locale jobs # & 在后台运行 vi start-dfs.sh & # 查看当前bash中有哪些工作 jobs -l # 从后台调到前台运行 fg %1 # 从前台暂停到后台运行 control+z # 结束工作任务 control+c 系统设置 clear # 清屏 clear crontab # ****************************************************** # * * * * * # - - - - - # | | | | | # | | | | +----- 星期中星期几 (0 - 7) (星期天 为0) # | | | +---------- 月份 (1 - 12) # | | +--------------- 一个月中的第几天 (1 - 31) # | +-------------------- 小时 (0 - 23) # +------------------------- 分钟 (0 - 59) # ****************************************************** # * 表示每分钟都要执行，以此类推 # a-b 表示从第 a 分钟到第 b 分钟这段时间内要执行，以此类推 # */n 表示每 n 分钟时间间隔执行一次，以此类推 # a, b, c,... 表示第 a, b, c,... 分钟要执行，以此类推 # ****************************************************** # 在12月内，每天的早上6点到12点，每隔3小时0分钟执行一次/usr/bin/backup 0 6-12/3 * 12 * /usr/bin/backup # 每月每天的午夜0点20分开始，每2小时，直到23点20分为止执行echo \"haha\" 20 0-23/2 * * * echo \"haha\" declare # 声明变量，同时赋值 declare -i a=1 # 声明只读变量 declare -r a=1 # 声明环境变量 declare -x a=1 export # 列出当前的环境变量值 export -p # 定义环境变量并赋值 # 仅对当前登录有效 export MYENV=7 rpm redhat package manager # 安装软件 # -h 套件安装时列出标记 # -v 显示指令执行过程 # -i 安装指定的套件档 rpm -hvi dejagnu-1.4.2-10.noarch.rpm # 显示软件安装信息 # -q 使用询问模式，当遇到任何问题时，rpm指令会先询问用户 rpm -qi dejagnu-1.4.2-10.noarch.rpm yum # 线上安装 yum install -y vim set # 显示环境变量 set env # 显示环境变量 env passwd # 修改用户密码 # 当提示 ’无效的密码： 密码少于 8 个字符‘ 无需管，重新输入密码即可 passwd test 备份压缩 zip # -q 不显示指令执行过程 # -r 递归处理，将指定目录下的所有文件和子目录一并处理 zip -qr test.zip * # -d 从压缩文件内删除指定的文件 zip -d test.zip log.txt unzip # 显示压缩包中的文件 unzip -l test.zip # 解压缩到指定目录 unzip test.zip -d mytest zipinfo # 显示压缩文件信息 zipinfo test.zip # 显示压缩文件中每个文件的信息 zipinfo -v test.zip tar # 压缩文件 # -c create # -z 通过gzip指令处理备份文件 # -v 显示指令执行过程 # -f 指定备份文件 tar -czvf test.tar.gz file* # 列出压缩文件内容 # -t 列出备份文件的内容 tar -tzvf test.tar.gz # 解压文件 # -x 从备份文件中还原文件 # -C 解压到指定目的目录 mkdir t1 & tar -xzvf test.tar.gz -C t1 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/os/linux/iTerm的使用.html":{"url":"src/os/linux/iTerm的使用.html","title":"iTerm的使用","keywords":"","body":"iTerm的使用 快捷键 水平分隔当前屏幕 shift+command+D 垂直分隔当前屏幕 command+D 向所有Tab的所有窗口广播命令 shift+command+i 向当前Tab的所有窗口广播命令 option+command+i 将窗口分组广播命令 所有窗口分组 option+command+i 单独一个窗口分组 shift+control+option+command+i 恢复窗口分组 shift+option+command+i Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/ide/intellijidea/IDEA配置开发环境.html":{"url":"src/ide/intellijidea/IDEA配置开发环境.html","title":"IDEA配置开发环境","keywords":"","body":"Java环境 配置JDK环境变量 vi ~/.zshrc export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_231.jdk/Contents/Home export PATH=$JAVA_HOME/bin:$PATH # 即时生效 source ~/.zshrc 配置JDK集成开发环境 项目右键 | open module settings | sdks | + jdk 指定 jdk home path 项目右键 | open module settings | project 指定sdk 项目右键 | open module settings | modules | language level: 8 Preferences | Build, Execution, Deployment | compiler | java compiler module 设置为8 project bytecode version 设置为8 Preferences | Build, Execution, Deployment | Debugger | Stepping do not step into the classes 去掉 java.* 和 javax.*，否则无法进入JDK源码进行调试 Scala环境 配置SDK环境变量 tar -zxvf scala-2.11.8.tgz -C ../install vi ~/.zshrc export SCALA_HOME=/Users/yangxiaoyu/work/install/scala-2.11.8 export PATH=$SCALA_HOME/bin:$PATH # 即时生效 source ~/.zshrc 配置SDK集成开发环境 下载 Scala 插件 Preferences | plugins | Scala 新建 pom module 新建 jar module 指定SDK File | Project Structure | Libraries | + | Scala SDK | 选择 scala-2.11.8 项目右键 | open module settings | modules | hadoop-scala-example | Dependencies | + | Libraries | 选择 scala-sdk-2.11.8 Java混编Scala src/main 新建 Directory | scala Mark Directory as | Sources Root src/test 新建 Directory | scala Mark Directory as | Test Sources Root 配置maven插件 org.apache.maven.plugins maven-compiler-plugin compile compile 1.8 1.8 UTF-8 net.alchim31.maven scala-maven-plugin scala-compile-first process-resources add-source compile scala-test-compile process-test-resources testCompile 打包运行 maven | package 下载 Scala 反编译插件 Preferences | plugins | CFR Decompile Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/ide/intellijidea/IDEA创建GitHub项目.html":{"url":"src/ide/intellijidea/IDEA创建GitHub项目.html","title":"IDEA创建GitHub项目","keywords":"","body":"GitHub空项目 GitHub上创建空项目 如：hadoop-main IDEA创建project check out from version control | 选择 git 指定URL https://github.com/sciatta/hadoop-main.git 和本地目录 | clone | 选择 create project from existing sources 后续默认选择 IDEA创建module 创建 parent pom module hadoop-main | new module | maven groupid: com.sciatta.hadoop artifact: hadoop-main module name: hadoop-main # hadoop-main根目录作为module parent content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main # 修改pom.xml pom 创建child pom module hadoop-main | new module | maven parent: hadoop-main add as module to: hadoop-main artifact: hadoop-hdfs module name: hadoop-hdfs content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs # 修改pom.xml pom 创建child jar module hadoop-hdfs | new module | maven parent: hadoop-hdfs add as module to: hadoop-hdfs artifact: hadoop-hdfs-example module name: hadoop-hdfs-example content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs/hadoop-hdfs-example # 修改pom.xml jar GitHub已有项目 HTTPS GitHub下载源码 # 下载源码 git clone https://github.com/sciatta/hadoop.git # 切换到目标版本 cd hadoop git checkout -b work-2.7.0 release-2.7.0 导入GitHub项目 open | 指定项目路径 加载即可。如果是maven项目，则等待更新依赖，成功后会正确显示maven项目。 SSH 设置用户信息 git config --global user.name \"yangxiaoyu\" git config --global user.email 15058553030@163.com # 查看设置是否生效 git config --list 创建 SSH KEY ssh-keygen -t rsa -C \"15058553030@163.com\" # 拷贝公钥 id_rsa.pub 内容 cd /Users/yangxiaoyu/.ssh GITHUB 配置 Settings | SSH and GPG keys | New SSH key | Title: DEV Key: id_rsa.pub 内容 # 验证是否配置成功 # Are you sure you want to continue connecting (yes/no/[fingerprint])? yes ssh -T git@github.com 克隆仓库 git clone git@github.com:sciatta/JAVA-000.git 创建Project Create New Project | Maven Name: JAVA-000 Location: ~/work/bigdata/project/JAVA-000 4.0.0 com.sciatta java-000 1.0-SNAPSHOT pom 创建Module JAVA-000 右键 | New Module Parent: java-000 Name: week-01 Location: ~/work/bigdata/project/JAVA-000/Week_01 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "},"src/cloudplatform/vultr/Shadowsocks服务端配置.html":{"url":"src/cloudplatform/vultr/Shadowsocks服务端配置.html","title":"Shadowsocks服务端配置","keywords":"","body":"注册 登录 https://www.vultr.com/ 注册用户 新建Server Products | Deploy new server Server Location Australia | Sydney Server Type 64 bit OS | CentOS | 7 x64 管理Server 登录 ping -c 3 IP Address 查看延时情况 ssh root@IP Address UI界面 copy password 复制到控制台登录 （不需要修改root密码） 安装wget 视情况安装 yum -y install wget 初始化目录 mkdir -p /export/ && cd /export/ 下载脚本 # 下载 wget https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh # 修改权限 chmod +x shadowsocks.sh 运行脚本 cd /export/ # 当出现 Enjoy it! 表示安装成功 ./shadowsocks.sh 2>&1 | tee shadowsocks.log # ==== # 密码 # 端口号 # 加密（可以使用默认aes-256-gcm） # ==== 检查服务 ps -ef | grep ssserver 停止服务 ssserver -c /etc/shadowsocks.json -d stop 启动服务 ssserver -c /etc/shadowsocks.json -d start Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-12-24 23:50:06 "}}