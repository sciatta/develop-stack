{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-19 17:49:23 "},"src/bigdata/hadoop/hdfs/HDFS命令.html":{"url":"src/bigdata/hadoop/hdfs/HDFS命令.html","title":"HDFS命令","keywords":"","body":"hdfs hdfs dfs hdfs有两种命令风格 hadoop fs hdfs dfs 两种命令等价 help hadoop fs -help ls hdfs dfs -help ls ls # 查看根目录文件列表 hdfs dfs -ls / # 递归显示目录内容 hdfs dfs -ls -R / # 显示本地文件系统列表，默认hdfs # file:// 表示本地文件协议 hdfs dfs -ls file:///bigdata/ touchz # 创建空文件 hdfs dfs -touchz /mynote appendToFile # 向文件末尾追加内容 # 注意命令区分大小写 hdfs dfs -appendToFile hello /mynote cat # 查看文件内容 hdfs dfs -cat /mynote put # 上传本地文件到hdfs hdfs dfs -put hello /h1 copyFromLocal（同put） hdfs dfs -copyFromLocal hello /h2 moveFromLocal # 上传成功，删除本机文件 hdfs dfs -moveFromLocal hello /h3 get # 下载hdfs文件到本地 hdfs dfs -get /h1 hello copyToLocal（同get） hdfs dfs -copyToLocal /h1 hello1 mkdir # 创建目录 hdfs dfs -mkdir /shell rm # 删除文件到垃圾桶 # 不能删除目录 hdfs dfs -rm /h1 # 递归删除目录 hdfs dfs -rm -r /shell rmr # 递归删除目录 # 不建议使用，可使用 rm -r 代替 hdfs dfs -rmr /hello mv # 目的文件或目录不存在，修改文件名或目录名 # 目的文件存在，不可移动 hdfs dfs -mv /h2 /h22 # 目的目录存在，将子文件或子目录移动到目录 hdfs dfs -mv /h22 /hello cp # 拷贝文件 # 若目的文件存在，不可复制 hdfs dfs -cp /bigf /bf # 拷贝文件到目的目录 hdfs dfs -cp /bigf /hello find # 查找文件 hdfs dfs -find / -name \"h*\" text # 查看文件内容，若为SequenceFile，即使压缩，也可以正常查看压缩前内容 hdfs dfs -text /sequence/none expunge # 清空回收站，同时创建回收站checkpoint hdfs dfs -expunge hdfs getconf namenodes # 获取NameNode节点名称，可能有多个 hdfs getconf -namenodes confKey # 用相同的命令可以获得其他属性值 # 获取最小块大小 默认1048576byte（1M） hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size nnRpcAddresses # 获取NameNode的RPC地址 hdfs getconf -nnRpcAddresses hdfs dfsadmin safemode # 查看当前安全模式状态 hdfs dfsadmin -safemode get # 进入安全模式 # 安全模式只读 # 增删改不可以，查可以 hdfs dfsadmin -safemode enter # 退出安全模式 hdfs dfsadmin -safemode leave allowSnapshot 快照顾名思义，就是相当于对我们的hdfs文件系统做一个备份，我们可以通过快照对我们指定的文件夹设置备份，但是添加快照之后，并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 # 创建快照之前，先要允许该目录创建快照 hdfs dfsadmin -allowSnapshot /main # 禁用 hdfs dfsadmin -disallowSnapshot /main # 指定目录创建快照 # Created snapshot /main/.snapshot/s20200113-114345.126 # 可以通过浏览器访问 http://192.168.2.100:50070/explorer.html#/main/.snapshot/s20200113-114345.126 hdfs dfs -createSnapshot /main # 创建快照指定名称 hdfs dfs -createSnapshot /main snap1 # 快照重命名 hdfs dfs -renameSnapshot /main snap1 snap2 # 列出当前用户下的所有快照目录 hdfs lsSnapshottableDir # 比较两个快照的不同 hdfs snapshotDiff /main snap1 snap2 # 删除快照 hdfs dfs -deleteSnapshot /main snap1 hdfs fsck # 查看文件的文件、块、位置信息 hdfs fsck /h3 -files -blocks -locations hdfs namenode format # 格式化NameNode，只在初次搭建集群时使用 hdfs namenode -format hdfs oiv # 查看fsimage内容 offine image view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oiv -i fsimage_0000000000000000196 -p XML -o test.xml hdfs oev # 查看edits内容 offine edits view # -i 输入文件 # -p 处理格式 # -o 输出文件 hdfs oev -i edits_0000000000000000001-0000000000000000009 -p XML -o test.xml hadoop hadoop fs（同hdfs dfs） hadoop checknative # 查看本地库安装状态 hadoop checknative hadoop jar # 在集群上执行自定义的jar hadoop jar hadoop-mapreduce-wordcount-1.0-SNAPSHOT.jar com.sciatta.hadoop.mapreduce.wordcount.WordCount hadoop archive # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main hadoop distcp # hadoop 集群间数据拷贝 hadoop distcp hdfs://node01:8020/test hdfs://cluster:8020/ Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 15:32:50 "},"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html":{"url":"src/bigdata/hadoop/hdfs/HDFS核心功能原理.html","title":"HDFS核心功能原理","keywords":"","body":"hdfs优缺点 优点 高容错性 数据自动保存多个副本； 某一个副本丢失后，可以自动恢复。 适合批处理 它是通过移动计算而不是移动数据； 它会把数据位置暴露给计算框架。 适合大数据处理 数据规模，可以处理数据规模达到GB、TB、甚至PB级别的数据； 文件规模，能够处理百万级别规模以上的文件数量； 节点规模：能够处理10K节点的规模。 流式数据访问 一次写入，多次读取，不能修改，只能追加； 它能保证数据的一致性。 可构建在廉价机器上，通过多副本机制，提高可靠性 它通过多副本机制，提高可靠性； 它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。 缺点 不适合低延时数据访问，比如毫秒级的数据访问 不适合毫秒级的数据访问； 它适合高吞吐率的场景，就是在某一时间内写入大量的数据。 无法高效的对大量小文件进行存储 存储大量小文件，会占用NameNone大量内存存储元数据。但NameNode的内存总是有限的； 小文件存储的寻址时间会超过读取时间，违反了HDFS的设计目标。 不支持并发写入、文件随机修改 一个文件只能有一个线程写，不允许多个线程多时写； 仅支持数据append，不支持文件的随机修改。 hdfs写入流程 概述 写入本地file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode向HDFS写入数据，先调用DistributedFileSystem的 create 方法获取FSDataOutputStream。 DistributedFileSystem调用NameNode的 create 方法，发出文件创建请求。NameNode对待上传文件名称和路径做检验，如上传文件是否已存在同名目录，文件是否已经存在，递归创建文件的父目录（如不存在）等。并将操作记录在edits文件中。 ClientNode调用FSDataOutputStream向输出流输出数据（假设先写block1）。 FSDataOutputStream调用NameNode的 addBlock 方法申请block1的blockId和block要存储在哪几个DataNode（假设DataNode1，DataNode2和DataNode3）。若pipeline还没有建立，则根据位置信息建立pipeline。 同返回的第一个DataNode节点DataNode1建立socket连接，向其发送package。同时，此package会保存一份到ackqueue确认队列中。 写数据时先将数据写到一个校验块chunk中，写满512字节，对chunk计算校验和checksum值（4字节）。 以带校验和的checksum为单位向本地缓存输出数据（本地缓存占9个chunk），本地缓存满了向package输入数据，一个package占64kb。 当package写满后，将package写入dataqueue数据队列中。 将package从dataqueue数据对列中取出，沿pipeline发送到DataNode1，DataNode1保存，然后将package发送到DataNode2，DataNode2保存，再向DataNode3发送package。DataNode3接收到package，然后保存。 package到达DataNode3后做校验，将校验结果逆着pipeline回传给ClientNode。 DataNode3将校验结果传给DataNode2，DataNode2做校验后将校验结果传给DataNode1，DataNode1做校验后将校验结果传给ClientNode。 ClientNode根据校验结果判断，如果”成功“，则将ackqueue确认队列中的package删除；如果”失败“，则将ackqueue确认队列中的package取出，重新放入到dataqueue数据队列末尾，等待重新沿pipeline发送。 当block1的所有package发送完毕。即DataNode1、DataNode2和DataNode3都存在block1的完整副本，则三个DataNode分别调用NameNode的 blockReceivedAndDeleted方法。NameNode会更新内存中DataNode和block的关系。 ClientNode关闭同DataNode建立的pipeline。 文件仍存在未发送的block2，则继续执行4。直到文件所有数据传输完成。 全部数据输出完成，调用FSDataOutputStream的 close 方法。 ClientNode调用NameNode的 complete 方法，通知NameNode全部数据输出完成。 容错 假设当前构建的pipeline是DataNode1、DataNode2和DataNode3。当数据传输过程中，DataNode2中断无法响应，则当前pipeline中断，需要重建。 先将ackqueue中的所有package取出放回到dataqueue末尾。 ClientNode调用NameNode的 updateBlockForPipeline 方法，为当前block生成新的版本，如ts1（本质是时间戳），然后将故障DataNode2从pipeline中删除。 FSDataOutputStream调用NameNode的 getAdditionalDataNode 方法，由NameNode分配新的DataNode，假设是DataNode4。 FSDataOutputStream把DataNode1、DataNode3和DataNode4建立新的pipeline，DataNode1和DataNode3上的block版本设置为ts1，通知DataNode1或DataNode3将block拷贝到DataNode4。 新的pipeline创建好后，FSDataOutputStream调用NameNode的 updataPipeline 方法更新NameNode元数据。之后，按照正常的写入流程完成数据输出。 后续，当DataNode2从故障中恢复。DataNode2向NameNode报送所有block信息，NameNode发现block为旧版本（非ts1），则通过DataNode2的心跳返回通知DataNode2将此旧版本的block删除。 分析源码 解惑 [x] Lease（租约） 在HDFS写文件中是通过Lease（租约）来维护写文件凭证的，所以得到一个文件的写权限之后将其租约进行存储并定时更新。 [ ] block、package数据格式，如何校验（TCP协议，为什么会出现丢包），输出的基本单位（package），如何输出 [ ] hdfs block的上限是多少？ hdfs读取流程 概述 获取file文件，假设文件200M，则共有2个块，block1为128M（hdfs默认块大小为128M），block2为72M。默认三个副本。 ClientNode调用DistributedFileSystem的 open 方法获取FSDataInputStream。 DistributedFileSystem向NameNode发出请求获取file文件的元数据，包括所有块所在的DataNode的位置信息。 ClientNode调用FSDataInputStream获取数据流。 FSDataInputStream调用就近DanaNode获取block1。DanaNode开始传输数据给客户端，从磁盘里面读取数据输入流，以Packet为单位来做校验。ClientNode以Packet为单位接收数据，先在本地缓存，然后写入目标文件。 文件仍存在未读取的block2，则继续执行4。直到文件所有数据读取完成。 全部数据接收完成，关闭数据流FSDataInputStream。 分析源码 解惑 [ ] 文件块信息LocatedBlocks包含哪些重要信息？ [ ] 重传机制 [ ] DataNode优先位置策略 [ ] 数据的版本问题 NN和SNN功能剖析 概述 NameNode对集群中元数据进行管理，外围节点需要频繁随机存取元数据。 如何支持快速随机存取？因此需要把元数据存储在内存中。但内存中的数据在服务器断电后就会丢失，所以内存中的元数据需要被持久化。 持久化哪里？持久化到文件系统中，存储为fsimage文件。随着时间的流逝，fsimage文件会变得越来越庞大，同时对内存和fsimage元数据的增、删、改、查操作，fsimage文件的大小就会成为存取速度的瓶颈。 如何优化？引入edits日志文件。fsimage为某一时间节点的全量元数据，而edits日志为最新元数据。也就是说，Namenode同时对内存和edits日志进行操作。 之后又会出现edits日志越来越大，以及如何同fsimage合并的问题？系统引入了SecondNameNode，其负责将edits日志和fsimage合并，然后将最新的fsimage推送给NameNode，而NameNode则是向最新生成的edits日志文件写入元数据。 工作机制 如果NameNode是首次启动，则需要格式化HDFS，生成fsimage和edits；否则，启动时读取fsimage和edits到内存中初始化元数据。 ClientNode向NameNode发起元数据操作请求（增删改查），NameNode将元数据先写入edits（防止NameNode挂掉，内存中元数据丢失导致客户端无法访问数据），再写入内存中。 SecondNameNode向NameNode定时发起请求确认是否需要checkpoint。如果满足到达设置定时时间间隔或edits文件写满，则发起checkpoint请求；否则，继续等待。 checkpoint需满足条件： 时间达到一个小时fsimage与edits就会进行合并 dfs.namenode.checkpoint.period 3600s hdfs操作达到1000000次也会进行合并 dfs.namenode.checkpoint.txns 1000000 检查间隔 每隔多长时间检查一次hdfs dfs.namenode.checkpoint.check.period 60s 请求执行check point。 NameNode滚动生成新的edits.new文件，后续ClientNode对元数据操作请求都记录到edits.new文件中。 SecondNameNode通过http get获取NameNode滚动前的edits和fsimage文件。 SecondNameNode将fsimage读入内存，逐条执行edits，合并生成fsimage.ckpt文件。 SecondNameNode通过http post将fsimage.ckpt文件发送到NameNode上。 NameNode将fsimage.ckpt改名为fsimage（此文件为此刻全量元数据，待后续NameNode重启加载），将edits.new改名为edits。同时，会更新fstime。 解惑 [ ] 如何回放？ [ ] NameNode内存不够，如何解决？ 联邦机制（什么是？） 增加物理内存。 DataNode工作机制和数据存储 工作机制 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件：一个是数据本身，一个是元数据包括数据块的长度，数据块的校验和，以及时间戳。 DataNode启动后向NameNode注册，通过后则周期性（1小时）的向NameNode上报所有的块信息。 心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令。如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 集群运行中可以安全加入和退出一些机器。 数据完整性 当Client读取DataNode上block的时候，会计算checksum。如果计算后的checksum，与block创建时值不一样，说明block已经损坏。这时Client需要读取其他DataNode上的block。 DataNode在其文件创建后周期验证checksum。 掉线参数设置 DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval dfs.namenode.heartbeat.recheck-interval 300000ms dfs.heartbeat.interval 3s 解惑 [x] 一个DataNode上存储的所有数据块可以有相同的，为什么？ 一般出于安全性和高可用性考虑，并不会把一个block的多个副本放在同一个datanode上。但也不是绝对。例如，三个datanode，副本默认是三个的话，那么正常来说，每个节点上存储一个block副本是最好的（安全、可靠性高，单节点出现问题，并不会丢失数据），如果把3个副本都放在一个节点上，一旦这个节点出现问题，数据就可能丢失了；如果副本数是5个的话，那么就存在同一个datanode有多个副本了，即副本数比datanode数多的时候，必然存在一个datanode上存放多个相同block了。 小文件治理 概述 hdfs中文件以block存储在DataNode中，而所有文件的元数据全部存储在NameNode的内存中。无论文件大小，都会占用NameNode元数据的内存存储空间，大约占用150K左右。所以，系统中如果有大量小文件的话，会出现DataNode的磁盘容量没有充分利用，而NameNode的内存却被大量消耗，然而NameNode的内存是有容量限制的。所以，需要对小文件治理。 HAR方案 本质启动MapReduce，因此需要首先启动Yarn。 创建归档文件 # -archiveName 档案名称 # -p 父目录 # * 相对于父目录的相对路径 # 存储档案名称的路径 hadoop archive -archiveName data.har -p /main data data1 data2 /main # 源文件不会删除 查看归档文件 # 显示归档包含文件 # 归档文件的类型是d，即目录 # -R 递归列出目录内容 hdfs dfs -ls -R /main/data.har # 显示归档包含实际内容 hdfs dfs -ls -R har:///main/data.har 解压归档文件 # -p 如果目录已经存在，不会失败 hdfs dfs -mkdir -p /main/out hdfs dfs -cp har:///main/data.har/* /main/out SequenceFile方案 SequenceFile是由record构成，每个record是由键值对构成，其中文件名作为record的key，而文件内容作为record的value。Record间随机插入Sync，方便定位到Record的边界。 SequenceFile是可以分割的，所以可以利用MapReduce切分，独立运算。 HAR不支持压缩，而SequenceFile支持压缩。支持两类压缩： Record压缩 Block压缩，一次性压缩多条Record作为一个Block；每个Block开始处都需要插入Sync 当不指定压缩算法时，默认使用zlib压缩 无论是否压缩，采用何种算法，均可使用 hdfs dfs -text 命令查看文件内容 一般情况下，以Block压缩为最好选择。因为一个Block包含多条Record，利用Record间的相似性进行压缩，压缩效率更高。 把已有小文件转存为SequenceFile较慢，相比先写小文件，再写SequenceFile而言，直接将数据写入SequenceFile是更好的选择，省去小文件作为中间媒介。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-24 14:55:45 "},"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html":{"url":"src/bigdata/hadoop/mapreduce/MapReduce工作原理.html","title":"MapReduce工作原理","keywords":"","body":"概述 MapReduce的核心思想是“分而治之”，把大任务分解为小任务并行计算。Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。Reduce负责“合”，即对Map阶段的结果进行全局汇总。 编程模型 Map阶段 第一步：InputFormat 设置InputFormat类，以split作为输入单位，将数据切分成 (key, value) 输出。本质是把大任务拆分为互相独立的小任务。如TextInputFormat，输入的是split（默认大小和block一样），输出的key是行偏移位置，value是每行的内容。 InputFormat 描述 TextInputFormat 1、默认将每个block作为一个split；2、输出的key是行偏移位置，value是每行的内容 CombineTextInputFormat 1、解决小文件导致过多split的问题。涉及到虚拟存储和切片过程，可以自定义split大小；2、输出的key是行偏移位置，value是每行的内容 KeyValueTextInputFormat 1、默认将每个block作为一个split；2、以自定义分隔符进行分割，输出相应的key和value NLineInputFormat 1、以输入文件的N行作为一个split；2、输出的key是行偏移位置，value是每行的内容 InputFormat输入格式类 InputSplit输入分片类：InputFormat输入格式类将输入文件分成一个个分片InputSplit；每个MapTask对应一个split分片 RecordReader记录读取器类：：读取分片数据，一行记录生成一个键值对 第二步：MapTask 自定义map逻辑，对输入的 (key, value) 进行处理，转换成新的 (key, value) 进行输出。本质是把小任务拆分为最小可计算任务。例如统计单词数量，MapTask输入的key是行偏移位置，value是每行的内容。可以进一步拆分单行的内容，输出的key是一个单词，value是1。后面把所有相同单词的value累加，即为一个单词出现的数量。最后汇总所有单词即可。 Shuffle阶段 第三步：Partition 对输入的 (key, value) 进行分区。满足条件的key划分到一个分区中，一个分区发送到一个ReduceTask。 如果指定6个分区，而ReduceTask的个数是3，则会出现异常； 如果指定6个分区，而ReduceTask的个数是9，则后3个ReduceTask没有数据输入。 第四步：Order 排序是MapReduce框架中最重要的操作之一。 MapTask和ReduceTask均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。 对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 各种排序的分类： 部分排序 MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序 全排序 最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构 辅助排序 在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。 二次排序 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 第五步：Combine 对分组后的数据进行规约(combine操作)，降低数据的网络拷贝（可选步骤） Combiner的父类是Reducer，区别在于Combiner是在MapTask节点运行，而Reduce在ReduceTask节点运行，接收全局所有MapTask的输出结果。Combiner的意义在于对MapTask的输出做局部汇总，减少网络传输量。但Combiner应用的前提是不影响最终的业务逻辑。 第六步：Group 对排序后的数据进行分组，将相同key的value放到一个集合当中。 GroupingComparator是MapReduce当中reduce端的一个功能组件，主要的作用是决定哪些数据作为一组，调用一次reduce的逻辑，默认是每个不同的key作为多个不同的组，每个组调用一次reduce逻辑，我们可以自定义GroupingComparator实现不同的key作为同一个组，调用一次reduce逻辑。 Reduce阶段 第七步：ReduceTask 自定义Reduce逻辑，对输入的 (key, value) 进行处理，转换成新的 (key, value) 进行输出 第八步：OutputFormat 设置Outputformat将输出的 (key, value) 保存到文件中 MapTask工作机制 MapTask个数 MapTask的并行度是由什么决定的？ 在MapReduce中每个MapTask处理一个切片split的数据量，注意block是hdfs系统存储数据的单位，而切片是每个MapTask处理数据量单位。 block：hdfs在物理上把文件数据切分成一块一块。 split：是逻辑上对输入进行切片，并不会影响磁盘文件。 一个job的Map阶段的并行度是由客户端提交job时的切片数决定的 每一个切片分配一个MapTask并行处理 默认情况下，切片大小和block大小相等 切片时不考虑数据整体，而是针对单个文件单独切片 切片大小的计算公式：Math.max(minSize, Math.min(maxSize, blockSize)); 其中， mapreduce.input.fileinputformat.split.minsize=1 mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue block的默认大小是128M，所以split的默认大小是128M，同block的大小一致。 如果要控制split的数量，则只需要改变minsize或maxsize就可以改变切片的大小。如果自定义split大于128M，minsize要大于128M；如果自定义split小于128M，maxsize要小于128M。 如果有1000个小文件，每个文件在1K-100M之间，默认情况下有1000个block，1000个split，1000个MapTask并行处理，效率如何？ 默认情况下，使用的TextInputFormat按照文件规划切片，不管文件多小（小于128M）都会作为一个单独的切片，启动一个MapTask处理。而启动MapTask所消耗的资源要远大于计算，因此采用TextInputFormat效率极低。通过CombineTextInputFormat来控制小文件的切片数量，可以在逻辑上将多个小文件规划到一个切片中，从而控制MapTask的数量。 CombineTextInputFormat切片机制： CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4m 切片过程包括虚拟存储和切片两步： 虚拟存储：将所有小文件依次同MaxInputSplitSize参数作比较，使得最后划分的逻辑块都不大于MaxInputSplitSize 小于参数，则直接作为一个逻辑块 大于参数，且不大于2倍参数，则均分作为两个逻辑块（防止出现太小的切片） 大于2倍参数，则先按MaxInputSplitSize参数切割一个逻辑块，剩下的继续1、2、3步判断 如：MaxInputSplitSize设置为4M，输入文件大小为8.02M，满足3，则先划分一个4M的逻辑块，剩下人4.02继续判断；4.02满足2，则均分为两个2.01M的逻辑块。 切片：将所有虚拟存储逻辑块依次同MaxInputSplitSize参数作比较，使得最后划分的切片接近MaxInputSplitSize 如果大于等于 MaxInputSplitSize ，则作为一个切片。继续下一个逻辑块开始判断 如果小于 MaxInputSplitSize ，则同下一个虚拟存储逻辑块一起作为一个切片。继续1、2步判断，直到满足条件1或所有虚拟存储逻辑块合并完成 如：MaxInputSplitSize设置为4M，有4个小文件大小：1.7M、5.1M、3.4M以及6.8M 虚拟存储为6个逻辑块：1.7M、（2.55M+2.55M）、3.4M、（3.4M+3.4M） 合并为3个切片：（1.7M+2.55M=4.25M）、（2.55M+3.4M=5.95M）、（3.4M+3.4M=6.8M） 测试场景 MaxInputSplitSize = 4M 测试用例 虚拟存储 切片 MapTask 10个文件：0.1K 10个0.1k的逻辑块 1k（只有一个切片，因为所有文件合并后仍小于4M） 1 2个文件：8.1M、8K （4M+2.05M+2.05M）、8K 4M、（2.05M+2.05M）、8K 3 工作机制 Read阶段：InputFormat以split作为输入单位，将数据切分成 (k1, v1) 输出。 Map阶段：将 (k1, v1) 交给用户编写map()函数处理，并产生 (k2, v2) 输出。 Collect阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的 (k2, v2) 分区（调用Partitioner），并写入一个环形内存缓冲区中。 Spill阶段：即“溢写”，当环形缓冲区满80%后，MapTask会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。 Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。 在合并溢写文件时，如果至少有3个溢写文件，并且设置了map端combine的话，会在合并的过程中触发combine操作； 但是若只有2个或1个溢写文件，则不触发combine操作（因为combine操作，本质上是一个reduce，需要启动JVM虚拟机，有一定的开销） 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 ReduceTask工作机制 ReduceTask个数 ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置： // 默认值是1，手动设置为4 job.setNumReduceTasks(4); // 可以设置为0，不需要ReduceTask处理 job.setNumReduceTasks(0); 工作机制 Copy阶段：ReduceTask启动线程从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 Sort：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。注意，Merge和Sort两个阶段，交替进行。 Reduce阶段：按照key排序的数据，调用GroupingComparator对数据分组，使得相同key的不同value放到一个集合中，每一组数据 (k2, v2) 只调用一次reduce()函数，输出 (k3, v3) 。 Write阶段：最后由OutputFormat将计算结果写到HDFS上。一个ReduceTask对应一个文件。 Shuffle中的数据压缩 压缩算法 在shuffle阶段，从map阶段输出的数据，都要通过网络拷贝发送到reduce阶段。这一过程，涉及到大量的网络IO，如果数据能够进行压缩，那么数据的发送量就会少很多。 hadoop支持的压缩算法 压缩格式 工具 算法 文件扩展名 是否可切分 DEFLATE 无 DEFLATE .deflate 否 Gzip gzip DEFLATE .gz 否 bzip2 bzip2 bzip2 bz2 是 LZO lzop LZO .lzo 否 LZ4 无 LZ4 .lz4 否 Snappy 无 Snappy .snappy 否 各种压缩算法对应使用的java类 压缩格式 java类 DEFLATE org.apache.hadoop.io.compress.DeFaultCodec Gzip org.apache.hadoop.io.compress.GZipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec LZ4 org.apache.hadoop.io.compress.Lz4Codec Snappy org.apache.hadoop.io.compress.SnappyCodec 常见的压缩速率比较 压缩算法 原始文件大小 压缩后的文件大小 压缩速度 解压缩速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO-bset 8.3GB 2GB 4MB/s 60.6MB/s LZO 8.3GB 2.9GB 135 MB/s 410 MB/s snappy 8.3GB 1.8GB 172MB/s 409MB/s 常用的压缩算法主要有 LZO 和 snappy 等。 启用压缩 编码中设置 Map阶段压缩 Configuration configuration = new Configuration(); configuration.set(\"mapreduce.map.output.compress\",\"true\"); configuration.set(\"mapreduce.map.output.compress.codec\",\"org.apache.hadoop.io.compress.SnappyCodec\"); Reduce阶段压缩 configuration.set(\"mapreduce.output.fileoutputformat.compress\",\"true\"); configuration.set(\"mapreduce.output.fileoutputformat.compress.type\",\"RECORD\"); configuration.set(\"mapreduce.output.fileoutputformat.compress.codec\",\"org.apache.hadoop.io.compress.SnappyCodec\"); 修改mapred-site.xml（全局） Map阶段压缩 mapreduce.map.output.compress true mapreduce.map.output.compress.codec org.apache.hadoop.io.compress.SnappyCodec Reduce阶段压缩 mapreduce.output.fileoutputformat.compress true mapreduce.output.fileoutputformat.compress.type RECORD mapreduce.output.fileoutputformat.compress.codec org.apache.hadoop.io.compress.SnappyCodec 注意，所有节点都需要修改mapred-site.xml。修改后，重启集群生效。 数据倾斜 什么是数据倾斜 数据中不可避免地会出现离群值（outlier），并导致数据倾斜。这些离群值会显著地拖慢MapReduce的执行。 常见的数据倾斜有以下几类： 数据频率倾斜——某一个区域的数据量要远远大于其他区域。比如某一个key对应的键值对远远大于其他键的键值对。 数据大小倾斜——部分记录的大小远远大于平均值。 在map端和reduce端都有可能发生数据倾斜。 在map端的数据倾斜可以考虑使用combine：导致磁盘IO和网络IO过大 在reduce端的数据倾斜常常来源于MapReduce的默认分区器：ReduceTask繁忙空闲严重不均 数据倾斜会导致map和reduce的任务执行时间大为延长，也会让需要缓存数据集的操作消耗更多的内存资源。 如何诊断哪些键存在数据倾斜 发现倾斜数据之后，有必要诊断造成数据倾斜的那些键。有一个简便方法就是在代码里实现追踪每个键的最大值。 为了减少追踪量，可以设置数据量阀值，只追踪那些数据量大于阀值的键，并输出到日志中。 运行作业后就可以从日志中判断发生倾斜的键以及倾斜程度；跟踪倾斜数据是了解数据的重要一步，也是设计MapReduce作业的重要基础 解决数据倾斜 Reduce数据倾斜一般是指map的输出数据中存在数据频率倾斜的状况，即部分输出键的数据量远远大于其它的输出键 如何减小reduce端数据倾斜的性能损失？常用方式有： 自定义分区 基于输出键的背景知识进行自定义分区。 例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。 Combine 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。 combine的目的就是在Map端聚合并精简数据。 抽样和范围分区 Hadoop默认的分区器是HashPartitioner，基于map输出键的哈希值分区。这仅在数据分布比较均匀时比较好。在有数据倾斜时就很有问题。 使用分区器需要首先了解数据的特性。TotalOrderPartitioner 中，可以通过对原始数据进行抽样得到的结果集来预设分区边界值。 TotalOrderPartitioner 中的范围分区器可以通过预设的分区边界值进行分区。因此它也可以很好地用在矫正数据中的部分键的数据倾斜问题。 数据大小倾斜的自定义策略 在map端或reduce端的数据大小倾斜都会对缓存造成较大的影响，乃至导致OutOfMemoryError异常。处理这种情况并不容易。可以参考以下方法：设置mapreduce.input.linerecordreader.line.maxlength 来限制RecordReader读取的最大长度。RecordReader在TextInputFormat和KeyValueTextInputFormat类中使用。默认长度没有上限。 MapReduce中的join操作 Order订单表 id date pid amount 1001 20150710 P0003 2 1002 20150710 P0002 3 1002 20150710 P0003 3 Product产品表 id name categoryid price P0001 xiaomi 1000 2000 P0002 apple 1000 5000 P0003 samsung 1000 3000 产品表和订单表是一对多关系，即一个产品可以属于多个订单，而一个订单只能有一个产品（唯一产品id，多个相同产品由amount字段确定）。两张表的数据以文件形式存储在hdfs上，且数据量非常大。现需要用MapReduce程序来实现SQL查询运算。 select o.id,o.date,p.name,p.categoryid,p.price from order o join product p on o.pid = p.id reduce端的join操作 两张表首先会划分为多个split输入，启动split数量的MapTask。把两张表的产品id放到Mapper的key中，把产品记录或订单记录放在Mapper的value中。经过Shuffle阶段，会将与产品id关联的订单和产品记录放到一个集合中作为value送到Reducer中，然后遍历集合将订单和产品合并。 假设，产品表相对较小，而在订单表中99%的订单都包含一种相同产品，对于多个并行计算的ReduceTask，则会出现99%的数据发送到同一个ReduceTask，造成其非常繁忙，而其他启动的ReduceTask则有可能出现空闲。 这就是数据倾斜 问题。并行计算力无法充分利用。为解决问题，可以自定义Partitioner，将数据尽量均匀的分区到不同的ReduceTask。注意，之所以出现数据倾斜，是由于在reduce端出现的问题，因此可以省略reduce一步，直接在map端完成数据的合并工作。即map端的join操作。 map端的join操作 适用于关联小表情况，在程序初始化时保存全局hdfs缓存文件路径，后续在mapper启动时一次读取小表数据放入缓存。这样，就可以在map阶段join操作，完全利用MapTask的并发算力，快速完成join操作。 注意，在不设置ReduceTask的情况下，默认仍有一个ReduceTask，可以观察输出文件 part-r-00000 第二位是r，表示由ReduceTask输出；因为所有逻辑都在MapTask中完成，不需要ReduceTask，因此设置 job.setNumReduceTasks(0); ，可以观察输出文件 part-m-00000 第二位是m，表示由MapTask输出。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-01 15:34:32 "},"src/bigdata/hadoop/yarn/YARN架构.html":{"url":"src/bigdata/hadoop/yarn/YARN架构.html","title":"YARN架构","keywords":"","body":"YARN架构 YARN 的全称是 Yet Another Resource Negotiator，YARN 是经典的主从 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave。 ResourceManager（RM） RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，主要有两个组件构成： 调度器：Scheduler； 调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。 应用程序管理器：Applications Manager，ASM。 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动 AM、监控 AM 运行状态并在失败时重新启动它等。 NodeManager（NM） NM 是每个节点上运行的资源和任务管理器。 它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态； 它接收并处理来自 AM 和 RM 的请求（Container 启动/停止）。 ApplicationMaster（AM） 提交的每个作业都会包含一个 AM，主要功能包括： 与 RM 协商以获取资源（用 container 表示）； 将得到的任务进一步分配给内部的任务； 与 NM 通信以启动/停止任务； 监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。 MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。 Container Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，以及环境变量、启动命令等任务运行相关的信息。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。 作业提交流程 client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 。 新的作业ID（应用ID）和 资源提交路径 由 RM 分配。 client 核实作业的输出，计算输入的split，将作业的资源（包括Jar包、配置文件、split信息）拷贝给 HDFS。 client 调用 RM 的 submitApplication() 来提交作业。 当 RM 收到 submitApplication() 的请求时，就将该请求发给 scheduler，scheduler 分配 container，并与对应的 NM 通信，要求其在这个 container 中启动 AM。 MapReduce 作业的 AM 是一个主类为 MRAppMaster 的 Java 应用，其通过构造一些 bookkeeping 对象来监控作业的进度，得到任务的进度和完成报告。 MRAppMaster 通过 HDFS 得到由 client 计算好的输入 split。然后为每个输入 split 创建 MapTask，根据 mapreduce.job.reduces 创建 ReduceTask。 如果作业很小，AM会选择在其自己的 JVM 中运行任务；如果不是小作业，那么 AM 向 RM 请求 container 来运行所有的 MapTask 和 ReduceTask。请求是通过心跳来传输的，包括每个 MapTask 的数据位置（如存放输入split的主机名和机架）。scheduler 利用这些信息来调度任务，尽量将任务分配给存储数据的节点，或退而分配给和存放输入 split 节点相同机架的节点。 当一个任务由 RM 调度分配一个 container 后，AM与该 NM 通信，要求其在这个 container 中启动任务。其中，任务是一个主类为 YarnChild 的 Java 应用执行。 YarnChild 在运行任务之前首先本地化任务需要的资源，如作业配置、JAR文件、以及HDFS的所有文件。将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 YarnChild 运行 MapTask 或 ReduceTask。YarnChild运行在一个专用的JVM中，但是YARN不支持JVM重用。 YARN中的任务将其进度和状态返回给AM，以让AM随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。同时，client通过 mapreduce.client.progressmonitor.pollinterval 向AM请求进度更新，向用户展示。 应用程序运行完成后，AM 向 RM 请求注销并关闭自己。 容错 对于分布式系统，不论是调度系统还是其他系统，容错机制都是非常必要的，YARN 需要容错的地方，有以下四个地方： ApplicationMaster 容错：ResourceManager 会和 ApplicationMaster 保持通信，一旦发现 ApplicationMaster 失败或者超时，会为其重新分配资源并重启。重启后 ApplicationMaster 的运行状态需要自己恢复，如 MRAppMaster 会把相关状态记录到 HDFS 上，重启后从 HDFS 读取运行状态恢复； NodeManager 容错：NodeManager 如果超时，则 ResourceManager 会认为它失败，将其上的所有 container 标记为失败并通知相应的 ApplicationMaster，由 AM 决定如何处理（可以重新分配资源运行某一个任务，也可以整个作业失败，重新运行所有任务）； container 容错：如果 ApplicationMaster 在一定时间内未启动分配的 container，RM 会将其收回；如果 Container 运行失败，RM 会通知对应的 AM 由其处理； ResourceManager 容错：RM 采用 HA 机制。 任务调度器 资源调度器是YARN最核心的组件之一，是一个插拔式的服务组件，负责整个集群资源的管理和分配。YARN提供了三种可用的资源调度器：FIFO、Capacity Scheduler、Fair Scheduler。 先进先出调度器（FIFO Scheduler） FIFO 是最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线性队列相应的位置，在资源调度时，按照队列的先后顺序、先进先出地进行调度和资源分配。很明显这种调度器过于简单，在实际的生产中，应用不是很多，毕竟需要调度的作业是有不同优先级的。 容量调度器（Capacity Scheduler） 容量调度器是 Yahoo 为 Hadoop 开发的多用户调度器，应用于用户量众多的应用场景，与公平调度器相比，其更强调资源在用户之间而非作业之间的公平性。 它将用户和任务组织成多个队列，每个队列可以设定资源最低保障和使用上限，当一个队列的资源有剩余时，可以将剩余资源暂时分享给其他队列。调度器在调度时，优先将资源分配给资源使用率最低的队列（即队列已使用资源量占分配给队列的资源量比例最小的队列）；在队列内部，则按照作业优先级的先后顺序遵循 FIFO 策略进行调度。 容量调度器有以下几点特点： 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源； 灵活性：如果一个队列资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列； 多重租赁：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束； 安全保证：每个队列有严格的 ACL 列表规定它访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序； 动态更新配置文件：管理员可以根据需要动态修改各种配置参数。 举例： root ​ — dev 60% ​ — spark 50% ​ — hadoop 50% ​ — prod 40% 假设job1提交到队列dev.spark，此时prod队列没有任务，dev.hadoop队列也没有任务。如果运行job需要的资源占30%够用，直接运行即可；否则，会弹性的先占用dev.hadoop队列的容量，占到60%容量资源；如仍然不够，则继续弹性占用prod队列容量。为了防止dev队列占用太多资源，可以为其设置上限，如75%。 Apache版本默认使用容量调度器。 公平调度器（Fair Scheduler） 公平调度器先将用户的任务分配到多个队列中，每个队列设定资源分配最低保障和最高上限，管理员也可以指定队列的优先级，优先级高的队列将会被分配更多的资源，当一个队列资源有剩余时，可以临时将剩余资源共享给其他队列。公平调度器的调度过程如下： 根据每个队列的最小资源保障，将系统中的部分资源分配给各个队列； 根据队列的指定优先级将剩余资源按照比例分配给各个队列； 在各个队列中，按照作业的优先级或者根据公平策略将资源分配给各个作业； 公平调度器有以下几个特点： 支持抢占式调度，即如果某个队列长时间未能被分配到公平共享量的资源，则调度器可以杀死过多分配资源的队列的任务，以空出资源供这个队列使用； 强调作业之间的公平性：在每个队列中，公平调度器默认使用公平策略来实现资源分配，这种公平策略是最大最小公平算法的一种具体实现，可以尽可能保证作业间的资源分配公平性； 负载均衡：公平调度器提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到给各个节点上； 调度策略配置灵活：允许管理员为每个队列单独设置调度策略； 提高小应用程序响应时间：由于采用了最大最小公平算法，小作业可以快速获得资源并运行完成。 举例： A、B、C三个队列，每个队列中的作业按照优先级分配资源，优先级越高分配的资源越多，但都可以分配到资源，以确保公平。当在资源有限的情况下，每个作业需要获得的资源与实际获得的资源存在差距，这个差距称作”缺额“。在同一个队列中，作业的”缺额“越大，则越优先被调度执行。此时，A、B、C三个队列，每个队列中会有多个作业同时运行。 CDH版本默认使用公平调度器。 配置多用户资源隔离 配置 修改yarn-site.xml node01执行 yarn.resourcemanager.scheduler.class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler yarn.scheduler.fair.allocation.file /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/fair-scheduler.xml yarn.scheduler.fair.preemption true yarn.scheduler.fair.preemption.cluster-utilization-threshold 0.8f yarn.scheduler.fair.user-as-default-queue true default is True yarn.scheduler.fair.allow-undeclared-pools false default is True 添加fair-scheduler.xml node01执行 30 512mb,4vcores 102400mb,100vcores 100 1.0 fair 512mb,4vcores 30720mb,30vcores 100 fair 1.0 * 512mb,4vcores 20480mb,20vcores 100 fair 2.0 hadoop hadoop hadoop hadoop 512mb,4vcores 20480mb,20vcores 100 fair 1.0 develop develop develop develop 512mb,4vcores 20480mb,20vcores 100 fair 1.5 test,hadoop,develop test test group_businessC,supergroup 分发 node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop scp yarn-site.xml fair-scheduler.xml node02:$PWD scp yarn-site.xml fair-scheduler.xml node03:$PWD 重启YARN集群 stop-yarn.sh start-yarn.sh 修改作业的提交队列 当前用户和用户组是hadoop hadoop // 不设置队列时，使用当前系统用户和用户组提交队列，提交至队列root.hadoop // 不可提交至队列root.develop(也可以简写成develop) ，因为其限制用户和用户组是develop develop // configuration.set(\"mapred.job.queue.name\", \"develop\"); 当前用户和用户组是develop develop // 可提交至队列root.develop configuration.set(\"mapred.job.queue.name\", \"develop\"); Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-05 12:05:30 "},"src/bigdata/hadoop/HDFS&MapReduce&YARN端口映射.html":{"url":"src/bigdata/hadoop/HDFS&MapReduce&YARN端口映射.html","title":"HDFS&MapReduce&YARN端口映射","keywords":"","body":"HDFS 参数 位置 默认 描述 fs.defaultFS core-site.xml 8020 NameNode的RPC地址 dfs.namenode.http-address hdfs-site.xml 50070 NameNode的http地址 dfs.namenode.secondary.http-address hdfs-site.xml 50090 SecondaryNameNode的http地址 MapReduce 参数 位置 默认 描述 mapreduce.jobhistory.address mapred-site.xml 10020 jobhistory的RPC地址 mapreduce.jobhistory.webapp.address mapred-site.xml 19888 jobhistory的http地址 YARN 参数 位置 默认 描述 yarn.resourcemanager.webapp.address yarn-site.xml 8088 yarn的http地址 8032 ResourceManager的RPC地址 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-02-05 11:05:17 "},"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html":{"url":"src/versioncontrol/git/GitBook&GitHub&Typora最佳实践.html","title":"GitBook&GitHub&Typora最佳实践","keywords":"","body":"概述 目前有众多的知识管理软件，对于比较私密的文档使用印象笔记，不仅支持Markdown这种轻量级的标记语言，也支持复杂的富文本语言，但是一些比较有用的功能则需要收费，比如查看历史版本、离线访问、容量限制。最近，使用了一下阿里的语雀，书写体验、排版和易操作方面确实属于不错的一款产品，可是，某一天断网了，不支持离线访问是硬伤。因此，想要寻找如下一款产品满足如下需求： 沉浸式的书写体验，不需要丰富的社交场景，也不需要过于酷炫的功能，只想安安静静的码字。 离线编辑，想什么时候访问自己做主。同时，同步是必不可少的。 支持版本管理。 不收费，也没有容量限制。想写多少写多少。 支持私有笔记。 支持同其他同事、同学沟通，因为大部分都是程序员，所以可以查看Markdown文件是必不可少的。 对笔记中的目录结构没有限制。 对笔记中关联的图像，本地和远程都可以访问，而且没有容量限制。 可以方便的导出其他格式，比如html，PDF等。 通过调研，使用GitHub&GitBook&Typora方案，可以完美的解决以上问题。 搭建环境 GitHub GitHub创建develop-stack项目，其中.gitignore选择gitbook 回到本地创建本地目录 mkdir note && cd note 下载项目到本地 git clone https://github.com/sciatta/develop-stack.git GitBook 安装nodejs brew install node 安装GitBook命令行工具 npm install -g gitbook-cli 在develop-stack目录执行 gitbook init 初始化Gitbook目录环境，生成两个重要文件README.md和SUMMARY.md Typora 编辑SUMMARY.md描述项目的目录和文件结构 在develop-stack目录执行 gitbook init 。GitBook会查找SUMMARY.md文件中描述的目录和文件，如果没有则会将其创建。注意，如果删除SUMMARY.md描述项目的目录和文件结构，执行 gitbook init 命令不会删除相应的目录或文件，需要手动维护。 GitBook 本地运行 gitbook serve --port 8088 开启GitBook服务。通过 http://localhost:8080 访问本地服务。在执行命令的同时会执行构建命令 gitbook build 生成 _book 目录。注意， _book 目录是临时目录，每次构建时全部重建。如果退出服务的话，执行 control + c 即可 本地构建 gitbook build ./ docs 。 GitHub 执行 git add 命令 git add -A 将文件提交到暂存区 执行 git commit 命令 git commit -m “test” 将文件提交到本地仓库 执行 git push 命令 git push origin master 将文件提交到远程仓库 GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Source选择 master branch/docs folder 。成功后显示 Your site is ready to be published at https://sciatta.github.io/develop-stack/ ，稍后即可访问GitHub Pages网站。 GitBook插件 在develop-stack目录创建book.json文件，配置插件后，运行 gitbook install 命令自动安装插件。 hide-element 隐藏默认gitbook左侧提示：Published with GitBook { \"plugins\": [ \"hide-element\" ], \"pluginsConfig\": { \"hide-element\": { \"elements\": [\".gitbook-link\"] } } } expandable-chapters gitbook默认目录没有折叠效果。 { \"plugins\": [ \"expandable-chapters\" ] } code 在代码区域的右上角添加一个复制按钮，点击一键复制代码。 { \"plugins\" : [ \"code\" ] } splitter 左侧目录和右侧文章可以拖动调节宽度。 { \"plugins\": [ \"splitter\" ] } search-pro 支持中英文。 { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ] } pageview-count 记录每个文章页面被访问的次数。本质是访问 https://hitcounter.pythonanywhere.com/ { \"plugins\": [ \"pageview-count\"] } tbfed-pagefooter 在每个文章下面标注版权信息和文章时间。 { \"plugins\": [ \"tbfed-pagefooter\" ], \"pluginsConfig\": { \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy sciatta.com 2020\", \"modify_label\": \"修订时间: \", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" } } } popup 点击可以在新窗口展示图片。 { \"plugins\": [ \"popup\" ] } auto-scroll-table 为避免表格过宽，增加滚动条。 { \"plugins\": [\"auto-scroll-table\"] } -sharing 去掉 GitBook 默认的分享功能。由于默认的一些推特，脸书都需要翻墙，所以将分享功能全部关闭。 \"plugins\": [ \"-sharing\" ] github-buttons 给 GitBook 添加 GitHub 的图标来显示 star 和 follow。 { \"plugins\": [ \"github-buttons\" ], \"pluginsConfig\": { \"github-buttons\": { \"buttons\": [{ \"user\": \"sciatta\", \"repo\": \"develop-stack\", \"type\": \"star\", \"count\": true, \"size\": \"small\" }] } } } github 在右上角显示 github 仓库的图标链接 { \"plugins\": [ \"github\" ], \"pluginsConfig\": { \"github\": { \"url\": \"https://github.com/sciatta\" } } } Google统计 添加Google统计 Google统计 https://analytics.google.com/ 在一个平台上可以全面分析业务数据，进而做出更明智的决策。在网站注册，获取跟踪ID { \"plugins\": [\"ga\"], \"pluginsConfig\": { \"ga\": { \"token\": \"UA-156491400-1\" } } } BaiDu 添加BaiDu统计 百度统计 https://tongji.baidu.com/ 在网站注册，获取跟踪ID { \"plugin\": [\"baidu-v3\"], \"pluginsConfig\": { \"baidu\": { \"token\": \"c6612709c010da681bbd4b785968a638\" } } } Donate Gitbook 捐赠打赏插件 { \"plugins\": [\"donate\"], \"pluginsConfig\": { \"donate\": { \"wechat\": \"/assets/wechat.jpg\", \"alipay\": \"/assets/alipay.jpg\", \"title\": \"\", \"button\": \"捐赠\", \"alipayText\": \" \", \"wechatText\": \" \" } } } anchors 标题带有 github 样式的锚点 { \"plugins\" : [ \"anchors\" ] } anchor-navigation-ex 页面内导航，一键回到顶部。 { \"plugins\": [\"anchor-navigation-ex\"], \"pluginsConfig\": { \"anchor-navigation-ex\": { \"showLevel\": true, \"associatedWithSummary\": false, \"printLog\": false, \"multipleH1\": true, \"mode\": \"float\", \"showGoTop\": true, \"float\": { \"floatIcon\": \"fa fa-navicon\", \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" }, \"pageTop\": { \"showLevelIcon\": false, \"level1Icon\": \"fa fa-hand-o-right\", \"level2Icon\": \"fa fa-hand-o-right\", \"level3Icon\": \"fa fa-hand-o-right\" } } } } sitemap 生成站点地图，便于爬虫抓取页面。可以通过http://www.sciatta.com/sitemap.xml 访问。 { \"plugins\": [\"sitemap\"], \"pluginsConfig\": { \"sitemap\": { \"hostname\": \"http://www.sciatta.com/\" } } } Typora设置 偏好设置 | 编辑器 | 图片插入 复制图片到 ./${filename}.assets 文件夹 优先使用相对路径 偏好设置 | 通用 | 启动选项 | 打开指定目录 选择工作目录 偏好设置 | 通用 | 侧边栏 侧边栏的大纲视图允许折叠和展开 绑定域名 获取GitHub Pages的IP地址 ping -c 3 sciatta.github.io 注意，ping的时候不需要加仓库的名称。 配置阿里云 进入阿里云解析列表，添加记录： 记录类型 主机记录 记录值 A @ 185.199.111.153 A www 185.199.111.153 配置GitHub Pages GitHub的develop-stack项目，Settings标签页的的GitHub Pages，修改Custom domain：www.sciatta.com Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-29 11:46:49 "},"src/virtualization/vmware/VMware系统安装.html":{"url":"src/virtualization/vmware/VMware系统安装.html","title":"VMware系统安装","keywords":"","body":"Mac安装VMware 安装VMware_Fusion_Pro_11.5 注册码 7HYY8-Z8WWY-F1MAN-ECKNY-LUXYX 设置网络 偏好设置 | 网络 | 新建vmnet2自定义网络连接 选择：使用NAT 选择：将MAC主机连接到该网络 取消选择：通过DHCP在该网络上提供地址 修改子网 # 空格需用\\转义 cd /Library/Preferences/VMware\\ Fusion sudo vi networking answer VNET_2_HOSTONLY_SUBNET 192.168.2.0 修改网关 cd vmnet2 sudo vi nat.conf # NAT gateway address ip = 192.168.2.2 偏好设置 | 网络 | vmnet2 取消选择：将MAC主机连接到该网络 | 应用 选择：将MAC主机连接到该网络 | 应用 目的是为了使配置生效。 创建自定义虚拟机 选择操作系统 Linux | Centos 7 64 创建完成后设置 内存：2048 MB 硬盘：40 GB 网络适配器：vmnet2 启动磁盘：CD/DVD（即设置BIOS） 安装Centos7 设置中选择 CD/DVD（IDE） 选中连接CD/DVD驱动器 选择镜像CentOS-7-x86_64-DVD-1810.iso 启动 install Centos 7 language：english date & time：Asia/shanghai installation destination：automatic partitioning selected network & hostname：ens33 | on root password：root（太短，双击确认即可） Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 16:52:11 "},"src/virtualization/vmware/VMware部署Hadoop生态环境.html":{"url":"src/virtualization/vmware/VMware部署Hadoop生态环境.html","title":"VMware部署Hadoop生态环境","keywords":"","body":"环境准备 准备三台虚拟机 ip设置 vi /etc/sysconfig/network-scripts/ifcfg-ens33 BOOTPROTO=\"static\" IPADDR=192.168.2.100 NETMASK=255.255.255.0 GATEWAY=192.168.2.2 DNS1=192.168.2.2 准备三台linux机器，IP地址分别设置成为 第一台机器IP地址：192.168.2.100 第二台机器IP地址：192.168.2.101 第三台机器IP地址：192.168.2.102 关闭防火墙 root用户下执行 systemctl stop firewalld systemctl disable firewalld 关闭selinux root用户下执行 vi /etc/selinux/config SELINUX=disabled 更改主机名 vi /etc/hostname node01 第一台主机名更改为：node01 第二台主机名更改为：node02 第三台主机名更改为：node03 更改主机名与IP地址映射 vi /etc/hosts 192.168.2.100 node01 192.168.2.101 node02 192.168.2.102 node03 同步时间 定时同步阿里云服务器时间 yum -y install ntpdate crontab -e */1 * * * * /usr/sbin/ntpdate time1.aliyun.com 添加用户 三台linux服务器统一添加普通用户hadoop，并给以sudo权限，用于以后所有的大数据软件的安装 并统一设置普通用户的密码为 hadoop useradd hadoop passwd hadoop 为普通用户添加sudo权限 visudo hadoop ALL=(ALL) ALL 定义统一目录 定义三台linux服务器软件压缩包存放目录，以及解压后安装目录，三台机器执行以下命令，创建两个文件夹，一个用于存放软件压缩包目录，一个用于存放解压后目录 # root 用户执行 mkdir -p /bigdata/soft # 软件压缩包存放目录 mkdir -p /bigdata/install # 软件解压后存放目录 chown -R hadoop:hadoop /bigdata # 将文件夹权限更改为hadoop用户 安装JDK 使用hadoop用户来重新连接三台机器，然后使用hadoop用户来安装jdk软件。上传压缩包到第一台服务器的/bigdata/soft下面，然后进行解压，配置环境变量，三台机器都依次安装 ```bash # 分别上传jdk文件 scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.100:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.101:/bigdata/soft scp jdk-8u141-linux-x64.tar.gz hadoop@192.168.2.102:/bigdata/soft cd /bigdata/soft/ tar -zxf jdk-8u141-linux-x64.tar.gz -C /bigdata/install/ sudo vi /etc/profile #添加以下配置内容，配置jdk环境变量 export JAVA_HOME=/bigdata/install/jdk1.8.0_141 export PATH=:$JAVA_HOME/bin:$PATH # 立即生效 source /etc/profile ``` ## hadoop用户免密码登 三台机器在hadoop用户下执行以下命令生成公钥与私钥 ```bash # 三台机器在hadoop用户下分别执行 ssh-keygen -t rsa # 三台机器拷贝公钥到node01 ssh-copy-id node01 # 在node01的hadoop用户下执行，将authorized_keys拷贝到node02和node03 # node01已经存在authorized_keys cd /home/hadoop/.ssh/ scp authorized_keys node02:$PWD scp authorized_keys node03:$PWD ``` # 安装zookeeper集群 注意：三台机器一定要保证时钟同步 zookeeper分发到node01 分发 本机执行 scp zookeeper-3.4.5-cdh5.14.2.tar.gz hadoop@192.168.2.100:/bigdata/soft 解压 node01执行 cd /bigdata/soft tar -zxvf zookeeper-3.4.5-cdh5.14.2.tar.gz -C /bigdata/install/ 修改配置文件 node01执行 cd /bigdata/install/zookeeper-3.4.5-cdh5.14.2/conf cp zoo_sample.cfg zoo.cfg mkdir -p /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas vi zoo.cfg dataDir=/bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas autopurge.snapRetainCount=3 autopurge.purgeInterval=1 server.1=node01:2888:3888 server.2=node02:2888:3888 server.3=node03:2888:3888 添加myid配置 node01执行 echo 1 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid zookeeper分发到node02和node03 分发 node01执行 scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node02:/bigdata/install/ scp -r /bigdata/install/zookeeper-3.4.5-cdh5.14.2/ node03:/bigdata/install/ 修改myid配置 node02执行 echo 2 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid node03执行 echo 3 > /bigdata/install/zookeeper-3.4.5-cdh5.14.2/zkdatas/myid 启动zookeeper服务 三台机器分别执行 /bigdata/install/zookeeper-3.4.5-cdh5.14.2/bin/zkServer.sh start # 查看启动状态 /bigdata/install/zookeeper-3.4.5-cdh5.14.2/bin/zkServer.sh status 安装hadoop CDH软件版本重新进行编译 为何编译 CDH和Apache发布包不支持C程序库。本地库可以用于支持压缩算法和c程序调用。 安装JDK 需要版本jdk1.7.0_80（1.8编译会出现错误） 安装maven # 解压缩 tar -zxvf apache-maven-3.0.5-bin.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export MAVEN_HOME=/bigdata/install/apache-maven-3.0.5 export MAVEN_OPTS=\"-Xms4096m -Xmx4096m\" export PATH=:$MAVEN_HOME/bin:$PATH # 立即生效 source /etc/profile 安装findbugs # 安装wget sudo yum install -y wget # 下载findbugs cd /bigdata/soft wget --no-check-certificate https://sourceforge.net/projects/findbugs/files/findbugs/1.3.9/findbugs-1.3.9.tar.gz/download -O findbugs-1.3.9.tar.gz # 解压findbugs tar -zxvf findbugs-1.3.9.tar.gz -C ../install/ # 配置环境变量 sudo vi /etc/profile export FINDBUGS_HOME=/bigdata/install/findbugs-1.3.9 export PATH=:$FINDBUGS_HOME/bin:$PATH # 立即生效 source /etc/profile 安装依赖 sudo yum install -y autoconf automake libtool cmake sudo yum install -y ncurses-devel sudo yum install -y openssl-devel sudo yum install -y lzo-devel zlib-devel gcc gcc-c++ sudo yum install -y bzip2-devel 安装protobuf # 解压缩 tar -zxvf protobuf-2.5.0.tar.gz -C ../install/ # 执行 cd /bigdata/install/protobuf-2.5.0 ./configure # root 用户执行 make && make install 安装snappy # 解压缩 cd /bigdata/soft/ tar -zxf snappy-1.1.1.tar.gz -C ../install/ # 执行 cd ../install/snappy-1.1.1/ ./configure # root 用户执行 make && make install 编译 # 解压缩 tar -zxvf hadoop-2.6.0-cdh5.14.2-src.tar.gz -C ../install/ # 编译 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 # 编译不支持snappy压缩 mvn package -Pdist,native -DskipTests -Dtar # 编译支持snappy压缩 mvn package -DskipTests -Pdist,native -Dtar -Drequire.snappy -e -X 安装hadoop集群 安装环境服务部署规划 服务器IP HDFS HDFS HDFS YARN YARN 历史日志服务器 192.168.2.100 NameNode SecondaryNameNode DataNode ResourceManager NodeManager JobHistoryServer 192.168.2.101 DataNode NodeManager 192.168.2.102 DataNode NodeManager 上传压缩包并解压 重新编译之后支持snappy压缩的hadoop包上传到第一台服务器并解压 主机执行 scp hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz hadoop@192.168.2.100:/bigdata/soft node01执行 cd /bigdata/soft/ tar -zxvf hadoop-2.6.0-cdh5.14.2_after_compile.tar.gz -C ../install/ 查看hadoop支持的压缩方式以及本地库 node01执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2 bin/hadoop checknative 如果出现openssl为false，那么所有机器在线安装openssl su root yum -y install openssl-devel 修改配置文件 修改core-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi core-site.xml fs.defaultFS hdfs://node01:8020 hadoop.tmp.dir /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas io.file.buffer.size 4096 fs.trash.interval 10080 修改hdfs-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hdfs-site.xml dfs.hosts /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/accept_host dfs.hosts.exclude /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/deny_host --> dfs.namenode.secondary.http-address node01:50090 dfs.namenode.http-address node01:50070 dfs.namenode.name.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas dfs.datanode.data.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas dfs.namenode.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits dfs.namenode.checkpoint.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name dfs.namenode.checkpoint.edits.dir file:///bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits dfs.replication 2 dfs.permissions false dfs.blocksize 134217728 修改hadoop-env.sh node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi hadoop-env.sh export JAVA_HOME=/bigdata/install/jdk1.8.0_141 修改mapred-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi mapred-site.xml mapreduce.framework.name yarn mapreduce.job.ubertask.enable true mapreduce.jobhistory.address node01:10020 mapreduce.jobhistory.webapp.address node01:19888 修改yarn-site.xml node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi yarn-site.xml yarn.resourcemanager.hostname node01 yarn.nodemanager.aux-services mapreduce_shuffle 修改slaves文件 node01 执行 cd /bigdata/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop vi slaves node01 node02 node03 创建文件存放目录 node01 执行 mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/tempDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/datanodeDatas mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/snn/name mkdir -p /bigdata/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/snn/edits 安装包分发 node01 执行 cd /bigdata/install/ scp -r hadoop-2.6.0-cdh5.14.2/ node02:$PWD scp -r hadoop-2.6.0-cdh5.14.2/ node03:$PWD 配置hadoop的环境变量 三台机器执行 sudo vi /etc/profile export HADOOP_HOME=/bigdata/install/hadoop-2.6.0-cdh5.14.2 export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 配置完成之后生效 source /etc/profile 集群启动 要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个集群。 注意：首次启动HDFS时，必须对其进行格式化操作。本质上是一些清理和准备工作，因为此时的 HDFS 在物理上还是不存在的。 node01 执行 hdfs namenode -format 或者 hadoop namenode –format 单个节点逐一启动 # 在主节点上使用以下命令启动 HDFS NameNode: hadoop-daemon.sh start namenode # 在每个从节点上使用以下命令启动 HDFS DataNode: hadoop-daemon.sh start datanode # 在主节点上使用以下命令启动 YARN ResourceManager: yarn-daemon.sh start resourcemanager # 在每个从节点上使用以下命令启动 YARN nodemanager: yarn-daemon.sh start nodemanager # 以上脚本位于$HADOOP_PREFIX/sbin/目录下 # 如果想要停止某个节点上某个角色，只需要把命令中的start改为stop即可 脚本一键启动 如果配置了 etc/hadoop/slaves 和 ssh 免密登录，则可以使用程序脚本启动所有Hadoop两个集群的相关进程，在主节点所设定的机器上执行。 node01 执行 启动集群 start-dfs.sh start-yarn.sh mr-jobhistory-daemon.sh start historyserver 停止集群 stop-dfs.sh stop-yarn.sh mr-jobhistory-daemon.sh stop historyserver 浏览器查看启动页面 hdfs集群访问地址 http://192.168.2.100:50070/dfshealth.html#tab-overview yarn集群访问地址 http://192.168.2.100:8088/cluster jobhistory访问地址 http://192.168.2.100:19888/jobhistory Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 20:02:28 "},"src/virtualization/docker/Docker架构.html":{"url":"src/virtualization/docker/Docker架构.html","title":"Docker架构","keywords":"","body":"Docker架构 基础 什么是容器 ​ 作为容器，包含了开发、共享、部署的一切，包括代码，运行时，系统库，系统工具和相关的设置。因此，它可以快速运行，并可以在多台不同环境的计算机间无缝运行，避免解决各种因软件和硬件环境差异带来的各种开发和系统问题。 安装 ​ 登录DockerHub ​ 下载Docker.dmg，并安装 卸载 ​ 在【应用程序】卸载docker ​ 无法使用docker命令 架构 Docker使用的是client-server架构。Docker client同Docker daemon（守护进程）通信，Docker daemon负责构建、运行和分发Docker容器。Docker client和Docker daemon可以运行在同一个系统内，Docker client也可以连接到一个远程的Docker daemon。 ​ The Docker daemon ​ Docker daemon（dockerd）监听Docker API请求，管理Docker对象（镜像，容器，网络和卷）。一个daemon也可以同其他daemons通信来管理Docker服务。 ​ The Docker client ​ Docker client（docker）是Docker用户与Docker交互的主要方式。当您使用docker run等命令时，client会将这些命令发送给dockerd，dockerd会执行这些命令。docker命令使用docker API。Docker client可以与多个daemon通信。 ​ Docker registries ​ Docker registry存储Docker镜像。Docker Hub是一个公共的注册中心，任何人均可使用，Docker默认从Docker Hub中获取镜像。 ​ Docker objects ​ IMAGES 镜像 ​ 创建Docker容器的只读模板。为了创建自己的镜像，你需要创建一个Dockerfile，定义一些语法步骤，来创建和运行一个镜像。在Dockerfile中的每一个指令创建镜像中的一层。当你改变Dockerfile，然后重建镜像，仅仅改变的那些层被重建。 ​ CONTAINERS 容器 ​ 一个容器是一个镜像的运行实例。一个容器与其他容器，主机隔离。 ​ SERVICES 服务 ​ 服务允许跨多个Docker daemon来扩展容器，多个管理者和工作者作为一个集群工作。一个集群的每一个成员都是一个Docker daemon，这些daemon使用Docker API来通信。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 20:01:50 "},"src/virtualization/docker/Docker命令.html":{"url":"src/virtualization/docker/Docker命令.html","title":"Docker命令","keywords":"","body":"Docker命令 管理命令 docker builder docker checkpoint docker config docker container docker container run docker container run --publish 8000:8080 --detach --name tc ti:1.0 --detach: 在后台运行容器，打印容器ID --name: 给容器指定一个名称 --publish: 发布容器的端口到主机，要求Docker将主机8000端口的流量转发到容器的8080端口 在一个新容器中运行命令。其中，容器名称必须唯一。 docker container ls docker container ls -a -a: 显示所有容器，默认仅显示正在运行的容器 列举所有容器 docker container rm docker container rm --force tc --force: 强制删除正在运行的容器 删除一个或多个容器。 docker context docker image docker image build docker image build -t ti:1.0 . -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签 执行Dockerfile文件命令，构建镜像。成功后显示 Successfully tagged testubuntu:1.0。 docker image ls docker image ls 列举所有镜像 docker image rm docker image rm ti 删除本地镜像文件，rm后可以是镜像名称，也可以是镜像 ID。 docker image tag docker image tag ti:1.0 sciatta/ti:1.0 为镜像创建标签，创建的新标签指向的是原镜像。 在Docker Hub共享镜像，必须命名为/:的样式。 docker image pull docker image push docker image push sciatta/ti:1.0 上传镜像到仓库。 docker network docker node docker plugin docker secret docker service docker stack docker swarm docker system docker trust docker volume 命令 docker attach docker build docker commit docker cp docker create docker deploy docker diff docker events docker exec docker exec -it tc ps aux 在一个正在运行的容器内执行命令。 docker export docker history docker images docker import docker info docker inspect docker kill docker load docker login docker login 输入用户名和密码，登录到docker仓库。默认是Docker Hub。如果没有Docker ID（用户名），需要在https://hub.docker.com注册。 登录成功后显示Login Succeeded。 docker logout docker logs docker pause docker port docker ps docker pull docker push docker rename docker restart docker rm docker rmi docker run docker run -i -t sciatta/ti /bin/bash -i: 以交互模式运行容器 -t: 为容器重新分配一个伪输入终端 如果本机没有sciatta/ti镜像，从配置仓库pull镜像，同运行命令docker pull。 创建一个新的容器，同运行命令docker container create。 Docker为容器创建一个可读写的文件系统作为最后一层。允许一个正在运行的容器，创建和修改本机文件系统的文件、目录。 在没有指定网络选项时，Docker创建一个网络接口，连接容器到默认的网络，包括为容器分配一个IP地址。默认，容器可以使用主机的网络连接来连接到外部网络。 Docker启动容器，执行/bin/bash。因为容器以交互式运行，并且已经附加到终端 ，所以当日志输出到你的终端时，可以使用键盘输入。 当输入exit退出/bin/bash命令后，容器停止运行，但容器没有被删除。可以重新启动或者删除容器。 docker save docker search docker start docker stats docker stop docker tag docker top docker unpause docker update docker version docker wait Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:59:48 "},"src/virtualization/docker/Dockerfile参考.html":{"url":"src/virtualization/docker/Dockerfile参考.html","title":"Dockerfile参考","keywords":"","body":"Dockerfile参考 概述 Docker从Dockerfile文件中读取指令，自动构建镜像。 docker build 从Dockerfile和上下文构建镜像。构建上下文是在指定位置 PATH 或 URL 下的文件集合。其中，PATH是本地文件系统的目录，URL是Git仓库的位置。上下文的处理是递归的。 docker build . 以当前目录作为上下文。 构建过程是运行在 Docker daemon，而并不是CLI。构建过程的第一件事是发送整个上下文（递归）到daemon。因此，最好是把Dockerfile放置在一个空目录作为上下文，仅增加对构建需要的文件。为了使用上下文下的文件，可以使用COPY指令。为了提高构建性能，可以在上下文目录增加一个.dockerignore文件来排除文件和目录。 注释 指令不是大小写敏感的，但按照惯例，把指令全部写成大写来与参数作区分。Docker按顺序运行Dockerfile内的指令。一个Dockerfile必须以FROM指令开始（可在解析器指令、注释、全局范围参数之后）。 Docker会将以＃开头的行视为注释，除非该行是有效的解析器指令。一行中其他任何地方的＃标记均被视为参数。注释中不支持换行符。 # Comment RUN echo 'we are running some # of cool things' 第一个 # 为注释；第二个 # 被视为参数，正常输出 we are running some # of cool things 。 解析器指令 解析器指令是可选的，影响Dockerfile后续行的执行。解析器指令不会增加层，也不会显示为一个构件步骤。解析器指令被写成一个特殊的注释# directive=value。处理完注释，空行或构件指令后，Docker不再寻找解析器指令。 而是将格式化为解析器指令的任何内容都视为注释，并且不会尝试验证它是否可能是解析器指令。 因此，所有解析器指令必须位于Dockerfile的最顶部。 解析器指令不是大小写敏感的，但按照惯例，全部写成小写。约定还应在任何解析器指令之后包含一个空白行。 解析器指令不支持换行符。而非换行符的空格字符是支持的。 syntax # syntax=[remote image reference] escape # escape=\\ (backslash) 转义指令设置Dockerfile中的转义字符，如果没有指定，默认是 \\ 。 .dockerignore 文件 docker CLI 发送上下文到 docker daemon之前，会首先在上下文的root目录查找.dockerignore文件。如果存在，CLI会修改上下文，排除同模式匹配的文件和目录。这样可以避免发送不必要的大或敏感文件和目录到daemon。如果需要这些文件或目录的话，可以使用 ADD 或 COPY 指令。 CLI将.dockerignore文件解释为以换行符分隔的模式列表，类似于Unix shell的文件组。为了匹配，上下文的根被认为是工作目录和根目录。例如，模式 /foo/bar 和 foo/bar 都在PATH或位于URL的git仓库根目录的foo子目录中排除名为bar的文件或目录。 如果.dockerignore文件中第一行以 # 开始 ，则被看做是注释，在CLI解释之前被忽略。 # 注释 # comment注释 * 匹配任意字符 /temp匹配在root的直接子目录中，以temp为前缀的文件或目录 //temp* 匹配在root的二级子目录中，以temp为前缀的文件或目录 ** 匹配任意数量的目录，包括0个 */.go排除所有目录以.go结尾的文件，包括root目录 ! 排除例外 !README.md包含文件README.md ? 匹配一个字符 temp?匹配在root中以一个字符扩展temp的所有文件或目录 FROM FROM [AS ] FROM [:] [AS ] FROM [@] [AS ] 一个有效的Dockerfile文件必须包含一个 FROM 指令。 ARG 是唯一一个可以先于FROM的指令。 FROM可以在单个Dockerfile中多次出现，以创建多个映像或将一个构建阶段用作对另一个构建阶段的依赖。 只需在每个新的FROM指令之前记录一次提交输出的最后一个镜像ID。 每个FROM指令清除由先前指令创建的任何状态。 可以增加 AS 到 FROM 指令，为一个新的构建阶段提供一个可选的名称。名称可以用于后续 FROM 和 COPY --from= 指令，来在这个阶段引用镜像。 tag 和 digest 值是可选的。如果省略，构建器默认latest tag。如果没有找到tag，则返回一个错误。 ARG VERSION=latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION VERSION定义先于FROM指令，因此在FROM指令之后，ARG不可以用于其他指令。在构建阶段如果需要使用ARG的默认值，则使用ARG指令不赋予值即可。 RUN RUN 以shell方式运行，默认 /bin/sh -c； 如：RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME' RUN [\"executable\", \"param1\", \"param2\"] 以exec方式。注意双引号，JSON格式; 如：RUN [\"/bin/bash\", \"-c\", \"echo hello\"] 在当前镜像的顶层创建一个新层来执行命令，然后提交结果。 不像shell方式，以exec方式方式并不会调用命令shell。也就是说shell处理将不会发生。例如 RUN [ \"echo\", \"$HOME\" ]在 $HOME上将不会进行变量替换。如果需要进行shell处理，则以shell形式，或直接执行shell，如：RUN [ \"sh\", \"-c\", \"echo $HOME\" ]。会进行环境变量扩展，而不是docker。 CMD CMD [\"executable\",\"param1\",\"param2\"] exec方式 CMD [\"param1\",\"param2\"] ENTRYPOINT的默认参数 CMD command param1 param2 shell方式 在Dockerfile中仅可以有一条 CMD 指令。如果有多条，仅有最后一条有效。 主要目的为正在执行容器提供默认执行方式。当运行镜像时，CMD指令被执行。 运行 docker run 指定参数，将覆盖 CMD 中的默认相同参数。 在构建阶段，RUN 执行一个命令，然后提交结果；而 CMD 并不在构建阶段执行，针对镜像的预期指定命令，即在容器执行时执行。 LABEL LABEL = = = ... 为镜像增加元数据。一个LABEL是一个 key-value 对。为了在LABEL的value中包括空字符，需要使用引号和反斜杠。 一个镜像可以有多个LABEL；可以指定多个LABEL在一行。 LABEL可以包括在父镜像中，被用于继承。如果LABEL重复，后者覆盖前者。使用 docker inspect 查看LABEL。 LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" MAINTAINER (deprecated) MAINTAINER 生成镜像的作者。LABEL比MAINTAINER更灵活，建议使用LABEL替换。 例如 LABEL maintainer=\"SvenDowideit@home.org.au\" EXPOSE EXPOSE [/...] 在运行时，容器监听的指定网络端口。可以指定端口监听 TCP 或是 UDP，如果协议没有指定，则TCP是默认协议。 EXPOSE 指令实际并没有发布端口，它的主要功能是在构建镜像者和运行容器者之间作为一种规约，也就是预期要发布的端口。当运行容器时，实际要发布端口的话，使用 -p 标志发布并映射一个或多个端口，或 -P 发布所有 EXPOSE 的端口并映射到高阶端口。 EXPOSE 80/udp 默认TCP协议，可以指定协议为UDP。 不管 EXPOSE 如何设置，都可以在运行时使用 -p 标志覆盖。 ENV ENV ENV = ... 在构建阶段，环境变量可以像使用变量一样用于后续的指令中，被Dockerfile所解析。环境变量可以在Dockerfile中以$variable_name 或 ${variable_name} 方式来引用。 如果在变量前面加上转义字符 \\ ，则按字面字符解释。如 $foo 或者 ${foo}，被分别解释为 $foo 和 ${foo} 。 环境变量在运行时持续存在。可以使用docker inspect 查看。 可以在一个command中设置一个环境变量，使用 RUN = 。 ADD ADD [--chown=:] ... ADD [--chown=:] [\"\",... \"\"] 1、路径包括空字符 2、--chown 只支持UNIX系统 从复制文件，目录或者远程 URL，到镜像文件系统。 可以指定多个资源，如果它们是文件或目录，则将其路径解释为相对于构建上下文的路径。 ADD hom* /mydir/ ADD hom?.txt /mydir/ 可以是一个绝对路径，也可以是一个相对 WORKDIR 的相对路径。 ADD test relativeDir/ # adds \"test\" to `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # adds \"test\" to /absoluteDir/ 路径必须在构建上下文内部；不可使用../something /something，因为docker构建的第一步（FROM）已经把上下文目录发送到docker daemon。 如果是一个URL，并且没有以斜杠结束，则文件从URL下载并复制到。 如果是一个URL，并且以斜杠结束，则文件名按照URL进行推导，文件下载为/。如 ADD http://example.com/foobar / 将创建文件 /foobar 。 如果是一个目录，则目录的所有内容被复制，包括文件系统的元数据。注意，目录本身不会被复制，仅是内容。 如果是一个本地tar压缩格式，则被解压缩为一个目录。来自远程URL的资源不被解压缩。 如果多个资源被指定，或者是目录，或者是通配符形式，则必须是一个目录，必须以 反斜杠结尾。 如果不存在，路径所有缺失的目录被创建。 COPY COPY [--chown=:] ... COPY [--chown=:] [\"\",... \"\"] 从复制文件，目录，到容器文件系统。 ENTRYPOINT ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec form, preferred) ENTRYPOINT command param1 param2 (shell form) 配置容器，作为可执行方式运行。 以exec方式运行，命令行参数会追加到 docker run ENTRYPOINT之后，并覆盖CMD指定的元素。可以搭配 CMD 命令使用，一般是变参会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参。可以使用ENTRYPOINT的exec方式，作为稳定默认命令和参数。然后使用两种形式的CMD设置更可能更改的其他默认值。 FROM nginx ENTRYPOINT [\"nginx\", \"-c\"] # 定参 CMD [\"/etc/nginx/nginx.conf\"] # 变参 docker run nginx:test 不传参运行，启动主进程 nginx -c /etc/nginx/nginx.conf docker run nginx:test -c /etc/nginx/new.conf 传参运行，容器内会默认运行 nginx -c /etc/nginx/new.conf 允许向ENTRYPOINT传参。如 docker run -d ，将 -d 传入到ENTRYPOINT。可以使用 docker run --entrypoint 覆盖 ENTRYPOINT指令。 以Shell方式运行，可防止使用任何 CMD 或 run 命令行参数，但缺点是ENTRYPOINT将作为/bin/sh -c 的子命令启动，该子命令不传递信号。这意味着executable将不是容器的PID 1，并且不会接收Unix信号，因此executable将不会从 docker stop 接收到信号。 多个ENTRYPOINT指令，只有最后一个有效。 ENTRYPOINT和CMD交互 No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUME VOLUME [\"/var/log/\"] VOLUME /var/log or VOLUME /var/log /var/db 创建一个指定名称的挂载点，可以将主机或其他容器的目录挂载到容器中。当容器删除时，数据被持久化而不会丢失。 主机目录是在容器运行时声明的：主机目录（挂载点）从本质上说是依赖于主机的。 这是为了保证镜像的可移植性，因为不能保证给定的主机目录在所有主机上都可用。 因此，您无法从Dockerfile中挂载主机目录。VOLUME 指令不支持指定host-dir参数。 当创建或运行容器时，必须指定挂载点。 USER USER [:] USER [:] USER指令设置运行镜像时要使用的用户名（或UID）以及可选的用户组（或GID），以及Dockerfile中跟随该镜像的所有RUN，CMD和ENTRYPOINT指令。 如果用户没有组，则该镜像（或接下来指令）将用root组运行。 WORKDIR WORKDIR /path/to/workdir WORKDIR指令为Dockerfile中跟在其后的所有RUN，CMD，ENTRYPOINT，COPY和ADD指令设置工作目录。 如果WORKDIR不存在，即使后续的Dockerfile指令中未使用，WORKDIR也将被创建。 WORKDIR指令可在Dockerfile中多次使用。 如果提供了相对路径，则它将相对于上一个WORKDIR指令的路径。 WORKDIR /a WORKDIR b WORKDIR c RUN pwd 最后输出的WORKDIR是 /a/b/c 。 ARG ARG [=] ARG指令定义的变量，用户可以在构建阶段，通过 docker build 指令使用 --build-arg = 覆盖已定义的变量。 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:58:59 "},"src/os/linux/Linux命令.html":{"url":"src/os/linux/Linux命令.html","title":"Linux命令","keywords":"","body":"Linux命令 A B bye C cat Concatenate FILE(s), or standard input, to standard output. cat file1 file2 > file3 1、将file1内容和file2内容输出到file3 2、“命令 > 文件”将标准输出重定向到一个文件中（清空原有文件的数据） 3、“命令 >> 文件”将标准输出重定向到一个文件中（追加到原有内容的后面） cat -n file1 file2 > file3 由1开始对所有输出的行编号，包括空行 cat -b file1 file2 > file3 由1开始对所有输出的行编号，不包括空行 cat 拷贝标准输入到标准输出 cat file1 拷贝file1到标准输出 cat file1 file2 拷贝file1和file2到标准输出 cat /dev/null > file1 1、清空file1的内容 2、dev/null：在类Unix系统中，/dev/null称空设备，它丢弃一切写入其中的数据（但报告写入操作成功）， 读取它则会立即得到一个EOF cat file1 > /dev/null 不会得到任何信息，因为将本来该通过标准输出显示的文件信息重定向到了/dev/null中 chgrp chmod chown cp Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY. -a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。 -d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。 -f：覆盖已经存在的目标文件而不给出提示。 -i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答\"y\"时目标文件将被覆盖。 -p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。 -r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。 -l：不复制文件，只是生成链接文件。 cp -r test test1 复制test目录下的内容到test1 cd dirName：要切换的目标目录。可为绝对路径或相对路径。\"~\" 表示为 home 目录，\".\" 则是表示目前所在的目录，\"..\" 则表示目前目录位置的上一层目录。 cd a/b 切换当前工作目录为目标目录 cd 若目录名称省略，则变换至使用者的 home 目录 D df Show information about the file system on which each FILE resides, or all file systems by default. df -h 显示文件系统的磁盘使用情况统计 Filesystem Size Used Avail Use% Mounted on 显示文件系统、挂载点等信息 du Summarize disk usage of each FILE, recursively for directories. du -h 只显示当前目录和子目录的大小 du -h * 显示当前目录的文件大小，子目录，以及递归子目录的大小（子目录下文件不显示大小） du -sh * 仅显示当前目录的子目录和文件的总计大小 E F find search for files in a directory hierarchy find . -name \"file*\" 查找当前目录和子目录下所有匹配样式file*的文件或目录，输出路径 find . -name \"*.lastUpdated\" | xargs rm -rf tp G grep Search for PATTERN in each FILE or standard input. grep java file* 在当前目录文件名为file前缀的文件中，查找包含java的文件，并输出行内容 grep -rn java . 递归方式查找当前目录和其子目录 grep -n '2019-10-24 00:01:11' *.log H I J K L less Less is a program similar to more, but which allows backward movement in the file as well as forward movement. Also, less does not have to read the entire input file before starting, so with large input files it starts up faster than text editors like vi. G 移动到最后一行 g 移动到第一行 /字符串 向下搜索“字符串” ?字符串 向上搜索“字符串”的功能 n 重复前一个搜索（与 / 或 ? 有关） N 反向重复前一个搜索（与 / 或 ? 有关） Enter（j） 向下移动一行 k 向上移动一行 空格键（d） 向下翻一页 b 向上翻一页 q（ZZ） 退出less命令 ps -ef |less ps查看进程信息并通过less分页显示 history | less 查看命令历史使用记录并通过less分页显示 less file1 file2 1、浏览多个文件 2、输入 :n 切换到file2 :p 切换到file1 ls ls -al 默认当前目录，显示所有文件和目录，包括隐藏档 M more 空格键 向下翻一页 b 向上翻一页 more -s file1 如有连续两行以上空白行则以一行空白行显示 more +5 file1 从第5行开始显示 mv mv file1 file 移动并改名 mv ok/ no ok和no都是目录。当no不存在时，则将ok改名为no；当no存在时，将ok目录转移到no目录。 mv ok/* no ok和no都是目录。当no不存在时，失败；当no存在时，将ok目录所有内容转移到no目录。 mkdir Create the DIRECTORY(ies), if they do not already exist. mkdir [-p] dirName -p 确保目录名称存在，不存在的就建一个。 mkdir -p a/b 在当前工作目录下创建目录，父目录不存在，则一起创建 N O P pwd Print the full filename of the current working directory. pwd [--help][--version] --help 在线帮助。 --version 显示版本信息。 pwd 输出当前工作路径 Q R rcp rm Remove (unlink) the FILE(s). rm [options] name... -i 删除前逐一询问确认。 -f 即使原档案属性设为唯读，亦直接删除，无需逐一确认。 -r 将目录及以下之档案亦逐一删除。 rm services rm: remove regular file ‘services’? n 删除文件需要询问 rm -r test 删除目录 rm -rf test 强制删除所有文件和目录（即使只读），不需要询问 S ssh ssh root@172.16.92.132 1、登录远程服务器，接着输入密码 2、iTerm远程连接centos服务器，显示命令帮助为中文，而centos服务器内部可以显示英文。 centos语言为US，iTerm语言为CN，因此需要修改iTerm。 1）vim ~/.zshrc 2）末尾添加 export LC_ALL=en_US.UTF-8 export LANG=en_US.UTF-8 3）source ~/.zshrc scp T touch touch services -rw-r--r--. 1 root root 670293 Dec 31 17:10 services -rw-r--r--. 1 root root 670293 Dec 31 21:07 services 文件存在，更新修改时间为当前时间 touch file1 文件不存在，创建空文件 U V W which Write the full path of COMMAND(s) to standard output. which [文件...] -n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 -p 与-n参数相同，但此处的包括了文件的路径。 -w 指定输出时栏位的宽度。 -V 显示版本信息。 which bash which指令会在环境变量$PATH设置的目录里查找符合条件的文件 which -a bash 打印所有匹配的路径（不仅只有第一个） wc Print newline, word, and byte counts for each FILE, and a total line ifmore than one FILE is specified. With no FILE, or when FILE is -,read standard input. A word is a non-zero-length sequence of charactersdelimited by white space. wc [-clw][--help][--version][文件...] -c或--bytes 只显示Bytes数。 -m或--chars 只显示字符数。 -l或--lines 只显示行数。 -w或--words 只显示单词数。 --help 在线帮助。 --version 显示版本信息。 wc file1 file2 2 3 17 file1 3 4 18 file2 5 7 35 total 行数 单词数 字节数，以及总统计值 X Y Z Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:38:51 "},"src/os/linux/iTerm的使用.html":{"url":"src/os/linux/iTerm的使用.html","title":"iTerm的使用","keywords":"","body":"iTerm的使用 快捷键 水平分隔当前屏幕 shift+command+D 垂直分隔当前屏幕 command+D 向所有TAB的所有窗口广播命令 shift+command+i 向当前TAB的所有窗口广播命令 option+command+i 将窗口分组广播命令 1、方法一 所有窗口分组 option+command+i 单独一个窗口分组 shift+control+option+command+i 2、方法二 单独一个窗口分组 shift+control+option+command+i 其他窗口分在一组 shift+control+option+command+i 恢复窗口分组 shift+option+command+i Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-20 19:57:55 "},"src/ide/intellijidea/IDEA配置Java开发环境.html":{"url":"src/ide/intellijidea/IDEA配置Java开发环境.html","title":"IDEA配置Java开发环境","keywords":"","body":"配置jdk 项目右键 | open module settings | sdks | + jdk 指定 jdk home path 项目右键 | open module settings | project 指定sdk 项目右键 | open module settings | modules | language level: 8 Preferences | Build, Execution, Deployment | compiler | java compiler module 设置为8 project bytecode version 设置为8 Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-31 16:36:01 "},"src/ide/intellijidea/IDEA创建GitHub项目.html":{"url":"src/ide/intellijidea/IDEA创建GitHub项目.html","title":"IDEA创建GitHub项目","keywords":"","body":"GitHub空项目 在GitHub上创建空项目，如：hadoop-main IDEA创建project check out from version control | 选择 git 指定URL https://github.com/sciatta/hadoop-main.git 和本地目录 | clone | 选择 create project from existing sources 后续默认选择 初始化project git flow init 后续在develop分支开发 IDEA创建module 创建 parent pom module hadoop-main | new module | maven groupid: com.sciatta.hadoop artifact: hadoop-main module name: hadoop-main # hadoop-main根目录作为module parent content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main # 修改pom.xml pom 创建child pom module hadoop-main | new module | maven parent: hadoop-main add as module to: hadoop-main artifact: hadoop-hdfs module name: hadoop-hdfs content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs # 修改pom.xml pom 创建child jar module hadoop-hdfs | new module | maven parent: hadoop-hdfs add as module to: hadoop-hdfs artifact: hadoop-hdfs-example module name: hadoop-hdfs-example content root: /Users/yangxiaoyu/work/bigdata/project/hadoop-main/hadoop-hdfs/hadoop-hdfs-example # 修改pom.xml jar Copyright © sciatta.com 2020 all right reserved，powered by Gitbook修订时间: 2020-01-21 20:17:03 "}}